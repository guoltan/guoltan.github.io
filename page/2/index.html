<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"guoltan.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="guoltan个人博客">
<meta property="og:url" content="https://guoltan.github.io/page/2/index.html">
<meta property="og:site_name" content="guoltan个人博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="guoltan">
<meta property="article:tag" content="摸鱼，自嗨，困奋">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://guoltan.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>guoltan个人博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="guoltan个人博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">guoltan个人博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">guoltan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/guoltan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;guoltan" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:897156356@qq.com" title="E-Mail → mailto:897156356@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2023/08/26/helm%20list%20%E6%98%BE%E7%A4%BA%E4%B8%8D%E5%87%BA%E8%B5%84%E6%BA%90%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/26/helm%20list%20%E6%98%BE%E7%A4%BA%E4%B8%8D%E5%87%BA%E8%B5%84%E6%BA%90%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">helm list 显示不出资源原因分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-08-26 18:11:25" itemprop="dateCreated datePublished" datetime="2023-08-26T18:11:25+08:00">2023-08-26</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>958</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p> 部分项目由客户运维，有些客户对 helm 指令不熟悉，客户误操作查询不出 release 会引起一些歧义。本文基于过去的一些处理的案例，对 helm list 不出资源的可能原因进行说明，并解释 helm list -a 和 -A 的区别。</p>
<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><h2 id="helm-list-不出东西怎么办"><a href="#helm-list-不出东西怎么办" class="headerlink" title="helm list 不出东西怎么办"></a>helm list 不出东西怎么办</h2><p>  helm list 是用于列举特定命名空间下的 release。本质上 release 是存储在 secret 的（Helm v2 存储在 configmap）。所以 helm list 这个指令实际上可类比为 kubectl get secret -n 命名空间 -l owner&#x3D;helm。如果说 helm list 找不到东西，可能原因如下：</p>
<ul>
<li><p>helm list 没有加 -n 选项，没有在指定命名空间下查询 release</p>
</li>
<li><p>资源已被全部删除</p>
</li>
<li><p>资源没有被删除，但是用于追踪状态的 secret 被误删除。</p>
</li>
<li><p>错误的使用 helm ls -a 去获取集群当前所有的 release。</p>
</li>
</ul>
<p>基于原因，可检查项：</p>
<ul>
<li><p>helm ls 有没有加上 -n 命名空间是否正确</p>
</li>
<li><p>资源是否本身已被删除过了，通过 kubectl get pod -A 是否能看到相关业务 Pod。</p>
</li>
<li><p>kubectl get secret -A | grep sh.helm.release 检查集群中的保存 release 状态的 secret 是否还存在。</p>
</li>
<li><p>是否使用了 helm ls -a 选项。需要修改成 helm ls -A</p>
</li>
</ul>
<h2 id="helm-list-A-和-a-有什么区别？"><a href="#helm-list-A-和-a-有什么区别？" class="headerlink" title="helm list -A 和 -a 有什么区别？"></a>helm list -A 和 -a 有什么区别？</h2><p>helm list –help 中，给出 -a 、-A 的区别如下</p>
<p> -a, –all show all releases without any filter applied</p>
<p> -A, –all-namespaces list releases across all namespaces</p>
<p>-a 简单翻译就是显示未被筛选的所有 release 版本。对于某些中间状态，默认情况下 helm list 是不会显示的，如果要显示的话，就可以增加 -a 的选项。</p>
<p>-A 简单的说就是列出集群所有命名空间下的 release。如果对 release 配置在哪个命名空间不大清楚，是可以通过 -A 选项去查询的。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2023/08/15/%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89%E5%8C%96%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%20registry/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/15/%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89%E5%8C%96%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%20registry/" class="post-title-link" itemprop="url">搭建 Kubernetes 私有化镜像仓库 registry</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-08-15 18:11:25" itemprop="dateCreated datePublished" datetime="2023-08-15T18:11:25+08:00">2023-08-15</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h1><p>  在使用 Kubernetes 平台过程中，常常会有搭建私有镜像仓库的需求。而本文主要是记录如何在 Kubernetes 集群上，以 staticpod 的方式快速拉起一个单实例的 registry。以提供容器镜像存储使用。</p>
<h1 id="2-部署步骤"><a href="#2-部署步骤" class="headerlink" title="2. 部署步骤"></a>2. 部署步骤</h1><h2 id="2-1-创建-registry-需要使用的证书"><a href="#2-1-创建-registry-需要使用的证书" class="headerlink" title="2.1. 创建 registry 需要使用的证书"></a>2.1. 创建 registry 需要使用的证书</h2><p>  docker 提供的 registry 默认情况下需要配置 HTTPS 证书，所以我们在搭建前需要先获取 HTTPS 证书。而证书的获取则是通过自签名证书的方式实现。值得注意的是，新版本 go 1.15 以上对证书字段存在要求，必须使用” SAN 字段”。否则在使用过程中，可能会触发下面的错误：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">error: failed to solve: registry.sealos.hub:5000/library/nginx:latest: failed to do request:</span> <span class="string">Head</span> <span class="attr">&quot;https://registry.sealos.hub:5000/v2/library/nginx/manifests/latest&quot;:</span> <span class="attr">tls: failed to verify certificate: x509:</span> <span class="string">certificate</span> <span class="string">relies</span> <span class="string">on</span> <span class="string">legacy</span> <span class="string">Common</span> <span class="string">Name</span> <span class="string">field,</span> <span class="string">use</span> <span class="string">SANs</span> <span class="string">instead</span></span><br></pre></td></tr></table></figure>

<p>以下是自签名证书实现的过程。</p>
<h3 id="2-1-1-创建证书信息文件"><a href="#2-1-1-创建证书信息文件" class="headerlink" title="2.1.1. 创建证书信息文件"></a>2.1.1. 创建证书信息文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 创建证书信息文件 openssl.cnf</span><br><span class="line"># 其中 alt_names 的 IP、DNS 字段可以根据实际需要去增加和修改。</span><br><span class="line"></span><br><span class="line">[req]</span><br><span class="line">req_extensions = v3_req</span><br><span class="line">distinguished_name = req_distinguished_name</span><br><span class="line"></span><br><span class="line">[req_distinguished_name]</span><br><span class="line">countryName = Country Name (2 letter code)</span><br><span class="line">countryName_default = XX</span><br><span class="line">stateOrProvinceName = State or Province Name (full name)</span><br><span class="line">stateOrProvinceName_default = YourState</span><br><span class="line">localityName = Locality Name (eg, city)</span><br><span class="line">localityName_default = YourCity</span><br><span class="line">organizationName = Organization Name (eg, company)</span><br><span class="line">organizationName_default = YourCompany</span><br><span class="line">commonName = Common Name (e.g. server FQDN or YOUR name)</span><br><span class="line">commonName_max = 64</span><br><span class="line"></span><br><span class="line">[v3_req]</span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line"></span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1 = sealos.hub</span><br><span class="line">DNS.2 = registry.sealos.hub</span><br><span class="line">IP.1 = 192.168.214.101</span><br><span class="line">IP.2 = 192.168.214.1</span><br></pre></td></tr></table></figure>

<h3 id="2-1-2-创建自签名证书"><a href="#2-1-2-创建自签名证书" class="headerlink" title="2.1.2. 创建自签名证书"></a>2.1.2. 创建自签名证书</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成私钥文件</span></span><br><span class="line">openssl genpkey -algorithm RSA -out registry.key</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成证书签名（CSR）文件，一直回车</span></span><br><span class="line">openssl req -new -key registry.key -out registry.csr -config openssl.cnf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用私钥和配置文件生成自签名证书</span></span><br><span class="line">openssl x509 -req -signkey registry.key -<span class="keyword">in</span> registry.csr -out registry.crt -extensions v3_req -extfile openssl.cnf -days 36500</span><br></pre></td></tr></table></figure>

<h2 id="2-2-创建-registry-的-staticPod"><a href="#2-2-创建-registry-的-staticPod" class="headerlink" title="2.2. 创建 registry 的 staticPod"></a>2.2. 创建 registry 的 staticPod</h2><p>  新建 registry.yaml 文件，存放在 &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F; 目录下，内容如下</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">registry</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">registry</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">registry:2</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">private-repository-k8s</span></span><br><span class="line">      <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REGISTRY_HTTP_TLS_CERTIFICATE</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;/certs/registry.crt&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REGISTRY_HTTP_TLS_KEY</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;/certs/registry.key&quot;</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5000</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">certs-vol</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/certs</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registry-vol</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/registry</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">certs-vol</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/opt/certs</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registry-vol</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/opt/registry</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line"><span class="attr">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>  此处的 volumes 字段，certs-vol、registry-vol 的 path 路径可以根据实际存储位置进行调整。</p>
<p>  执行完成以后，通过 kubectl get pod -n kube-system -o wide | grep registry，就可以看到创建出的 registry pod。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master01certs]<span class="comment"># kubectl get pod -n kube-system -o wide | grep registry</span></span><br><span class="line">registry-k8s-master01                      1/1     Running   2          4h36m   192.168.214.101   k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h2 id="2-3-如何解决客户端不信任-registry-提供自签名证书"><a href="#2-3-如何解决客户端不信任-registry-提供自签名证书" class="headerlink" title="2.3. 如何解决客户端不信任 registry 提供自签名证书"></a>2.3. 如何解决客户端不信任 registry 提供自签名证书</h2><p>  经过上述步骤以后，此时可以通过 nerdctl push 测试镜像推送，如果是自签名证书，此时可能会触发如下报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tls: failed to verify certificate: x509: certificate signed by unknown authority</span><br></pre></td></tr></table></figure>

<p>  这是因为客户端没有信任 registry 提供的 HTTPS 证书导致的，对于这种问题，可解决的方法：</p>
<ul>
<li><p>关闭客户端的 HTTPS 校验功能，忽略不受信任。– 对于测试、学习环境，可以通过该方式快速解决。</p>
</li>
<li><p>通过配置客户端，将自签名证书添加到受信任的证书列表。 – 虽然操作起来比较复杂，但这种方式相对安全的多。</p>
<p>需要提醒的是笔者使用的是 containerd 而非 docker，所以操作方式以 containerd 为例。</p>
</li>
</ul>
<h3 id="2-3-1-关闭-HTTPS-校验"><a href="#2-3-1-关闭-HTTPS-校验" class="headerlink" title="2.3.1. 关闭 HTTPS 校验"></a>2.3.1. 关闭 HTTPS 校验</h3><p>  参考如下方法来完成 HTTPS 校验关闭的动作</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑 /etc/containerd/config.toml 追加如下配置</span></span><br><span class="line">    [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry]</span><br><span class="line">      [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs.<span class="string">&quot;registry.sealos.hub:5000&quot;</span>.tls]</span><br><span class="line">        insecure_skip_verify = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境可能默认没有 /etc/containerd/config.toml 文件，可以通过 containerd config dump &gt; /etc/containerd/config.toml 去生成</span></span><br></pre></td></tr></table></figure>

<h3 id="2-3-2-为-containerd-添加-HTTPS-证书"><a href="#2-3-2-为-containerd-添加-HTTPS-证书" class="headerlink" title="2.3.2. 为 containerd 添加 HTTPS 证书"></a>2.3.2. 为 containerd 添加 HTTPS 证书</h3><p>   我们可以将 HTTPS 证书放置在 &#x2F;etc&#x2F;containerd&#x2F;certs.d 目录下，需要在所有 node 上执行这个动作。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果没有 /etc/containerd/certs.d 目录，则需要创建</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /etc/containerd/certs.d</span><br><span class="line"><span class="comment"># 将 2.1.2. 生成的上传证书文件到 /etc/containerd/certs.d/ 目录下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果环境配置了 buildkitd，此时也需要追加证书</span></span><br><span class="line">修改 buildkitd 配置文件的内容 /etc/buildkit/buildkitd.toml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加如下内容，将对应镜像仓库信息、证书路径写入</span></span><br><span class="line">[registry.<span class="string">&quot;registry.sealos.hub:5000&quot;</span>]</span><br><span class="line">  http = <span class="literal">false</span></span><br><span class="line">  insecure = <span class="literal">false</span></span><br><span class="line">  ca=[<span class="string">&quot;/opt/certs/registry.crt&quot;</span>]</span><br><span class="line">  [[registry.<span class="string">&quot;registry.sealos.hub:5000&quot;</span>.keypair]]</span><br><span class="line">    key=<span class="string">&quot;/opt/certs/registry.key&quot;</span></span><br><span class="line">    cert=<span class="string">&quot;/opt/certs/registry.crt&quot;</span></span><br><span class="line">[worker.containerd]</span><br><span class="line">  namespace = <span class="string">&quot;k8s.io&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="2-4-验证上传下载功能"><a href="#2-4-验证上传下载功能" class="headerlink" title="2.4. 验证上传下载功能"></a>2.4. 验证上传下载功能</h2><p>  经过上述的步骤以后，就可以在客户端侧进行镜像的推拉测试，测试无误后。至此镜像仓库搭建已完成。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从公网拉取镜像</span></span><br><span class="line"><span class="comment"># 修改 tag</span></span><br><span class="line">nerdctl --namespace k8s.io tag daocloud.io/nginx:latest registry.sealos.hub:5000/library/nginx:latest</span><br><span class="line"><span class="comment"># 推送</span></span><br><span class="line">nerdctl --namespace k8s.io push registry.sealos.hub:5000/library/nginx:latest</span><br><span class="line"><span class="comment"># 拉取镜像</span></span><br><span class="line">nerdctl --namespace k8s.io registry.sealos.hub:5000/library/nginx:latest</span><br></pre></td></tr></table></figure>

<p>参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/moby/buildkit/blob/master/docs/buildkitd.toml.md">https://github.com/moby/buildkit/blob/master/docs/buildkitd.toml.md</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2023/04/23/volcano%E8%B0%83%E5%BA%A6%E8%B0%83%E4%BC%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/23/volcano%E8%B0%83%E5%BA%A6%E8%B0%83%E4%BC%98/" class="post-title-link" itemprop="url">volcano调度调优</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-04-23 20:01:15" itemprop="dateCreated datePublished" datetime="2023-04-23T20:01:15+08:00">2023-04-23</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>25k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>46 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h1><p>  某客户反馈 AI 平台中有某几个节点的负载很高，业务都调度到这些机器上了，会出现因为负载高导致 hang 死的问题。但是同时 AI 平台中还存在很多空闲节点，这些节点上没有运行业务 Pod，希望确认原因。</p>
<p>  而目前 AI 平台使用的是 volcano 作为调度器，现在 AI 平台在调度业务 Pod（训练、推理、notebook等等）上，是通过 binpack 算法尽量先把已有节点填满，再去分发业务到新的计算节点。这样做的目的是尽量减少资源碎片，有效的提升节点资源的利用率。由于 binpack 这种形式是先填满已有节点，所以当平台环境初期业务量不多的时候，就会出现客户反馈的问题，集群中个别几个业务量很大，其他节点则没有运行业务 Pod（存在空闲节点）的现象。</p>
<p>  而节点出现因为负载高导致 hang 死的问题是因为 volcano 默认给节点预留的资源较少（2U4G）。而客户发放的业务规格有大有小，通过 binpack 的方式集中调度后，导致机器的资源利用率压榨的比较极限，机器有出现部分 limits 超出的情况（下图的内容是仅计入 K8S Pod 的数据。有部分系统进程也需要使用 CPU、内存资源，未计入）当业务 Pod 同时在运行，并且资源使用率较高时，节点负载就会很高，可能出现 hang 死的问题。</p>
<img src="/2023/04/23/volcano%E8%B0%83%E5%BA%A6%E8%B0%83%E4%BC%98/1.png" class="" title="1.png">

<h1 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a><strong>解决思路</strong></h1><p>对于上述的问题，目前有两种解决的思路，解决方法如下：</p>
<p>· 方案一：提升资源预留份额，通过修改 volcano 调度器的配置，提升节点资源预留的份额，这样能一定程度上避免节点业务运行过多，导致 hang 死的问题。修改以后由于还在使用 binpack 算法，初期集群中还是会出现空闲节点的现象。但随着整个集群业务的增加，空闲节点会逐渐减少。并且使用了 binpack 算法，最终能保证节点的资源高利用率。</p>
<p>· 方案二：去除 binpack 调度算法，通过修改 volcano 调度器的配置，去掉 binpack 算法。这样 Pod 就会打散在各个节点上，减少空闲节点的产生。对于环境初期来说，客户侧就能看到各个节点上能运行业务。但由于不使用 binpack 调度算法，环境后期可能会出现资源碎片较多的问题。</p>
<h1 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a><strong>操作步骤</strong></h1><h2 id="方案一：修改-reserveresource-中预留的资源数量"><a href="#方案一：修改-reserveresource-中预留的资源数量" class="headerlink" title="方案一：修改 reserveresource 中预留的资源数量"></a><strong>方案一：修改 reserveresource 中预留的资源数量</strong></h2><h3 id="配置修改"><a href="#配置修改" class="headerlink" title="配置修改"></a><strong>配置修改</strong></h3><h4 id="修改-volcano-scheduler-configmap-中的-reserveresource"><a href="#修改-volcano-scheduler-configmap-中的-reserveresource" class="headerlink" title="修改 volcano-scheduler-configmap 中的 reserveresource"></a><strong>修改 volcano-scheduler-configmap 中的 reserveresource</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit cm -n ti-inf volcano-scheduler-configmap</span><br><span class="line"></span><br><span class="line">-----------修改前-------------</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  volcano-scheduler.conf: |</span><br><span class="line">    actions: &quot;enqueue, reserve, allocate, backfill, preempt&quot;</span><br><span class="line">    tiers:</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: reserveresource    # reserveresource 的 plugins</span><br><span class="line">        arguments:</span><br><span class="line">          reserve.cpu: 2         # 根据现场环境调整保留的 CPU 资源，默认是 2C。</span><br><span class="line">          reserve.memory: 4Gi    # 根据现场环境调整保留的 CPU 资源，默认是 4G。</span><br><span class="line"></span><br><span class="line">-----------修改后-------------</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  volcano-scheduler.conf: |</span><br><span class="line">    actions: &quot;enqueue, reserve, allocate, backfill, preempt&quot;</span><br><span class="line">    tiers:</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: reserveresource    # reserveresource 的 plugins</span><br><span class="line">        arguments:</span><br><span class="line">          reserve.cpu: 4         # 根据现场环境调整保留的 CPU 资源，默认是 2C，本文此处修改为 4C。</span><br><span class="line">          reserve.memory: 8Gi    # 根据现场环境调整保留的 CPU 资源，默认是 4G，本文此处修改为 8G。</span><br></pre></td></tr></table></figure>

<h4 id="修改完成-configmap-以后，执行如下指令重启-volcano-scheduler"><a href="#修改完成-configmap-以后，执行如下指令重启-volcano-scheduler" class="headerlink" title="修改完成 configmap 以后，执行如下指令重启 volcano-scheduler"></a><strong>修改完成 configmap 以后，执行如下指令重启 volcano-scheduler</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod -n ti-inf volcano-scheduler-xxxx</span><br></pre></td></tr></table></figure>

<p> 注：输入 volcano-scheduler- 以后，按 tab 可以补全获取 Pod名称。</p>
<h2 id="方案二：去除-binpack-调度算法"><a href="#方案二：去除-binpack-调度算法" class="headerlink" title="方案二：去除 binpack 调度算法"></a><strong>方案二：去除 binpack 调度算法</strong></h2><h3 id="配置修改-1"><a href="#配置修改-1" class="headerlink" title="配置修改"></a><strong>配置修改</strong></h3><h4 id="修改-volcano-scheduler-configmap，删除-binpack-算法"><a href="#修改-volcano-scheduler-configmap，删除-binpack-算法" class="headerlink" title="修改 volcano-scheduler-configmap，删除 binpack 算法"></a><strong>修改 volcano-scheduler-configmap，删除 binpack 算法</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit cm -n ti-inf volcano-scheduler-configmap</span><br><span class="line"></span><br><span class="line">-----------修改前-------------</span><br><span class="line">data:</span><br><span class="line">  volcano-scheduler.conf: |</span><br><span class="line">    actions: &quot;enqueue, reserve, allocate, backfill, preempt&quot;</span><br><span class="line">    tiers:</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: reserveresource</span><br><span class="line">        arguments:</span><br><span class="line">          reserve.cpu: 2</span><br><span class="line">          reserve.memory: 4Gi</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: priority</span><br><span class="line">      - name: gang</span><br><span class="line">      - name: conformance</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: predicates</span><br><span class="line">      - name: nodeorder</span><br><span class="line">        arguments:</span><br><span class="line">          nodeaffinity.weight: 20</span><br><span class="line">          podaffinity.weight: 20</span><br><span class="line">          bizaffinity.weight: 20</span><br><span class="line">          resourcefit.weight: 1</span><br><span class="line">          leastrequested.weight: 0</span><br><span class="line">          balancedresource.weight: 1</span><br><span class="line">          imagelocality.weight: 1</span><br><span class="line">          similaraffinity.weight: 1</span><br><span class="line">      - name: preemptnodeorder</span><br><span class="line">      - name: binpack                            # 需要将 name: binpack 这个元素完全删除。</span><br><span class="line">        arguments:</span><br><span class="line">          binpack.weight: 10</span><br><span class="line">          binpack.cpu: 1</span><br><span class="line">          binpack.memory: 1</span><br><span class="line">          binpack.resources: nvidia.com/gpu,tencent.com/vcuda-core</span><br><span class="line">          binpack.resources.nvidia.com/gpu: 5</span><br><span class="line">          binpack.resources.tencent.com/vcuda-core: 5</span><br><span class="line"></span><br><span class="line">-----------修改后-------------</span><br><span class="line">data:</span><br><span class="line">  volcano-scheduler.conf: |</span><br><span class="line">    actions: &quot;enqueue, reserve, allocate, backfill, preempt&quot;</span><br><span class="line">    tiers:</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: reserveresource</span><br><span class="line">        arguments:</span><br><span class="line">          reserve.cpu: 2</span><br><span class="line">          reserve.memory: 4Gi</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: priority</span><br><span class="line">      - name: gang</span><br><span class="line">      - name: conformance</span><br><span class="line">    - plugins:</span><br><span class="line">      - name: predicates</span><br><span class="line">      - name: nodeorder</span><br><span class="line">        arguments:</span><br><span class="line">          nodeaffinity.weight: 20</span><br><span class="line">          podaffinity.weight: 20</span><br><span class="line">          bizaffinity.weight: 20</span><br><span class="line">          resourcefit.weight: 1</span><br><span class="line">          leastrequested.weight: 0</span><br><span class="line">          balancedresource.weight: 1</span><br><span class="line">          imagelocality.weight: 1</span><br><span class="line">          similaraffinity.weight: 1</span><br><span class="line">      - name: preemptnodeorder</span><br></pre></td></tr></table></figure>

<h4 id="修改完成-configmap-以后，执行如下指令重启-volcano-scheduler-1"><a href="#修改完成-configmap-以后，执行如下指令重启-volcano-scheduler-1" class="headerlink" title="修改完成 configmap 以后，执行如下指令重启 volcano-scheduler"></a><strong>修改完成 configmap 以后，执行如下指令重启 volcano-scheduler</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod -n ti-inf volcano-scheduler-xxxx</span><br></pre></td></tr></table></figure>

<p>注：输入 volcano-scheduler- 以后，按 tab 可以补全获取 Pod名称。</p>
<h3 id="效果测试"><a href="#效果测试" class="headerlink" title="效果测试"></a><strong>效果测试</strong></h3><p>  验证 binpack 是否被去除，可以通过下发新的业务 Pod，然后查看 volcano-scheduler 日志判断 binpack 的算法是否已去除。另外 binpack 是集中调度，会尽量往已有业务的节点上调度，也可以通过创建多个业务 Pod。查看 Pod 是否分散到不同的节点上来确认是否生效。</p>
<h4 id="通过日志去验证是否去除-binpack"><a href="#通过日志去验证是否去除-binpack" class="headerlink" title="通过日志去验证是否去除 binpack"></a><strong>通过日志去验证是否去除 binpack</strong></h4><p>在控制台 notebook 中，创建一个 1C1G 的示例，查看 volcano-scheduler 日志里面的执行步骤是否还存在 binpack 的算法。</p>
<p>获取 notebook 的 Pod 名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@tke01 ~]# kubectl get pod -n project-1  -o wide | grep notebook</span><br><span class="line">notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql    2/2     Running   0          78s     10.199.10.169   192.168.0.88   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-84e3ebf6-564e-42d8-aa7a-7c7d4c6ab1f6-6f76d587f9-mn87r    2/2     Running   0          4d3h    10.199.3.15     192.168.0.88   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>查看创建 1C1G notebook 示例的调度日志（注：此处日志级已修改为 level 4，关注的关键字是 notebook 名称，以及调度时是否有出现 binpack.go 的字样）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">I1217 07:57:01.750134       1 scheduler.go:91] Start scheduling ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.751435       1 cache.go:823] There are &lt;147&gt; Jobs, &lt;1&gt; Queues and &lt;4&gt; Nodes in total for scheduling.</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.751538       1 session.go:157] Open Session fd36a727-80af-4ec2-94a6-1ddf1ea1f9c0 with &lt;4&gt; Node, &lt;147&gt; Job and &lt;1&gt; Queues</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770238       1 enqueue.go:57] Enter Enqueue ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770259       1 enqueue.go:75] Added Queue &lt;default&gt; for Job &lt;tione/podgroup-b884bf44-30f4-42fe-9d35-cae336b15bda&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770292       1 enqueue.go:86] Added Job &lt;project-1/podgroup-30488abb-93ee-4d3b-9392-5aec5d43a39e&gt; into Queue &lt;default&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770321       1 enqueue.go:91] Try to enqueue PodGroup to 1 Queues</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770339       1 enqueue.go:162] Leaving Enqueue ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770344       1 reserve.go:28] Enter Reserve ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770355       1 reserve.go:35] No target job, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770361       1 reserve.go:36] Leaving Reserve ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770367       1 allocate.go:45] Enter Allocate ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770512       1 allocate.go:115] unlockedNode ID: c01f9ad5-f285-462a-8261-d397c28c261a, Name: 192.168.0.2</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770522       1 allocate.go:115] unlockedNode ID: 5b5c2742-2344-44d3-98fc-52d01a0adb79, Name: 192.168.0.25</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770527       1 allocate.go:115] unlockedNode ID: b2c5cdcf-d201-41f8-85cd-4c88e838a6aa, Name: 192.168.0.43</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770531       1 allocate.go:115] unlockedNode ID: b2802217-f48a-4dab-b92a-6911ae7a2e7d, Name: 192.168.0.88</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770548       1 allocate.go:170] Namespace &lt;cpaas-system&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770574       1 allocate.go:170] Namespace &lt;ns-1&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770600       1 allocate.go:209] Try to allocate resource to 1 tasks of Job &lt;project-1/podgroup-30488abb-93ee-4d3b-9392-5aec5d43a39e&gt; in Namespace &lt;project-1&gt; Queue &lt;default&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770608       1 allocate.go:226] There are &lt;4&gt; nodes for Task &lt;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770644       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&gt; on node &lt;192.168.0.2&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 24095.00, memory 45479352318.00, hostdev.k8s.io/dev_mem 255000.00, hugepages-1Gi 0.00, hugepages-2Mi 0.00, hostdev.k8s.io/dev_fuse 256000.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770669       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&gt; on node &lt;192.168.0.25&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 18011.00, memory 38152880128.00, hugepages-2Mi 0.00, hostdev.k8s.io/dev_fuse 256000.00, hostdev.k8s.io/dev_mem 256000.00, hugepages-1Gi 0.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770693       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&gt; on node &lt;192.168.0.88&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 3671.00, memory 49670938496.00, hostdev.k8s.io/dev_fuse 256000.00, hostdev.k8s.io/dev_mem 256000.00, hugepages-2Mi 0.00, hugepages-1Gi 0.00, tencent.com/vcuda-core 100000.00, tencent.com/vcuda-memory 59000.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.770680       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&gt; on node &lt;192.168.0.43&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 14604.00, memory 36533878784.00, hugepages-2Mi 0.00, hostdev.k8s.io/dev_fuse 256000.00, hostdev.k8s.io/dev_mem 251000.00, hugepages-1Gi 0.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771095       1 nodeorder.go:314] Total Score for task project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g on node 192.168.0.2 is: 195.000000, reason: il 0, ba 0, rf 100, br 95, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771113       1 nodeorder.go:314] Total Score for task project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g on node 192.168.0.88 is: 658.000000, reason: il 0, ba 25, rf 100, br 58, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771113       1 nodeorder.go:314] Total Score for task project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g on node 192.168.0.25 is: 199.000000, reason: il 0, ba 0, rf 100, br 99, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771131       1 nodeorder.go:314] Total Score for task project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g on node 192.168.0.43 is: 193.000000, reason: il 0, ba 0, rf 100, br 93, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771275       1 nodeorder.go:403] inter pod affinity Score for task project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g is: map[192.168.0.2:0 192.168.0.25:0 192.168.0.43:0 192.168.0.88:0]</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771309       1 nodeorder.go:455] taint toleration Score for task project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g is: map[192.168.0.2:100 192.168.0.25:100 192.168.0.43:100 192.168.0.88:100]</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771321       1 nodeorder.go:349] Batch Total Score for task project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g is: map[192.168.0.2:100 192.168.0.25:100 192.168.0.43:100 192.168.0.88:100]</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771351       1 allocate.go:256] Binding Task &lt;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&gt; to node &lt;192.168.0.88&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771367       1 scheduler_binder.go:323] AssumePodVolumes for pod &quot;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&quot;, node &quot;192.168.0.88&quot;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771380       1 scheduler_binder.go:333] AssumePodVolumes for pod &quot;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&quot;, node &quot;192.168.0.88&quot;: all PVCs bound and nothing to do</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771400       1 statement.go:271] After allocated Task &lt;project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g&gt; to Node &lt;192.168.0.88&gt;: idle &lt;cpu 2671.00, memory 48597196672.00, hugepages-1Gi 0.00, tencent.com/vcuda-core 100000.00, tencent.com/vcuda-memory 59000.00, hostdev.k8s.io/dev_fuse 256000.00, hostdev.k8s.io/dev_mem 256000.00, hugepages-2Mi 0.00&gt;, used &lt;cpu 15129.00, memory 30250037376.00&gt;, releasing &lt;cpu 0.00, memory 0.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771513       1 predicates.go:74] predicates, update pod project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g allocate to node [192.168.0.88]</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771533       1 nodeorder.go:186] node order, update pod project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g allocate to node [192.168.0.88]</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771578       1 preemptnodeorder.go:110] preempt node order, update pod project-1/notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g allocate to node [192.168.0.88]</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771586       1 statement.go:289] Allocating operations ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771602       1 statement.go:386] Committing operations ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771638       1 allocate.go:170] Namespace &lt;project-1&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771726       1 allocate.go:170] Namespace &lt;ti-base&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771804       1 allocate.go:170] Namespace &lt;ti-inf&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771871       1 allocate.go:170] Namespace &lt;timatrix&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771918       1 allocate.go:170] Namespace &lt;tione&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771932       1 allocate.go:300] Leaving Allocate ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.771939       1 backfill.go:41] Enter Backfill ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.772005       1 backfill.go:91] Leaving Backfill ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.772015       1 preempt.go:46] Enter Preempt ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.772055       1 preempt.go:92] No preemptors in Queue &lt;default&gt;, break.</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.772065       1 preempt.go:154] Leaving Preempt ...</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.787981       1 session.go:175] Close Session fd36a727-80af-4ec2-94a6-1ddf1ea1f9c0</span><br><span class="line"></span><br><span class="line">I1217 07:57:01.787999       1 scheduler.go:110] End scheduling ...</span><br></pre></td></tr></table></figure>

<p><strong>附：去除 binpack 之前的创建 1C1G notebook 示例的调度日志（注：此处日志级已修改为 level 4）</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">I1217 07:29:09.467598       1 scheduler.go:91] Start scheduling ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.468592       1 cache.go:823] There are &lt;146&gt; Jobs, &lt;1&gt; Queues and &lt;4&gt; Nodes in total for scheduling.</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.468673       1 session.go:157] Open Session e9d30d1d-5596-410f-b28e-eaf21574159d with &lt;4&gt; Node, &lt;146&gt; Job and &lt;1&gt; Queues</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483558       1 binpack.go:158] Enter binpack plugin ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483577       1 binpack.go:177] resources [nvidia.com/gpu] record in weight but not found on any node</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483589       1 binpack.go:161] Leaving binpack plugin. binpack.weight[10], binpack.cpu[1], binpack.memory[1], nvidia.com/gpu[5], tencent.com/vcuda-core[5] ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483597       1 enqueue.go:57] Enter Enqueue ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483603       1 enqueue.go:75] Added Queue &lt;default&gt; for Job &lt;project-1/podgroup-91ac6d92-4fea-4f1e-81f6-47401d4dbcc8&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483658       1 enqueue.go:91] Try to enqueue PodGroup to 0 Queues</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483674       1 enqueue.go:162] Leaving Enqueue ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483680       1 reserve.go:28] Enter Reserve ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483692       1 reserve.go:35] No target job, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483761       1 reserve.go:36] Leaving Reserve ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483768       1 allocate.go:45] Enter Allocate ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483924       1 allocate.go:115] unlockedNode ID: c01f9ad5-f285-462a-8261-d397c28c261a, Name: 192.168.0.2</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483935       1 allocate.go:115] unlockedNode ID: 5b5c2742-2344-44d3-98fc-52d01a0adb79, Name: 192.168.0.25</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483939       1 allocate.go:115] unlockedNode ID: b2c5cdcf-d201-41f8-85cd-4c88e838a6aa, Name: 192.168.0.43</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483944       1 allocate.go:115] unlockedNode ID: b2802217-f48a-4dab-b92a-6911ae7a2e7d, Name: 192.168.0.88</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.483960       1 allocate.go:170] Namespace &lt;cpaas-system&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484042       1 allocate.go:170] Namespace &lt;ns-1&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484067       1 allocate.go:209] Try to allocate resource to 1 tasks of Job &lt;project-1/podgroup-95dccc19-e4b1-4ccd-ae87-52545746996d&gt; in Namespace &lt;project-1&gt; Queue &lt;default&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484102       1 allocate.go:226] There are &lt;4&gt; nodes for Task &lt;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484140       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&gt; on node &lt;192.168.0.2&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 24095.00, memory 45479352318.00, hugepages-2Mi 0.00, hostdev.k8s.io/dev_fuse 256000.00, hostdev.k8s.io/dev_mem 255000.00, hugepages-1Gi 0.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484159       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&gt; on node &lt;192.168.0.25&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 18011.00, memory 38152880128.00, hostdev.k8s.io/dev_fuse 256000.00, hostdev.k8s.io/dev_mem 256000.00, hugepages-1Gi 0.00, hugepages-2Mi 0.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484208       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&gt; on node &lt;192.168.0.43&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 14604.00, memory 36533878784.00, hostdev.k8s.io/dev_mem 251000.00, hugepages-1Gi 0.00, hugepages-2Mi 0.00, hostdev.k8s.io/dev_fuse 256000.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484220       1 scheduler_helper.go:99] Considering Task &lt;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&gt; on node &lt;192.168.0.88&gt;: &lt;cpu 1000.00, memory 1073741824.00&gt; vs. &lt;cpu 4671.00, memory 50744680320.00, hostdev.k8s.io/dev_mem 256000.00, hugepages-2Mi 0.00, tencent.com/vcuda-core 100000.00, tencent.com/vcuda-memory 59000.00, hostdev.k8s.io/dev_fuse 256000.00, hugepages-1Gi 0.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484547       1 nodeorder.go:314] Total Score for task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.43 is: 193.000000, reason: il 0, ba 0, rf 100, br 93, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484550       1 nodeorder.go:314] Total Score for task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.2 is: 195.000000, reason: il 0, ba 0, rf 100, br 95, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484563       1 binpack.go:188] Binpack score for Task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.43 is: 638.0985332095383</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484565       1 nodeorder.go:314] Total Score for task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.88 is: 662.000000, reason: il 0, ba 25, rf 100, br 62, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484572       1 binpack.go:188] Binpack score for Task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.2 is: 494.11448190531286</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484578       1 binpack.go:188] Binpack score for Task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.88 is: 532.2531353036131</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484586       1 nodeorder.go:314] Total Score for task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.25 is: 199.000000, reason: il 0, ba 0, rf 100, br 99, na 0, sa 0,</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484602       1 binpack.go:188] Binpack score for Task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql on node 192.168.0.25 is: 594.3693530508443</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484725       1 nodeorder.go:403] inter pod affinity Score for task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql is: map[192.168.0.2:0 192.168.0.25:0 192.168.0.43:0 192.168.0.88:0]</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484811       1 nodeorder.go:455] taint toleration Score for task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql is: map[192.168.0.2:100 192.168.0.25:100 192.168.0.43:100 192.168.0.88:100]</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484819       1 nodeorder.go:349] Batch Total Score for task project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql is: map[192.168.0.2:100 192.168.0.25:100 192.168.0.43:100 192.168.0.88:100]</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484854       1 allocate.go:256] Binding Task &lt;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&gt; to node &lt;192.168.0.88&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484867       1 scheduler_binder.go:323] AssumePodVolumes for pod &quot;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&quot;, node &quot;192.168.0.88&quot;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484875       1 scheduler_binder.go:333] AssumePodVolumes for pod &quot;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&quot;, node &quot;192.168.0.88&quot;: all PVCs bound and nothing to do</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484895       1 statement.go:271] After allocated Task &lt;project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql&gt; to Node &lt;192.168.0.88&gt;: idle &lt;cpu 3671.00, memory 49670938496.00, tencent.com/vcuda-memory 59000.00, hostdev.k8s.io/dev_fuse 256000.00, hugepages-1Gi 0.00, hostdev.k8s.io/dev_mem 256000.00, hugepages-2Mi 0.00, tencent.com/vcuda-core 100000.00&gt;, used &lt;cpu 14129.00, memory 29176295552.00&gt;, releasing &lt;cpu 0.00, memory 0.00&gt;</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.484995       1 predicates.go:74] predicates, update pod project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql allocate to node [192.168.0.88]</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485014       1 nodeorder.go:186] node order, update pod project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql allocate to node [192.168.0.88]</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485032       1 preemptnodeorder.go:110] preempt node order, update pod project-1/notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql allocate to node [192.168.0.88]</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485035       1 statement.go:289] Allocating operations ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485049       1 statement.go:386] Committing operations ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485102       1 allocate.go:170] Namespace &lt;project-1&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485194       1 allocate.go:170] Namespace &lt;ti-base&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485267       1 allocate.go:170] Namespace &lt;ti-inf&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485339       1 allocate.go:170] Namespace &lt;timatrix&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485392       1 allocate.go:170] Namespace &lt;tione&gt; have no queue, skip it</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485403       1 allocate.go:300] Leaving Allocate ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485410       1 backfill.go:41] Enter Backfill ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485467       1 backfill.go:91] Leaving Backfill ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485497       1 preempt.go:46] Enter Preempt ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485545       1 preempt.go:92] No preemptors in Queue &lt;default&gt;, break.</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.485554       1 preempt.go:154] Leaving Preempt ...</span><br><span class="line"></span><br><span class="line">I1217 07:29:09.494793       1 session.go:175] Close Session e9d30d1d-5596-410f-b28e-eaf21574159d</span><br></pre></td></tr></table></figure>

<h4 id="通过创建业务-Pod-去验证"><a href="#通过创建业务-Pod-去验证" class="headerlink" title="通过创建业务 Pod 去验证"></a><strong>通过创建业务 Pod 去验证</strong></h4><p>  通过调整 notebook 副本数，增加 Pod 数量，观察 Pod 的在集群上的分布情况。该测试方法在机器少的环境对比效果不明显。</p>
<p>调整副本数量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@tke01 ~]# kubectl scale deploy -n project-1 notebook-1538aa66-da29-4866-80a7-598785e3e500 --replicas=10</span><br><span class="line">deployment.apps/notebook-1538aa66-da29-4866-80a7-598785e3e500 scaled</span><br></pre></td></tr></table></figure>

<p>查看 Pod 的分布情况，是否有尽量分布到不同的节点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@tke01 ~]# kubectl get pod -n project-1  -o wide  | grep notebook</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-8rg9g     2/2     Running           0          9m21s   10.199.10.177   192.168.0.88   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-bh28k     0/2     Init:0/1          0          3s      &lt;none&gt;          192.168.0.2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-bpfdj     0/2     Init:0/1          0          3s      &lt;none&gt;          192.168.0.25   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-c5sgv     0/2     Init:0/1          0          3s      &lt;none&gt;          192.168.0.25   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-dbcxm     2/2     Running           0          3s      10.199.10.189   192.168.0.88   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-jt97c     0/2     Init:0/1          0          3s      &lt;none&gt;          192.168.0.25   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-klh8n     0/2     PodInitializing   0          3s      10.199.10.192   192.168.0.88   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-mq5hf     0/2     Init:0/1          0          3s      10.199.10.191   192.168.0.2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-v68cq     0/2     Init:0/1          0          3s      &lt;none&gt;          192.168.0.25   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-1538aa66-da29-4866-80a7-598785e3e500-5895db877-xt4tx     0/2     Init:0/1          0          3s      &lt;none&gt;          192.168.0.25   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-788644f3-f7f1-4611-951e-6c5807bc7934-78465f59cf-5qxql    2/2     Running           0          37m     10.199.10.169   192.168.0.88   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">notebook-84e3ebf6-564e-42d8-aa7a-7c7d4c6ab1f6-6f76d587f9-mn87r    2/2     Running           0          4d4h    10.199.3.15     192.168.0.88   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2023/02/10/ServiceCatalog%E6%A6%82%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/10/ServiceCatalog%E6%A6%82%E5%BF%B5/" class="post-title-link" itemprop="url">ServiceCatalog 概念</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-10 20:11:25" itemprop="dateCreated datePublished" datetime="2023-02-10T20:11:25+08:00">2023-02-10</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>996</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>  服务目录（Service Catalog）可看作是一堆服务的集合，这些服务是由管理员对若干资源打包集成后结果。用户可以查阅这个服务目录获取到环境可被申请、使用的服务。通过调用申请服务按需获取自己需要的能力。其中用户不需要关心具体底层的实现以及其实现的过程。<br>  可参考的例子如公有云，管理员将底层资源池的资源进行抽象，将其整合为一个一个的服务，并在用户端体现一个可用的服务目录（列表），比如常见的云服务器、云硬盘、虚拟私有云等资源。用户访问公有云提供的服务目录，按需申请自己需要的资源。而用户不需要关心购买的服务对应的服务器是如何接入供电，如何上架到机房，如何部署上软件，只需要关注最终提供的可用的实例。<br>  那 Kubernetes 服务目录又是怎么样的东西？是为了解决什么场景的问题？实际上和上面公有云的例子进行关联，不难发现 Kubernetes 服务目录也是用于实现自助服务能力。可以先假设一个场景，用户业务运行在 Kubernetes 集群上，它们需要在 Kubernetes 集群使用数据库，需要集群管理员进行提供。这时候管理员可以怎么做？</p>
<ul>
<li><p>接收到用户的工单，然后管理员部署一套数据库集群，然后将数据库里面的一些鉴权、服务地址等信息提供给用户。用户将信息写入到 Secret 声明，然后在对应的 Pod 里面引用。</p>
</li>
<li><p>管理员提前抽象出数据库资源，实现按需部署，用户申请数据库资源，管理员审批后环境自动部署数据库实例。并将数据库的信息提供给用户。用户将信息写入到 Secret 声明，然后在对应的 Pod 里面引用。</p>
<p>从上面的实现方式来看，如果管理员实现了自助服务会简化一定程度上的工作。减少运维人员的工作量。我们可以基于Kubernetes ServiceCatalog 的功能去构建这个能力，将公有云上的服务、或者基于 Kubernetes 集群部署的服务，又或者是机房某个物理机上部署的服务，通过 Service Catalog 以服务代理的形式，将资源封装成服务用来提供给用户自助服务。</p>
</li>
</ul>
<p><code>笔者在编写到一半的时候，发现此项目已经不属于孵化项目，变成退休项目了。相关代码不再维护，镜像、chart 也无法被获取。导致没有办法继续编写下面的内容。。。不过 ServiceCatalog 这种提供自主服务集成的能力，对于某些场景下实际上还是很有用的。待后续了解其他替代方案再进行沉淀记录。</code></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2022/12/19/Kafka%20%E8%BF%90%E7%BB%B4%E6%8C%87%E4%BB%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/12/19/Kafka%20%E8%BF%90%E7%BB%B4%E6%8C%87%E4%BB%A4/" class="post-title-link" itemprop="url">Kafka 运维指令</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-12-19 20:11:25" itemprop="dateCreated datePublished" datetime="2022-12-19T20:11:25+08:00">2022-12-19</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h1><p>  本文主要介绍一些 kafka 常见的操作指令。并指导如何在 TKE 的场景下，对 kafka 进行操作。</p>
<h1 id="运维指令"><a href="#运维指令" class="headerlink" title="运维指令"></a><strong>运维指令</strong></h1><h2 id="Topic-类"><a href="#Topic-类" class="headerlink" title="Topic 类"></a><strong>Topic 类</strong></h2><h3 id="列出-topic"><a href="#列出-topic" class="headerlink" title="列出 topic"></a><strong>列出 topic</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --list --exclude-internal --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="查看-topic-详情"><a href="#查看-topic-详情" class="headerlink" title="查看 topic 详情"></a><strong>查看 topic 详情</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --describe --topic topicname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="创建-topic"><a href="#创建-topic" class="headerlink" title="创建 topic"></a><strong>创建 topic</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --create --topic topicname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="删除指定-topic"><a href="#删除指定-topic" class="headerlink" title="删除指定 topic"></a>删除指定 topic</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --delete --topic topicname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h2 id="Consumer-groups-类"><a href="#Consumer-groups-类" class="headerlink" title="Consumer groups 类"></a><strong>Consumer groups 类</strong></h2><h3 id="列出-consumer-groups"><a href="#列出-consumer-groups" class="headerlink" title="列出 consumer groups"></a><strong>列出 consumer groups</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --list --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="查看-consumer-group-详细信息"><a href="#查看-consumer-group-详细信息" class="headerlink" title="查看 consumer group 详细信息"></a><strong>查看 consumer group 详细信息</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --describe --group groupname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="删除指定-consumer-group"><a href="#删除指定-consumer-group" class="headerlink" title="删除指定 consumer group"></a><strong>删除指定 consumer group</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092  --delete --group groupname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h2 id="生产与消费类"><a href="#生产与消费类" class="headerlink" title="生产与消费类"></a><strong>生产与消费类</strong></h2><h3 id="生产"><a href="#生产" class="headerlink" title="生产"></a><strong>生产</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-console-producer.sh --broker-list kf1:9092,kf2:9092,kf3:9092 --topic topicname --producer.config /opt/kafka_2.12-2.2.1/config/producer.properties</span><br></pre></td></tr></table></figure>

<h3 id="消费"><a href="#消费" class="headerlink" title="消费"></a><strong>消费</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --topic topicname --from-beginning --consumer.config /opt/kafka_2.12-2.2.1/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h2 id="查看kafka数据保存时间"><a href="#查看kafka数据保存时间" class="headerlink" title="查看kafka数据保存时间"></a><strong>查看kafka数据保存时间</strong></h2><p>查看topic数据清理策略：</p>
<p><code>cat /opt/kafka/config/server.properties</code>  查看 <code>log.retention.hours</code> 参数，这个参数含义是kafka数据日志的保存时间，</p>
<h2 id="重新消费topic全部数据"><a href="#重新消费topic全部数据" class="headerlink" title="重新消费topic全部数据"></a><strong>重新消费topic全部数据</strong></h2><p>场景：需要重新消费一个topic里面的数据</p>
<p>消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）：</p>
<p>（1）使用一个全新的”group.id”（就是之前没有被任何消费者使用过）;</p>
<p>（2）指定”auto.offset.reset”参数的值为earliest；</p>
<p>修改 <code>/opt/kafka/config/consumer.properties</code> 里面的<code>group.id</code>和<code>auto.offset.reset</code></p>
<p>把<a target="_blank" rel="noopener" href="http://group.id/"><u><span class="15">group.id</span></u></a>值改成一个新的值,<code>auto.offset.reset</code> 值改成 <code>earliest</code>;</p>
<p>执行消费命令消费topic就可以重头开始消费数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --topic topicname --from-beginning --consumer.config /opt/kafka_2.12-2.2.1/config/consumer.properties</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">Redis响应慢导致接口超时问题分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-10 17:11:25" itemprop="dateCreated datePublished" datetime="2022-11-10T17:11:25+08:00">2022-11-10</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h1><p>  业务分析视频时，发现人物标签丢失图片。经研发定位发现业务的 CreateDownloadURL 接口会出现超时的问题，而该接口主要是使用了 Redis 服务，怀疑是 Redis 服务存在故障导致业务异常。</p>
<img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/1.jpg" class="" title="1.jpg">

<h1 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h1><p>  通过查看平台 Redis 监控，发现 Redis 某个实例存在大量的慢查询记录（1K）。怀疑该 Redis 实例存在问题。</p>
<img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/2.jpg" class="" title="2.jpg">

<p>  登录到该 Redis 实例，通过 slowlog get 指令，获取当前的慢查询记录，通过查询发现基本所有的慢查询记录都是查询 某个 key 触发的。并且查询的方式是 keys 指令（复杂度较高，指令执行时间较长）。</p>
<p>  通过 dbsize 查询，发现当前 Redis 集群的键数量已经达到 96W。在这个数量下执行 keys 类的指令（复杂度高），会拖慢 Redis 的处理效率（ Redis 单线程处理，其他指令会阻塞）。导致 Redis 性能下降，这样调用 Redis 的服务性能就会下降。出现异常。</p>
<img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/3.jpg" class="" title="3.jpg">

<p>  此时需要理清楚为什么集群中存在大量的 Key，以及某个 Key 存在大量查询的原因。</p>
<p>  某个 key 产生大量查询的原因。经咨询是研发的代码逻辑缺陷，查询方式使用了 keys 并且查询间隔非常短（毫秒级的轮询）而环境存在大量的 key，使用这种方式就会一直产生慢查询记录，影响 Redis 性能。该问题需要研发修复。</p>
<p>  存在大量 Key 的原因，需要先查询有哪些 key，研发侧怀疑是某个服务存量的 key 数量会比较多。因为当前该 key 为了满足下载有效期的时间，配置了 30 天的有效期。而近期环境一直在执行压测作业。集群可能会积压该服务的 key。</p>
<p>  通过查询，该服务的 key 的数量达到了 95W。也就是集群中基本上所有 key 都是由该服务产生的。这也符合了我们的推测，由于近期压测次数较多产生了大量的 key，key 的 TTL 过长导致集群积压大量 key，redis 性能下降导致服务超时。对于这个服务的处理，当前的策略是先调整 TTL 为 1 天，避免集群产生太多 Key。</p>
<img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/4.jpg" class="" title="4.jpg">

<p>  当前需要核实 key 数量对 Redis 实例性能的影响，需要先删除掉该服务相关的 key。执行如下命令清理：</p>
<p>清理掉所有 abc:token: 开头的 key</p>
<p>redis-cli -a 密码 keys “abc:token:*” | xargs redis-cli -a 密码 del</p>
<p>  清理完成以后，经研发反馈，性能恢复正常，接口响应变得非常快。</p>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><h2 id="同等配置下，如果用-Keys-方式查询，集群-Key-达到多少会开始产生慢查询记录？"><a href="#同等配置下，如果用-Keys-方式查询，集群-Key-达到多少会开始产生慢查询记录？" class="headerlink" title="同等配置下，如果用 Keys 方式查询，集群 Key 达到多少会开始产生慢查询记录？"></a>同等配置下，如果用 Keys 方式查询，集群 Key 达到多少会开始产生慢查询记录？</h2><p>  通过 redis-benchmark 写入 key，大约 8W 左右就会开始产生慢查询记录，能说明 keys 这种方式非常不适用于生产环境。应该要严格限制业务侧去使用 keys 等方式去查询 redis 上面的数据。</p>
<h2 id="Redis-后续可改进点？"><a href="#Redis-后续可改进点？" class="headerlink" title="Redis 后续可改进点？"></a>Redis 后续可改进点？</h2><p>  需要把监控报警能力覆盖进去，通过告警通知就可以快速获知这个现象，减少一些人工判断的动作。</p>
<p>  需要在 CI 的代码检测阶段引入相关的 keys 检测，及时提醒用户修正使用方法。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2022/09/10/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%9F%90%E8%BF%9B%E7%A8%8B%E5%BB%BA%E7%AB%8B%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/09/10/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%9F%90%E8%BF%9B%E7%A8%8B%E5%BB%BA%E7%AB%8B%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/" class="post-title-link" itemprop="url">如何查看某进程建立的连接信息</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-10 20:11:25" itemprop="dateCreated datePublished" datetime="2022-09-10T20:11:25+08:00">2022-09-10</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>  有些场景下，我们需要了解某个进程正在和哪些服务建立连接。本文主要是记录了捕获进程连接信息的几个实现方法。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="ss-指令"><a href="#ss-指令" class="headerlink" title="ss 指令"></a>ss 指令</h3><p>最先想到的方法是通过 netstat 或者 ss 的工具去持续查看机器的连接建立情况，并查看 PID 或者进程名称，参见如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取 sshd 进程的连接情况</span></span><br><span class="line">[root@10 ~]# ss -t  -p | grep sshd</span><br><span class="line">ESTAB      0      0      10.0.2.15:ssh                  10.0.2.15:42700                 users:((&quot;sshd&quot;,pid=2615,fd=3))</span><br><span class="line">ESTAB      0      0      192.168.214.254:ssh                  192.168.214.1:56070                 users:((&quot;sshd&quot;,pid=2863,fd=3))</span><br><span class="line">ESTAB      0      36     192.168.214.254:ssh                  192.168.214.1:56068                 users:((&quot;sshd&quot;,pid=2859,fd=3))考虑</span><br></pre></td></tr></table></figure>

<p>考虑到进程的连接信息是会持续变化的，可通过 while 的方法循环去查看某个进程的连接情况，实现类似 top 的效果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">while true ; do clear ; ss -t  -p | grep sshd ; sleep 1; done</span><br></pre></td></tr></table></figure>

<h3 id="audit-日志"><a href="#audit-日志" class="headerlink" title="audit 日志"></a>audit 日志</h3><p>  ss 指令的局限在于它没办法回溯历史，你只能查看到当前正在建立的连接信息。但是某些场景下，我们可能需要知道某个进程曾经和哪些机器建立过。这时候就可以借助 audit 日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@10 ~]# tail /var/log/audit/audit.log</span><br><span class="line">type=CRYPTO_KEY_USER msg=audit(1703823116.926:813): pid=2872 uid=0 auid=0 ses=89 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg=&#x27;op=destroy kind=server fp=SHA256:83:79:c8:32:03:bd:25:7b:54:54:f7:b9:99:2b:3d:c4:0d:d9:55:ed:6c:11:a2:8f:e9:b8:ba:0f:4c:18:8c:6f direction=? spid=2872 suid=0  exe=&quot;/usr/sbin/sshd&quot; hostname=10.0.2.15 addr=? terminal=pts/1 res=success&#x27;</span><br></pre></td></tr></table></figure>

<p>  可以看到 audit 的审计是可以记录进程的相关信息，此处记录了进程和谁互联。当然 audit 使用的前提是系统打开了该功能。</p>
<h3 id="perf-工具"><a href="#perf-工具" class="headerlink" title="perf 工具"></a>perf 工具</h3><p>  perf 工具可以用来追踪特定进程建立连接的信息，参考如下方法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">捕获信息</span></span><br><span class="line">perf record -e &#x27;syscalls:sys_enter_connect&#x27; -p &lt;进程名称&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看捕获结果</span></span><br><span class="line">perf report</span><br></pre></td></tr></table></figure>

<h3 id="lsof-工具"><a href="#lsof-工具" class="headerlink" title="lsof 工具"></a>lsof 工具</h3><p>  lsof 工具可以列出指定进程正在和谁建立连接，和 ss 类似</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2615 是 PID，根据实际情况调整</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">也可以用 -c 参数替代，-c 参数为追踪的 <span class="built_in">command</span></span></span><br><span class="line">[root@10 ~]# lsof -i -a -p 2615</span><br><span class="line">COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</span><br><span class="line">sshd    2615 root    3u  IPv4  32428      0t0  TCP 10.0.2.15:ssh-&gt;10.0.2.15:42700 (ESTABLISHED)</span><br></pre></td></tr></table></figure>

<p>  同样可以通过 while 循环实现持续查看状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">while true ; do clear ; lsof -i -a -p 2615 ; sleep 1; done</span><br></pre></td></tr></table></figure>

<h3 id="strace-工具"><a href="#strace-工具" class="headerlink" title="strace 工具"></a>strace 工具</h3><p>  strace 工具是非常强大的链路追踪工具，可以通过 strace 对运行中的进程或者某个命令的执行过程进行追踪。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 trace=network 可以列出网络相关的追踪信息</span></span><br><span class="line">[root@10 ~]# strace -e trace=network curl www.baidu.com</span><br><span class="line">socket(AF_INET6, SOCK_DGRAM, IPPROTO_IP) = 3</span><br><span class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0</span><br><span class="line">setsockopt(3, SOL_TCP, TCP_KEEPIDLE, [60], 4) = 0</span><br><span class="line">setsockopt(3, SOL_TCP, TCP_KEEPINTVL, [60], 4) = 0</span><br><span class="line">connect(3, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(&quot;120.232.145.144&quot;)&#125;, 16) = -1 EINPROGRESS (Operation now in progress)</span><br><span class="line">getsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0</span><br><span class="line">getpeername(3, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(&quot;120.232.145.144&quot;)&#125;, [128-&gt;16]) = 0</span><br><span class="line">getsockname(3, &#123;sa_family=AF_INET, sin_port=htons(43260), sin_addr=inet_addr(&quot;10.0.2.15&quot;)&#125;, [128-&gt;16]) = 0</span><br><span class="line">sendto(3, &quot;GET / HTTP/1.1\r\nUser-Agent: curl&quot;..., 77, MSG_NOSIGNAL, NULL, 0) = 77</span><br><span class="line">recvfrom(3, &quot;HTTP/1.1 200 OK\r\nAccept-Ranges: &quot;..., 16384, 0, NULL, NULL) = 2781</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class=&quot;bg s_ipt_wr&quot;&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus&gt;&lt;/span&gt;&lt;span class=&quot;bg s_btn_wr&quot;&gt;&lt;input type=submit id=su value=百度一下 class=&quot;bg s_btn&quot;&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;新闻&lt;/a&gt; &lt;a href=http://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;地图&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;视频&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;贴吧&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;登录&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write(&#x27;&lt;a href=&quot;http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=&#x27;+ encodeURIComponent(window.location.href+ (window.location.search === &quot;&quot; ? &quot;?&quot; : &quot;&amp;&quot;)+ &quot;bdorz_come=1&quot;)+ &#x27;&quot; name=&quot;tj_login&quot; class=&quot;lb&quot;&gt;登录&lt;/a&gt;&#x27;);&lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style=&quot;display: block;&quot;&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;关于百度&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;©2017 Baidu &lt;a href=http://www.baidu.com/duty/&gt;使用百度前必读&lt;/a&gt;  &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt; 京ICP证030173号  &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;</span><br><span class="line">+++ exited with 0 +++</span><br></pre></td></tr></table></figure>

<p>  另一个示例，使用 strace 追踪已运行的进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# strace -p 903 -e trace=network</span><br><span class="line">strace: Process 903 attached</span><br><span class="line">--- SIGCHLD &#123;si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=18908, si_uid=0, si_status=255, si_utime=0, si_stime=0&#125; ---</span><br><span class="line">accept(3, &#123;sa_family=AF_INET, sin_port=htons(34004), sin_addr=inet_addr(&quot;192.168.214.254&quot;)&#125;, [128-&gt;16]) = 5</span><br><span class="line">socketpair(AF_UNIX, SOCK_STREAM, 0, [8, 9]) = 0</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>  实际上可使用的方法不止上文提及的几种手段，也可以通过 ebpf 等方式来实现进程连接信息的捕获。本文仅列出了常见的集中可用手段。</p>
<p>  最后对这几个方法进行简单的小结，见下表</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>示例</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>ss 指令</td>
<td>ss -t  -p | grep &lt;进程ID&gt;</td>
<td>需要借助 while 的方法实现实时查看<br>精确度不能完全保证，查询之间存在间隔<br>不能查看历史的连接信息<br>系统默认会有 ss 指令，不需要额外去安装</td>
</tr>
<tr>
<td>audit 日志</td>
<td>tail &#x2F;var&#x2F;log&#x2F;audit&#x2F;audit.log</td>
<td>可查阅历史连接信息<br>依赖环境打开 audit 功能</td>
</tr>
<tr>
<td>perf 工具</td>
<td>perf record -e ‘syscalls:sys_enter_connect’ -p &lt;进程ID&gt;</td>
<td>精确度有保障，可实时捕获数据并写入到一个结果文件，便于事后分析。<br>同样的不支持查看历史的连接信息</td>
</tr>
<tr>
<td>lsof 工具</td>
<td>lsof -i -a -p &lt;进程ID&gt;</td>
<td>基本同 ss 指令</td>
</tr>
<tr>
<td>strace 工具</td>
<td>strace -e trace&#x3D;network -p &lt;进程ID&gt;</td>
<td>精确度有保障，可实时捕获数据<br>支持单条命令调试，也支持加载现有进程。<br>同样的不支持查看历史的连接信息。</td>
</tr>
</tbody></table>
<p> 基于上述的汇总，可以看出这些指令直接是可以取合集做一个工具实现更好的信息捕获的。后续可以实现一个自动化的工具，加入到 orca-tools 里面。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2022/08/23/%E6%8E%92%E6%9F%A5%20kubectl%20edit%20%E4%BF%9D%E5%AD%98%E5%A4%B1%E8%B4%A5%E6%80%9D%E8%B7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/08/23/%E6%8E%92%E6%9F%A5%20kubectl%20edit%20%E4%BF%9D%E5%AD%98%E5%A4%B1%E8%B4%A5%E6%80%9D%E8%B7%AF/" class="post-title-link" itemprop="url">排查 kubectl edit 保存失败方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-08-23 20:11:25" itemprop="dateCreated datePublished" datetime="2022-08-23T20:11:25+08:00">2022-08-23</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-常见原因"><a href="#1-常见原因" class="headerlink" title="1. 常见原因"></a><strong>1. 常见原因</strong></h2><p>保存失败的常见原因如下：</p>
<ul>
<li><p>使用 tab 产生空白字符。</p>
</li>
<li><p>使用了无效的选项、参数等。</p>
</li>
<li><p>value 未使用字符串。</p>
</li>
<li><p>环境的 kubernetes 配置了 webhook，提交 YAML 修改到 apiserver 以后需要完成校验，如果校验失败或 webhook 的组件故障，此时 kubectl edit 保存会失败。</p>
</li>
</ul>
<h2 id="2-排查思路"><a href="#2-排查思路" class="headerlink" title="2. 排查思路"></a><strong>2. 排查思路</strong></h2><p>  通过 kubectl edit 保存配置时，如果使用了无效的参数或者语法错误。将会打印相印的错误提示，错误提示的行首有 # 的字符。根据这些提示即可找到故障原因。</p>
<p>  另一种问题是，保存 YAML 时未产生错误提示，但是保存后产生了类似如下的错误提示 dial tcp: lookup ti-resource-server.ti-base.svc.cluster.local on [::1]:53: dial udp [::1]:53: connect: network is unreachable。这种场景通常是因为 webhook 组件故障引起的。需要解决 webhook 组件故障。</p>
<h2 id="3-常见错误原因"><a href="#3-常见错误原因" class="headerlink" title="3. 常见错误原因"></a><strong>3. 常见错误原因</strong></h2><h3 id="3-1-did-not-find-expected-key"><a href="#3-1-did-not-find-expected-key" class="headerlink" title="3.1 did not find expected key"></a><strong>3.1 did not find expected key</strong></h3><p>  如果看见 did not find expected key 的错误，通常是空格数量不正确导致的。上下级选项需要保持两个空格。</p>
<h3 id="3-2-The-edited-file-failed-validation"><a href="#3-2-The-edited-file-failed-validation" class="headerlink" title="3.2 The edited file failed validation"></a><strong>3.2 The edited file failed validation</strong></h3><p>  如果看见类似 invalid value: “The edited file failed validation”: ValidationError(Deployment.spec): unknown field “12345” in io.k8s.api.apps.v1.DeploymentSpec 的错误，通常是使用了错误的选项。可以通过 kubectl explain 资源.spec.选项名称 来获取有效合法的 key、values 值。</p>
<h3 id="3-3-cannot-convert-int64-to-string"><a href="#3-3-cannot-convert-int64-to-string" class="headerlink" title="3.3 cannot convert int64 to string"></a><strong>3.3 cannot convert int64 to string</strong></h3><p>  未使用”” 将整型数据括起来。常见于添加 nodeSelector 未给 value 配置 “”。</p>
<h3 id="3-4-no-matches-for-kind-“xxxx”-in-version-“xxxx”"><a href="#3-4-no-matches-for-kind-“xxxx”-in-version-“xxxx”" class="headerlink" title="3.4 no matches for kind “xxxx” in version “xxxx”"></a><strong>3.4 no matches for kind “xxxx” in version “xxxx”</strong></h3><p>  如果看见类似 The edited file had a syntax error: unable to recognize “edited-file”: no matches for kind “Deployment” in version “extension&#x2F;v1beta1” 的错误，通常是 YAML 编辑的资源使用了错误的 apiVersion 导致的。同样可以通过 kubectl explain 指令来获取对应资源的 apiVersion 可用版本。</p>
<h3 id="3-5-provided-port-is-not-in-the-valid-range-The-range-of-valid-ports-is-xxxx-xxxx"><a href="#3-5-provided-port-is-not-in-the-valid-range-The-range-of-valid-ports-is-xxxx-xxxx" class="headerlink" title="3.5 provided port is not in the valid range. The range of valid ports is xxxx-xxxx"></a><strong>3.5 provided port is not in the valid range. The range of valid ports is xxxx-xxxx</strong></h3><p>  只存在于 service 修改的场景，如果看见类似于 * spec.ports[0].nodePort: Invalid value: 10508: provided port is not in the valid range. The range of valid ports is 30000-32767 的操作，代表修改 service 配置的 NodePort 超出了 kube-apiserver 中 –service-node-port-range 定义的范围，这个范围默认值是 30000-32767，也就是说 NodePort 配置的端口不能超出这个范围。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2022/02/01/Pod%20%E5%86%85%E6%97%A0%E6%B3%95%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/01/Pod%20%E5%86%85%E6%97%A0%E6%B3%95%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF/" class="post-title-link" itemprop="url">Pod 内无法解析域名排查思路</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-01 20:01:15" itemprop="dateCreated datePublished" datetime="2022-02-01T20:01:15+08:00">2022-02-01</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h1><h2 id="Pod-内域名解析失败的现象"><a href="#Pod-内域名解析失败的现象" class="headerlink" title="Pod 内域名解析失败的现象"></a><strong>Pod 内域名解析失败的现象</strong></h2><p>  域名解析失败的现象，通过 ping、curl 或者服务日志中，可以看到域名解析失败的错误，关键字是 Could not resolve host 或者 Name or service not known。如下图所示</p>




<p>  可能故障的原因如下：</p>
<ul>
<li><p>客户端请求了错误的域名。</p>
</li>
<li><p>客户端未配置正确的 DNS 服务器。</p>
</li>
<li><p>coredns 服务故障。</p>
</li>
<li><p>网络插件存在异常，Pod 无法通过 kube-dns 这个服务访问 coredns。</p>
</li>
<li><p>客户端请求解析外部域名，coredns 无法连通外部 DNS 服务器，导致解析失败。</p>
</li>
</ul>
<h2 id="域名解析请求图"><a href="#域名解析请求图" class="headerlink" title="域名解析请求图"></a><strong>域名解析请求图</strong></h2>

<p>  客户端在请求域名解析前会先去查询 &#x2F;etc&#x2F;hosts 中是否有配置域名解析记录。此处可以通过在 Pod.Spec 中使用 hostAliases 参数来添加。如果 &#x2F;etc&#x2F;hosts 没有解析记录。它会再通过 &#x2F;etc&#x2F;resolv.conf 里面配置的参数，将域名解析请求转发给指定的服务器。使用非 HostNetwork 模式的 Pod 会将请求转发给 coredns。</p>
<p>  默认情况下，coredns 只存在 cluster.local 域的解析记录。默认情况下，每条解析记录都是以 <strong>服务名称.命名空间名称.svc.cluster.local</strong> 的形式存在的。对于非 cluster.local 域的解析请求，会通过 forward 的配置参数进行转发。默认情况下，会将请求转发到容器内配置的 &#x2F;etc&#x2F;resolv.conf 中指定的 nameserver。</p>
<h1 id="Pod-内无法解析域名的排查思路"><a href="#Pod-内无法解析域名的排查思路" class="headerlink" title="Pod 内无法解析域名的排查思路"></a><strong>Pod 内无法解析域名的排查思路</strong></h1><p>  排查场景分为内部服务域名解析和外部服务域名解析，内部服务域名指的是 kubernetes 内部 service 的域名，他的完整域名形式通常为 <strong>服务名称.命名空间名称.svc.cluster.local</strong>，如：mysql.inf.svc.cluster.local。而外部域名则是非 kubernetes 内部 service 的域名，例如：<a target="_blank" rel="noopener" href="http://api.weixin.qq.com/"><u><span class="16">api.weixin.qq.com</span></u></a>、redcs.tencent.com等等。</p>
<p>  如果是内部服务域名解析的问题，需要优先确认一下客户端请求的域名是否存在，coredns 服务是否正常，pod 和 coredns 之间的服务连通性是否正常。</p>
<p>  如果是外部服务域名解析的问题，基于内部服务域名解析的排查项还需要确认一下 coredns 和外部 DNS 之间的连通性是否正常，是否配置了正确的外部 DNS。</p>
<h2 id="内部服务域名解析失败排查"><a href="#内部服务域名解析失败排查" class="headerlink" title="内部服务域名解析失败排查"></a><strong>内部服务域名解析失败排查</strong></h2><h3 id="确认-Pod-内的-etc-resolv-conf-配置"><a href="#确认-Pod-内的-etc-resolv-conf-配置" class="headerlink" title="确认 Pod 内的 &#x2F;etc&#x2F;resolv.conf 配置"></a><strong>确认 Pod 内的 &#x2F;etc&#x2F;resolv.conf 配置</strong></h3><p>  <strong>通过 kubectl 指令进入容器，查询 &#x2F;etc&#x2F;resolv.conf 配置</strong></p>
<p>    可以通过 kubectl exec -it -n 命名空间 Pod名称 &#x2F;bin&#x2F;sh 的指令，进入的容器中，然后通过 cat &#x2F;etc&#x2F;resolv.conf 的指令获取 DNS 服务器配置。如果输出的内容如下示例，代表该容器的域名解析会提交给 coredns。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver 10.96.0.10</span><br><span class="line">search ti-inf.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>

<p>    如果输出的内容如下示例，说明该容器可能是直接使用的宿主机 &#x2F;etc&#x2F;resolv.conf 配置，容器可能是 hostNetwork 模式的。这个时候他请求域名解析的记录不会发送到 coredns。<strong>如果直接请求 kubernetes 内部服务名称，则会失败。</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nameserver 183.60.83.19</span><br><span class="line">nameserver 183.60.82.98</span><br></pre></td></tr></table></figure>

<p>  <strong>通过登录 Pod 宿主机，查看容器工作目录的方式查询 &#x2F;etc&#x2F;resolv.conf 的配置</strong></p>
<p>    某些容器内部无法使用 &#x2F;bin&#x2F;sh 或者 &#x2F;bin&#x2F;bash，这个时候如果要确认 resolv.conf 的配置，可以通过登录宿主机，然后执行 <code>docker ps -a | grep Pod 名称</code>，获取容器的 ID，再通过 <code>docker inspect 容器ID -f &#39;&#123;&#123;.ResolvConfPath&#125;&#125;&#39;</code> 的指令获取当前 Pod resolv.conf 在宿主机上的路径。期望的获取结果如下示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/docker/containers/a9691be1de0edd7d8edaf83b046e84594486ffbdd9bfb0a113d51d986830fbb1/resolv.conf</span><br></pre></td></tr></table></figure>

<h3 id="确认-coredns-服务是否正常"><a href="#确认-coredns-服务是否正常" class="headerlink" title="确认 coredns 服务是否正常"></a><strong>确认 coredns 服务是否正常</strong></h3><p>  通过 <strong>kubectl get pod -n kube-system -o wide | grep coredns</strong> 的指令获取当前 coredns 的信息，期望输出内容如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment"># kubectl get pod -n kube-system -o wide | grep coredns</span></span><br><span class="line">coredns-6d8cbdcf64-gkhcg               1/1     Running   0          14d   10.199.0.7     172.27.0.9                </span><br><span class="line">coredns-6d8cbdcf64-ht2vz               1/1     Running   0          14d   10.199.0.8     172.27.0.9</span><br></pre></td></tr></table></figure>

<p>  如果 coredns 状态为非 Running，代表 coredns 服务未正常允许，可以参考 Pod 排查指引来处理。</p>
<h3 id="确认内部域名是否合法"><a href="#确认内部域名是否合法" class="headerlink" title="确认内部域名是否合法"></a><strong>确认内部域名是否合法</strong></h3><p>  合法的 kubernetes 内部域名应该可以通过 <strong>kubectl get svc -n 命名空间 服务名称</strong> 来获取到的。比如说 ti-mysql.ti-inf.svc.cluster.local. 这个域名对应的服务应该是能够通过 kubectl get svc -n ti-inf ti-mysql 的指令获取到。</p>
<h3 id="收集-coredns-服务的日志，查看解析记录"><a href="#收集-coredns-服务的日志，查看解析记录" class="headerlink" title="收集 coredns 服务的日志，查看解析记录"></a><strong>收集 coredns 服务的日志，查看解析记录</strong></h3><p>  通过 kubectl logs -n kube-system coredns-xxxx-xxx 的指令可以获取 coredns 的日志输出，如果域名无法解析，需要转发给上级 DNS 服务器，类似输出的内容如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:46205-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:56372-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:36434-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. AAAA: <span class="built_in">read</span> udp 10.199.0.8:46337-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. AAAA: <span class="built_in">read</span> udp 10.199.0.8:41195-&gt;183.60.82.98:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. AAAA: <span class="built_in">read</span> udp 10.199.0.8:51501-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:54489-&gt;183.60.82.98:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:51802-&gt;183.60.82.98:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:48109-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:36779-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br></pre></td></tr></table></figure>

<p>  如果内部域名解析请求需要提交给上级 DNS 服务器来处理，则可能是该域名不合法，在环境中应该不存在对应的 service。如果在 coredns 未能查询到相关的日志，需要通过抓包确认流量是否已经到达 coredns。</p>
<h3 id="抓取-coredns-的报文，查看请求域名解析的流量是否已经到达"><a href="#抓取-coredns-的报文，查看请求域名解析的流量是否已经到达" class="headerlink" title="抓取 coredns 的报文，查看请求域名解析的流量是否已经到达"></a><strong>抓取 coredns 的报文，查看请求域名解析的流量是否已经到达</strong></h3><p>  通过如下步骤抓取报文，查看报文中是否有相应域名解析的记录。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1. 确认 Pod 当前运行节点</span><br><span class="line">  kubectl get pod -n kube-system -o wide | grep coredns</span><br><span class="line"></span><br><span class="line">2. 登录到 coredns 所在的节点，查找 coredns 对应的容器 ID</span><br><span class="line"> docker ps -a | grep coredns</span><br><span class="line"></span><br><span class="line">3. 获取容器的 PID</span><br><span class="line">  docker inspect -f &#123;&#123;.State.Pid&#125;&#125;</span><br><span class="line"></span><br><span class="line">4. 进入容器的网络命名空间</span><br><span class="line">  nsenter -t PID -n</span><br><span class="line"></span><br><span class="line">5. 抓取报文</span><br><span class="line">  tcpdump -i eth0</span><br></pre></td></tr></table></figure>

<h3 id="确认-coredns-配置，是否使用了-hosts-插件，但未配置-fallthrough"><a href="#确认-coredns-配置，是否使用了-hosts-插件，但未配置-fallthrough" class="headerlink" title="确认 coredns 配置，是否使用了 hosts 插件，但未配置 fallthrough"></a><strong>确认 coredns 配置，是否使用了 hosts 插件，但未配置 fallthrough</strong></h3><p>  coredns 默认情况下启用了多个插件，请求被当前插件以 Fallthrough 的形式处理，如果请求在该插件处理过程中有可能会跳转到下一个插件，这个过程被称为 fallthrough。根据 fallthrough 配置来控制插件是否能够跳转到下一个插件，比如启用了 hosts 插件，但是 hosts 解析域名失败，如果不配置 fallthrough，它将不会继续跳转到下一个插件。导致解析失败。</p>
<p>  于是乎我们还可以检查下 coredns 是否出现配置了 hosts 插件，但是未启用 fallthrough。参考如下配置</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    .:53 &#123;</span></span><br><span class="line"><span class="string">        errors</span></span><br><span class="line"><span class="string">        health</span></span><br><span class="line"><span class="string">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span></span><br><span class="line"><span class="string">           pods insecure</span></span><br><span class="line"><span class="string">           upstream</span></span><br><span class="line"><span class="string">           fallthrough in-addr.arpa ip6.arpa</span></span><br><span class="line"><span class="string">           ttl 30</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        hosts tke.cn &#123; #此处代表使用了 hosts 插件</span></span><br><span class="line"><span class="string">          172.27.0.123 a.tke.cn</span></span><br><span class="line"><span class="string">          fallthrough  # 观察此处是否配置了 fallthrough，如果启用 hosts 插件但是未配置该选项，会导致业务解析功能异常</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        prometheus :9153</span></span><br><span class="line"><span class="string">        forward . /etc/resolv.conf</span></span><br><span class="line"><span class="string">        cache 30</span></span><br><span class="line"><span class="string">        loop</span></span><br><span class="line"><span class="string">        reload</span></span><br><span class="line"><span class="string">        loadbalance</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br></pre></td></tr></table></figure>

<h2 id="外部服务域名解析失败排查"><a href="#外部服务域名解析失败排查" class="headerlink" title="外部服务域名解析失败排查"></a><strong>外部服务域名解析失败排查</strong></h2><h3 id="确认上级转发-DNS-的配置是否正确"><a href="#确认上级转发-DNS-的配置是否正确" class="headerlink" title="确认上级转发 DNS 的配置是否正确"></a><strong>确认上级转发 DNS 的配置是否正确</strong></h3><p>  通过 2.1.4 的步骤获取到 coredns 的服务日志时，如果刚好看到类似 read udp 10.199.0.8:46205-&gt;183.60.83.19:53: i&#x2F;o timeout 的错误，此时上级 DNS 就是 183.60.83.19。这种错误通常可能的原因是无法连通上级 DNS。现场可以根据实际环境对比一下日志打印的上级 DNS 服务器的地址是否是预期的地址。</p>
<p>  上级 DNS 是通过 coredns 中指定 forward 配置项来设置的，通过 kubectl get cm -n kube-system coredns -o yaml 可以查看 coredns 的配置，示例输出如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">.:53</span> &#123;</span><br><span class="line">    <span class="string">errors</span></span><br><span class="line">    <span class="string">health</span> &#123;</span><br><span class="line">       <span class="string">lameduck</span> <span class="string">5s</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">ready</span></span><br><span class="line">    <span class="string">kubernetes</span> <span class="string">cluster.local</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span> &#123;</span><br><span class="line">       <span class="string">pods</span> <span class="string">insecure</span></span><br><span class="line">       <span class="string">fallthrough</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span></span><br><span class="line">       <span class="string">ttl</span> <span class="number">30</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">prometheus</span> <span class="string">:9153</span></span><br><span class="line">    <span class="string">forward</span> <span class="string">.</span> <span class="string">/etc/resolv.conf</span> &#123;</span><br><span class="line">       <span class="string">max_concurrent</span> <span class="number">1000</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">    <span class="string">loop</span></span><br><span class="line">    <span class="string">reload</span></span><br><span class="line">    <span class="string">loadbalance</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>  如果 forward 的参数只指定了 &#x2F;etc&#x2F;resolv.conf 则代表会将请求转发 &#x2F;etc&#x2F;resolv.conf 中配置的 nameserver。值得注意的一点是，coredns 容器内的 &#x2F;etc&#x2F;resolv.conf 是最初从宿主机的 &#x2F;etc&#x2F;resolv.conf 继承过来的。如果使用 <strong>forward .  &#x2F;etc&#x2F;resolv.conf</strong> 的方式指定上级 DNS，后续需要修改 coredns 的上级 DNS 服务器则需要修改 coredns 宿主机的 &#x2F;etc&#x2F;resolv.conf，并重启 coredns 生效。</p>
<h3 id="确认上级转发-DNS-连通性是否正常"><a href="#确认上级转发-DNS-连通性是否正常" class="headerlink" title="确认上级转发 DNS 连通性是否正常"></a><strong>确认上级转发 DNS 连通性是否正常</strong></h3><p>  只需要登录到 coredns 所在的宿主机去 ping 上级 DNS 的地址或者通过在宿主机上使用 <strong>dig @服务器地址</strong> 域名 来测试连通性。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2022/01/20/Pending/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/20/Pending/" class="post-title-link" itemprop="url">Pod 状态 Pending 处理思路</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-20 18:00:15" itemprop="dateCreated datePublished" datetime="2022-01-20T18:00:15+08:00">2022-01-20</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>30 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h1><p>  如果 Pod 处于 Pending 状态，这意味着当前集群不存在满足 Pod 运行条件的节点，需要通过<code> kubectl describe pod名称 -n 命名空间</code> 获取该 Pod 相关的 <code>Events</code>。基于 <code>Events</code> 给出的信息来处理。</p>
<p>  对于 Pod 一直处于 Pending 状态的问题，通常可能的原因如下：</p>
<ul>
<li>集群节点资源不足（CPU、内存、GPU和其他）</li>
<li>不满足亲和性、反亲和性规则（nodeSelector、Affinity）</li>
<li>节点存在污点，Pod 无容忍（节点异常被动添加污点、管理员手动添加污点）</li>
<li>调度器存在 BUG 或者当前调度器存在故障</li>
</ul>
<h1 id="排查思路"><a href="#排查思路" class="headerlink" title="排查思路"></a>排查思路</h1><h2 id="节点资源类"><a href="#节点资源类" class="headerlink" title="节点资源类"></a>节点资源类</h2><h3 id="CPU-或内存不足"><a href="#CPU-或内存不足" class="headerlink" title="CPU 或内存不足"></a>CPU 或内存不足</h3><h4 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> Insufficient cpu, Insufficient memory.</code></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">Type</span>     <span class="string">Reason</span>            <span class="string">Age</span>                <span class="string">From</span>               <span class="string">Message</span></span><br><span class="line">  <span class="string">----</span>     <span class="string">------</span>            <span class="string">----</span>               <span class="string">----</span>               <span class="string">-------</span></span><br><span class="line">  <span class="string">Warning</span>  <span class="string">FailedScheduling</span>  <span class="string">10s</span> <span class="string">(x2</span> <span class="string">over</span> <span class="string">10s)</span>  <span class="attr">default-scheduler  0/4 nodes are available:</span> <span class="number">4</span> <span class="string">Insufficient</span> <span class="string">cpu,</span> <span class="number">3</span> <span class="string">Insufficient</span> <span class="string">memory.</span></span><br></pre></td></tr></table></figure>

<h4 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该类错误是由于集群的机器资源不足，无法满足待调度 Pod 最低运行要求。对于此类问题需要先获取待调度 Pod 的资源需求，通过查看 cpu、memory 剩余的可分配资源量，对比 Pod 申请的资源量，我们就可以知道节点资源和 Pod 申请量的差值。再根据实际情况去降低 Pod 的资源要求或者在节点上释放其他不需要的资源、扩容节点资源等方式处理。</p>
<p>  使用 volcano 作为调度器的场景，资源不足类的错误统一为 <code>all nodes are unavailable: 10 node(s) resource fit failed.</code> 如果看到类似报错，也适用下面的操作方法来排查问题。</p>
<h4 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="获取待调度-Pod-的资源需求"><a href="#获取待调度-Pod-的资源需求" class="headerlink" title="获取待调度 Pod 的资源需求"></a>获取待调度 Pod 的资源需求</h5><p>​    通过 <code>kubectl get pod -n 命名空间 Pod名称</code> 可以查询 Pod 的配置，查询关键字段是 containers 下的 resources</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span>  <span class="comment"># 运行该 Pod 的最低资源需求，调度也是根据 requests 的配置来的。</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;50&quot;</span>  <span class="comment"># 代表运行该 Pod 的节点至少需要 50U 可供使用，此处可能是以其他方式表示，如 50000m。</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">50Gi</span> <span class="comment"># 代表运行该 Pod 的节点至少需要 50Gi 资源，此处可能是以其他方式表示，如 50000Mi。</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>​    也可以通过 <code>kubectl</code> 提取关键字段的方式获取</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例指令</span></span><br><span class="line">kubectl get pod hello-world -o jsonpath=&#x27;&#123;.spec.containers[*].resources.requests&#125;&#x27;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出</span></span><br><span class="line">map[cpu:50 memory:50Gi]</span><br></pre></td></tr></table></figure>

<h5 id="获取节点可分配的资源量"><a href="#获取节点可分配的资源量" class="headerlink" title="获取节点可分配的资源量"></a>获取节点可分配的资源量</h5><p>​    通过 <code>kubectl describe node 节点名称 | grep -A20 &quot;Allocated resources:&quot;</code> 可以获取节点当前资源分配的情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Allocated resources:</span><br><span class="line">  (Total limits may be over 100 percent, i.e., overcommitted.)</span><br><span class="line">  Resource                 Requests       Limits</span><br><span class="line">  --------                 --------       ------</span><br><span class="line">  cpu                      10331m (21%)   37 (77%)</span><br><span class="line">  memory                   20796Mi (21%)  60363Mi (63%)</span><br><span class="line">  ephemeral-storage        0 (0%)         0 (0%)</span><br><span class="line">  hostdev.k8s.io/dev_fuse  0              0</span><br><span class="line">  hostdev.k8s.io/dev_mem   0              0</span><br><span class="line">Events:                    &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h3 id="GPU-资源不足"><a href="#GPU-资源不足" class="headerlink" title="GPU 资源不足"></a>GPU 资源不足</h3><h4 id="故障现象-1"><a href="#故障现象-1" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> Insufficient tencent.com/vcuda-core, Insufficient tencent.com/vcuda-memory.</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                From               Message</span><br><span class="line">  ----     ------            ----               ----               -------</span><br><span class="line">  Warning  FailedScheduling  10s (x2 over 10s)  default-scheduler  0/4 nodes are available: 4 Insufficient tencent.com/vcuda-core, 3 Insufficient tencent.com/vcuda-memory.</span><br></pre></td></tr></table></figure>

<h4 id="解决思路-1"><a href="#解决思路-1" class="headerlink" title="解决思路"></a>解决思路</h4><p>  此类问题的可能原因如下：</p>
<ul>
<li><p>gpu-manager 故障，或 GPU 节点未配置 gpu 标签。</p>
</li>
<li><p>gpu-manager 发现不到 GPU 资源，通常是 GPU 驱动未安装正确导致的。</p>
</li>
<li><p>同 <strong>CPU 或内存不足</strong> 的案例，可能是节点资源不足。</p>
</li>
<li><p>如果是节点资源充足，但是无法创建，可能原因是 kube-scheduler 的 BUG，或者是当前环境存在 GPU 碎片。</p>
<p>首先要确保集群内存在 GPU 资源，并且 node 信息上已经识别出 GPU 资源。然后要确保调度器已支持 GPU 类资源的使用，满足这两个条件环境才能实现 GPU 资源的调度。</p>
<p>GPU 资源是通过 device-plugins 的机制去发现的，如果说我们发现 GPU 节点上没有发现任何可用的 GPU 资源，这时候先要去检查  gpu-manager 的组件运行是否正常。</p>
<p>如果节点存在可用的资源，但是无法调用成功。此时可以参考 <strong>CPU 或内存不足</strong> 的处理方法，确认集群资源是否足够。此外还存在一种场景，就是如果环境同时去调度大量 Pod，这些 Pod 申请的 GPU 资源量较小也可能会导致 Pod 调度失败。原理可以参考<a target="_blank" rel="noopener" href="https://docs.qq.com/doc/DV2pJVUJVTmdycnFq">《GPU碎片》</a>，解决思路可以参考<a target="_blank" rel="noopener" href="https://gdc.lexiangla.com/teams/k100044/docs/9261de46c75511eba30996d948652e79?company_from=gdc">《云智天枢-单节点vGPU调度优化》</a> ，以及<a target="_blank" rel="noopener" href="https://gdc.lexiangla.com/teams/k100044/docs/a17471765a6511eb832766820c31d5db?company_from=gdc">《云智天枢-TKE 2.9.4+TIMatrix 2.5.1-vGPU调度失败问题》</a>。</p>
</li>
</ul>
<h4 id="处理步骤-1"><a href="#处理步骤-1" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="获取待调度-Pod-的资源需求-1"><a href="#获取待调度-Pod-的资源需求-1" class="headerlink" title="获取待调度 Pod 的资源需求"></a>获取待调度 Pod 的资源需求</h5><p>​    通过 <code>kubectl get pod -n 命名空间 Pod名称</code> 可以查询 Pod 的配置，查询关键字段是 containers 下的 resources</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span>  <span class="comment"># 运行该 Pod 的最低资源需求，调度也是根据 requests 的配置来的。</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-core:</span> <span class="string">&quot;50&quot;</span>  <span class="comment"># 代表 Pod 运行最低需要使用到的 GPU 卡的计算资源，50 代表 0.5 卡，当配置为 100 时将会独占单卡资源。</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-memory:</span> <span class="string">&quot;20&quot;</span> <span class="comment"># 代表 Pod 运行最低需要使用到的 GPU 卡显存资源，1 等同于 256 Mi 显存，配置为 20 代表需要使用 5120Mi 的显存。</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>​    也可以通过 <code>kubectl</code> 提取关键字段的方式获取信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例指令</span></span><br><span class="line">kubectl get pod hello-world -o jsonpath=&#x27;&#123;.spec.containers[*].resources.requests&#125;&#x27;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出</span></span><br><span class="line">map[cpu:50 memory:50Gi tencent.com/vcuda-core:50 tencent.com/vcuda-memory:20]</span><br></pre></td></tr></table></figure>

<h5 id="获取节点可分配的资源量-1"><a href="#获取节点可分配的资源量-1" class="headerlink" title="获取节点可分配的资源量"></a>获取节点可分配的资源量</h5><p>​    通过 <code>kubectl describe node 节点名称 | grep -A20 &quot;Capacity&quot;</code> 可以获取节点是否已成功获取到 GPU 资源</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Capacity:</span><br><span class="line"> cpu:                       20</span><br><span class="line"> ephemeral-storage:         515927276Ki</span><br><span class="line"> hostdev.k8s.io/dev_fuse:   128</span><br><span class="line"> hostdev.k8s.io/dev_mem:    128</span><br><span class="line"> hugepages-1Gi:             0</span><br><span class="line"> hugepages-2Mi:             0</span><br><span class="line"> memory:                    82319956Ki</span><br><span class="line"> nvidia.com/gpu:            1</span><br><span class="line"> pods:                      255</span><br><span class="line"> tencent.com/vcuda-core:    100    # 此处不能为 0，如果为 0 代表 gpu-manager 运行异常</span><br><span class="line"> tencent.com/vcuda-memory:  59     # 此处不能为 0，如果为 0 代表 gpu-manager 运行异常</span><br><span class="line">Allocatable:</span><br><span class="line"> cpu:                       19800m</span><br><span class="line"> ephemeral-storage:         475478576775</span><br><span class="line"> hostdev.k8s.io/dev_fuse:   128</span><br><span class="line"> hostdev.k8s.io/dev_mem:    128</span><br><span class="line"> hugepages-1Gi:             0</span><br><span class="line"> hugepages-2Mi:             0</span><br><span class="line"> memory:                    81193556Ki</span><br><span class="line"> nvidia.com/gpu:            1</span><br></pre></td></tr></table></figure>

<p>​    通过 <code>kubectl describe node 节点名称 | grep -A20 &quot;Allocated resources:&quot;</code> 可以获取节点当前资源分配的情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Allocated resources:</span><br><span class="line">  (Total limits may be over 100 percent, i.e., overcommitted.)</span><br><span class="line">  Resource                  Requests     Limits</span><br><span class="line">  --------                  --------     ------</span><br><span class="line">  cpu                       3314m (16%)  26900m (135%)</span><br><span class="line">  memory                    3427Mi (4%)  35936Mi (45%)</span><br><span class="line">  ephemeral-storage         0 (0%)       0 (0%)</span><br><span class="line">  hostdev.k8s.io/dev_fuse   0            0</span><br><span class="line">  hostdev.k8s.io/dev_mem    0            0</span><br><span class="line">  nvidia.com/gpu            0            0</span><br><span class="line">  tencent.com/vcuda-core    100          100</span><br><span class="line">  tencent.com/vcuda-memory  59           59</span><br><span class="line">Events:                     &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h5 id="检查-gpu-manager-组件状态是否可用"><a href="#检查-gpu-manager-组件状态是否可用" class="headerlink" title="检查 gpu-manager 组件状态是否可用"></a>检查 gpu-manager 组件状态是否可用</h5><p>​    如果在 <strong>获取节点可分配的资源量</strong> 步骤中发现节点无法获取到 GPU 资源。此时可以通过 <code>kubectl get pod -o wide -A | grep gpu-manager | grep 节点地址</code> 可以获取节点上是否正确运行 gpu-manager</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出如下，类似输出代表 gpu-manager 已正常运行</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get pod -o wide -A | grep gpu-manager | grep 192.168.0.57</span><br><span class="line">timaker        gpu-manager-9g9ps                                                1/1     Running     0          9h      10.199.5.181    192.168.0.57   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>  如果 gpu-manager 正常运行，但是 <code>tencent.com/vcuda-core</code> 依然为 0，需要通过 <code>kubectl logs -n 命名空间 Pod名称</code> 来收集组件的日志，提安灯工单给后端同学确认问题点。</p>
<p>  如果在指定节点上找不到 gpu-manager 这个 Pod，通常是由于节点不满足 gpu-manager 创建条件导致的。可能是节点未正确配置 gpu 的标签，通过 <code>kubectl get nodes --show-labels | grep 节点名称 | grep nvidia-device-enable</code> 来检查 GPU 标签。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出，注意 TKE、TIM 场景下，GPU 标签 key 固定为 nvidia-device-enable，value 为 <span class="built_in">enable</span> 代表该节点需要启动 gpu-manager。</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get nodes --show-labels | grep 192.168.0.57 | grep nvidia-device-enable</span><br><span class="line">192.168.0.57   Ready    &lt;none&gt;   19d   v1.16.15-alauda.3   GPUName=T4,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.57,kubernetes.io/os=linux,nvidia-device-enable=enable</span><br></pre></td></tr></table></figure>

<h3 id="端口不可用"><a href="#端口不可用" class="headerlink" title="端口不可用"></a>端口不可用</h3><h4 id="故障现象-2"><a href="#故障现象-2" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> didn&#39;t have free ports for the requested pod ports.</code></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">Type</span>     <span class="string">Reason</span>            <span class="string">Age</span>                <span class="string">From</span>               <span class="string">Message</span></span><br><span class="line">  <span class="string">----</span>     <span class="string">------</span>            <span class="string">----</span>               <span class="string">----</span>               <span class="string">-------</span></span><br><span class="line">  <span class="string">Warning</span>  <span class="string">FailedScheduling</span>  <span class="string">10s</span> <span class="string">(x2</span> <span class="string">over</span> <span class="string">10s)</span>  <span class="attr">default-scheduler  0/4 nodes are available:</span> <span class="number">4</span> <span class="string">node(s)</span> <span class="string">didn&#x27;t</span> <span class="string">have</span> <span class="string">free</span> <span class="string">ports</span> <span class="string">for</span> <span class="string">the</span> <span class="string">requested</span> <span class="string">pod</span> <span class="string">ports.</span></span><br></pre></td></tr></table></figure>

<h4 id="解决思路-2"><a href="#解决思路-2" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该问题出现在 Pod 使用了 <code>hostNetwork: true</code> 配置的场景，使用该参数 Pod 将会和宿主机网络命名空间，如果宿主机已监听了 Pod 运行需要使用的端口，此时将会触发这个错误。</p>
<p>  比较常见遇到该类问题的场景是更新 deployment 的配置，每当我们去修改 deployment 配置时，控制器就会自动创建新的 replicaset ，replicaset 使用最新的配置来生成 Pod。而 deployment 考虑到应用可用性。他在默认情况下是不会直接替换新的 replicaset，而是逐个替换旧的 replicaset 的 Pod，可以理解成是一种滚动更新的模式。</p>
<p>  而这种模式会在 Pod 使用了 <code>hostNetwork: true</code> 配置的场景下触发端口冲突的问题。因为控制器需要先创建可用的副本，才能释放旧副本。但是旧副本又占用了新副本的资源，相当于形成了一个死锁。在这种情况下业务将无法更新。</p>
<p>  解决思路如下：</p>
<ul>
<li>直接停止掉所有的旧副本，通过最新的 replicaset 创建新的 Pod 副本来更新配置。</li>
<li>修改 deployment 的更新策略。将默认使用的 RollingUpdate，修改为 Recreate 的模式。</li>
</ul>
<h4 id="处理步骤-2"><a href="#处理步骤-2" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="方法一：暂停所有副本，重新创建应用"><a href="#方法一：暂停所有副本，重新创建应用" class="headerlink" title="方法一：暂停所有副本，重新创建应用"></a>方法一：暂停所有副本，重新创建应用</h5><p>​    通过 scale 的方式调整副本数量，可参考如下步骤执行，<strong>注意该步骤执行时会暂停业务</strong>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先释放所有旧副本，此时业务不可用</span></span><br><span class="line">kubectl scale deployment -n 命名空间 deployment名称 --replicas=0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改副本数，重新创建新 Pod，恢复业务</span></span><br><span class="line">kubectl scale deployment -n 命名空间 deployment名称 --replicas=期望副本数</span><br></pre></td></tr></table></figure>

<h5 id="方法二：修改-deployment-更新策略为-Recreate"><a href="#方法二：修改-deployment-更新策略为-Recreate" class="headerlink" title="方法二：修改 deployment 更新策略为  Recreate"></a>方法二：修改 deployment 更新策略为  Recreate</h5><p>​    修改 deployment 的更新策略为 Recreate，这样一旦触发 deployment 更新配置，就会直接重建所有 Pod。此时不会触发端口冲突类的问题。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑配置</span></span><br><span class="line">kubectl edit deployment -n 命名空间 deployment 名称</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出结果</span></span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:       # 默认为 RollingUpdate 配置的参数，删除掉</span><br><span class="line">      maxSurge: 1        # 默认为 RollingUpdate 配置的参数，删除掉</span><br><span class="line">      maxUnavailable: 1  # 默认为 RollingUpdate 配置的参数，删除掉</span><br><span class="line">    type: RollingUpdate  # 此处替换为 Recreate</span><br></pre></td></tr></table></figure>

<h2 id="亲和性配置类"><a href="#亲和性配置类" class="headerlink" title="亲和性配置类"></a>亲和性配置类</h2><h3 id="节点不满足-nodeSelector-配置"><a href="#节点不满足-nodeSelector-配置" class="headerlink" title="节点不满足 nodeSelector 配置"></a>节点不满足 nodeSelector 配置</h3><h4 id="故障现象-3"><a href="#故障现象-3" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> didn&#39;t match node selector.</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Node-Selectors:  app=123</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 30s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 30s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                    From               Message</span><br><span class="line">  ----     ------            ----                   ----               -------</span><br><span class="line">  Warning  FailedScheduling  3m47s (x509 over 11h)  default-scheduler  0/4 nodes are available: 4 node(s) didn&#x27;t match node selector.</span><br></pre></td></tr></table></figure>

<h4 id="解决思路-3"><a href="#解决思路-3" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该问题出现在 Pod 使用 <code>nodeSelector</code> 配置的场景，该配置是用于固定调度 Pod 到满足标签的节点上。如果集群内不存在满足 Pod 配置标签的节点，调度就会失败。解决此类问题的方法是，排查 Pod 的 nodeSelector 配置以及当前集群各个节点的 labels 配置。和 <code>nodeSelector</code> 有关的其他信息，可参考<a target="_blank" rel="noopener" href="https://gdc.lexiangla.com/teams/k100044/docs/cf110fa8808611ebb2636263495fb44b?company_from=gdc">《 kubernetes 调度》</a> 中 <code>nodeSelector</code> 相关的章节。</p>
<h4 id="处理步骤-3"><a href="#处理步骤-3" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="1-确认当前-Pod-的-nodeSelector-配置"><a href="#1-确认当前-Pod-的-nodeSelector-配置" class="headerlink" title="1. 确认当前 Pod 的 nodeSelector 配置"></a>1. 确认当前 Pod 的 nodeSelector 配置</h5><p>  通过 <code>kubectl get pod -n 命名空间 Pod名称 -o yaml </code> 可以查询 Pod 的配置，查询关键字段是 spec 下的 nodeSelector</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">...</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - /bin/echo</span><br><span class="line">    - hello”,”world</span><br><span class="line">    image: busybox</span><br><span class="line">  nodeSelector:</span><br><span class="line">    app: &quot;123&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>  也可以通过 <code>kubectl get pod -n 命名空间 Pod名称 -o jsonpath=&#39;&#123;.spec.nodeSelector&#125;&#39;</code> 指令快速获取</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出</span></span><br><span class="line">[root@tke-192-168-0-2 tttgl]# kubectl get pod  hello-world -o jsonpath=&#x27;&#123;.spec.nodeSelector&#125;&#x27;</span><br><span class="line">map[app:123]</span><br></pre></td></tr></table></figure>

<h5 id="2-确认当前-node-的-label-配置"><a href="#2-确认当前-node-的-label-配置" class="headerlink" title="2. 确认当前 node 的 label 配置"></a>2. 确认当前 node 的 label 配置</h5><p>  通过 <code>kubectl get node --show-labels </code> 可以查询节点标签</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get nodes --show-labels</span><br><span class="line">NAME           STATUS   ROLES    AGE   VERSION             LABELS</span><br><span class="line">192.168.0.2    Ready    master   98d   v1.16.15-alauda.3   alertmanager=kube-prometheus,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,es=true,ingress=true,kafka=true,kube-ovn/role=master,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.2,kubernetes.io/os=linux,log=true,monitoring=enabled,node-role.kubernetes.io/master=,nvidia-device-enable=false,prometheus=prometheus-0,recovery=true,zk=true</span><br><span class="line">192.168.0.25   Ready    master   98d   v1.16.15-alauda.3   app/ti-license=,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,es=true,ingress=true,kafka=true,kube-ovn/role=master,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.25,kubernetes.io/os=linux,log=true,node-role.kubernetes.io/master=,nvidia-device-enable=false,recovery=true,zk=true</span><br><span class="line">192.168.0.43   Ready    master   13d   v1.16.15-alauda.3   app/ti-license=,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ecovery=true,es=true,ingress=true,kafka=true,kube-ovn/role=master,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.43,kubernetes.io/os=linux,log=true,node-role.kubernetes.io/master=,nvidia-device-enable=false,recovery=true,zk=true</span><br><span class="line">192.168.0.57   Ready    &lt;none&gt;   20d   v1.16.15-alauda.3   GPUName=T4,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.57,kubernetes.io/os=linux,nvidia-device-enable=enable</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">也可以通过 grep 过滤具备标签的节点，示例输出如下</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get nodes --show-labels | grep GPUName</span><br><span class="line">192.168.0.57   Ready    &lt;none&gt;   20d   v1.16.15-alauda.3   GPUName=T4,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.57,kubernetes.io/os=linux,nvidia-device-enable=enable</span><br></pre></td></tr></table></figure>

<p>  如果发现没有节点符合 Pod 配置的 <code>nodeSelector</code>，需要确认一下是 Pod 配置 <code>nodeSelector</code> 键值是否合理，还是节点配置了错误的标签键值。</p>
<h2 id="污点与容忍配置类"><a href="#污点与容忍配置类" class="headerlink" title="污点与容忍配置类"></a>污点与容忍配置类</h2><h3 id="节点污点导致无法调度"><a href="#节点污点导致无法调度" class="headerlink" title="节点污点导致无法调度"></a>节点污点导致无法调度</h3><h4 id="故障现象-4"><a href="#故障现象-4" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> had taint, that the pod didn&#39;t tolerate.</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age   From               Message</span><br><span class="line">  ----     ------            ----  ----               -------</span><br><span class="line">  Warning  FailedScheduling  99m   default-scheduler  0/2 nodes are available: 2 node(s) had taint, that the pod didn&#x27;t tolerate.</span><br></pre></td></tr></table></figure>

<h4 id="解决思路-4"><a href="#解决思路-4" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该问题出现在集群内可用节点都存在污点的情况下，在不做出任何修改的情况下，控制节点一般会打 <code>node-role.kubernetes.io/master</code> 的污点，避免业务调度到控制节点。而计算节点一般会运行业务，无特殊需求的情况下，默认是不对节点进行污点配置的。如果计算节点存在污点，很有可能是因为节点满足了某个条件，导致其被控制器打上了污点。此时 Pod </p>
<p>  通常情况下我们是不需要主动为 Pod 添加容忍配置，我们需要去解决为什么节点被打上污点的问题。与污点容忍配置相关细节可参考<a target="_blank" rel="noopener" href="https://gdc.lexiangla.com/teams/k100044/docs/cf110fa8808611ebb2636263495fb44b?company_from=gdc">《 kubernetes 调度》</a> 中 <code>污点与容忍</code> 相关的章节。</p>
<p>  常见的系统内置污点可参考如下：</p>
<ul>
<li><code>node.kubernetes.io/not-ready</code>：节点未准备好。这相当于节点状态 <code>Ready</code> 的值为 “<code>False</code>“。</li>
<li><code>node.kubernetes.io/unreachable</code>：节点控制器访问不到节点. 这相当于节点状态 <code>Ready</code> 的值为 “<code>Unknown</code>“。</li>
<li><code>node.kubernetes.io/memory-pressure</code>：节点存在内存压力。</li>
<li><code>node.kubernetes.io/disk-pressure</code>：节点存在磁盘压力。</li>
<li><code>node.kubernetes.io/pid-pressure</code>: 节点的 PID 压力。</li>
<li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li>
<li><code>node.kubernetes.io/unschedulable</code>: 节点不可调度。</li>
<li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个 “外部” 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li>
</ul>
<h4 id="处理步骤-4"><a href="#处理步骤-4" class="headerlink" title="处理步骤"></a>处理步骤</h4><p><strong>1. 确认集群内各个节点的污点配置</strong></p>
<p>  通过 <code>kubectl describe node node名称 </code> 可以查询污点的配置，查询关键字段是 spec 下的 taints</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-1</span> <span class="string">~</span>]<span class="comment"># kubectl describe node k8s-1</span></span><br><span class="line"><span class="attr">Name:</span>               <span class="string">k8s-1</span></span><br><span class="line"><span class="attr">Roles:</span>              <span class="string">control-plane,master</span></span><br><span class="line"><span class="attr">Taints:</span>             <span class="string">node-role.kubernetes.io/master:NoSchedule</span>  <span class="comment"># Taints 这个字段代表污点</span></span><br><span class="line"><span class="attr">Unschedulable:</span>      <span class="literal">false</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>  也可以通如下 for 循环直接获取所有节点的污点配置，参考如下执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取所有节点的污点配置</span></span><br><span class="line">for node in `kubectl get nodes --no-headers | awk &#x27;&#123; print $1 &#125;&#x27;`; do echo &quot;==$node==&quot; &amp;&amp;  kubectl get nodes $node -o jsonpath=&#x27;&#123;.spec.taints&#125;&#x27; &amp;&amp; echo -e &quot;\n&quot; ; done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出如下</span></span><br><span class="line">[root@k8s-1 ~]# for node in `kubectl get nodes --no-headers | awk &#x27;&#123; print $1 &#125;&#x27;`; do echo &quot;==$node==&quot; &amp;&amp;  kubectl get nodes $node -o jsonpath=&#x27;&#123;.spec.taints&#125;&#x27; &amp;&amp; echo -e &quot;\n&quot; ; done</span><br><span class="line">==k8s-1==</span><br><span class="line">[&#123;&quot;effect&quot;:&quot;NoSchedule&quot;,&quot;key&quot;:&quot;node-role.kubernetes.io/master&quot;&#125;]</span><br><span class="line"></span><br><span class="line">==k8s-2==</span><br><span class="line">[&#123;&quot;effect&quot;:&quot;NoSchedule&quot;,&quot;key&quot;:&quot;node.kubernetes.io/disk-pressure&quot;,&quot;timeAdded&quot;:&quot;2021-09-17T06:59:14Z&quot;&#125;]</span><br></pre></td></tr></table></figure>

<p><strong>2. 解决污点问题</strong></p>
<p>  通过步骤 1 获取到污点的 key 以后，再参考解决思路中的 <strong>常见的系统内置污点</strong> 进行处理。可行的处理方法可参考<a target="_blank" rel="noopener" href="https://gdc.lexiangla.com/teams/k100044/docs/842da35cae3511ebb4e66afd0c8a692a?company_from=gdc">《节点 NotReady 与驱逐的处理方法》</a>中的有关驱逐部分的章节，以及<a target="_blank" rel="noopener" href="https://iwiki.woa.com/pages/viewpage.action?pageId=1042992399">《集群产生 Evicted 状态 Pod》</a>的定位思路。</p>
<h2 id="存储类"><a href="#存储类" class="headerlink" title="存储类"></a>存储类</h2><h3 id="卷节点亲和性冲突"><a href="#卷节点亲和性冲突" class="headerlink" title="卷节点亲和性冲突"></a>卷节点亲和性冲突</h3><h4 id="故障现象-5"><a href="#故障现象-5" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code>had volume node affinity conflict</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason            Age              From               Message</span><br><span class="line">  ----     ------            ----             ----               -------</span><br><span class="line">  Warning  FailedScheduling  5s (x2 over 5s)  default-scheduler  0/4 nodes are available: 1 node(s) had volume node affinity conflict, 3 node(s) didn&#x27;t match node selector.</span><br></pre></td></tr></table></figure>

<h4 id="解决思路-5"><a href="#解决思路-5" class="headerlink" title="解决思路"></a>解决思路</h4><p>  这类错误主要发生在 local pv 的场景。当 PV 指定了 <code>local</code> 参数，使用宿主机的本地目录时，此时 PV 需要配置 <code>nodeAffinity</code> 的配置，将 PV 固定创建在特定的节点上。如果 Pod 需要引用这个 PV 作为存储卷使用，则 Pod 需要运行到 PV 绑定的节点上。如果该 PV 绑定的节点故障了，则 Pod 将无法调度到其他节点上。此时创建 Pod 时调度器将输出 <code>had volume node affinity conflict</code>  的信息。</p>
<p>  确认问题的思路是，先查看 Pod YAML，确认挂载 volumes 中的 PVC，然后再去查询该 PVC 对应的 PV 是固定调度到哪个机器上。再确认该节点的可用性。</p>
<h4 id="处理步骤-5"><a href="#处理步骤-5" class="headerlink" title="处理步骤"></a>处理步骤</h4><p><strong>1. 确认当前 Pod 的 volumes 配置</strong></p>
<p>  通过 <code>kubectl get pod -n 命名空间 Pod名称 -o yaml </code> 可以查询 Pod 的配置，查询关键字段是 spec 下的 volumes</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/echo</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">hello”,”world</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">kubernetes.io/hostname:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.57</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span>    <span class="comment"># 代表使用了 PVC</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">pvctest</span>      <span class="comment"># PVC 的 名称</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>

<h5 id="2-确认当前-Pod-挂载的-PVC-配置"><a href="#2-确认当前-Pod-挂载的-PVC-配置" class="headerlink" title="2. 确认当前 Pod 挂载的 PVC 配置"></a>2. 确认当前 Pod 挂载的 PVC 配置</h5><p>  通过 <code>kubectl describe pvc -n 命名空间 PVC名称</code> 确认 PVC 对应的 PV</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Name:</span>          <span class="string">pvctest</span></span><br><span class="line"><span class="attr">Namespace:</span>     <span class="string">default</span></span><br><span class="line"><span class="attr">StorageClass:</span></span><br><span class="line"><span class="attr">Status:</span>        <span class="string">Bound</span>    <span class="comment"># PVC 为 Bound 状态，代表它关联上了 PV</span></span><br><span class="line"><span class="attr">Volume:</span>        <span class="string">pvtest</span>   <span class="comment"># PV 名称，通过查询 PV 确认配置</span></span><br><span class="line"><span class="attr">Labels:</span>        <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Annotations:   cpaas.io/creator:</span> <span class="string">kubernetes-admin</span></span><br><span class="line">               <span class="attr">cpaas.io/updated-at:</span> <span class="number">2021-09-16T07:10:49Z</span></span><br><span class="line">               <span class="attr">pv.kubernetes.io/bind-completed:</span> <span class="literal">yes</span></span><br><span class="line"><span class="attr">Finalizers:</span>    [<span class="string">kubernetes.io/pvc-protection</span>]</span><br><span class="line"><span class="attr">Capacity:</span>      <span class="string">5Gi</span></span><br><span class="line"><span class="attr">Access Modes:</span>  <span class="string">RWO</span></span><br><span class="line"><span class="attr">VolumeMode:</span>    <span class="string">Filesystem</span></span><br><span class="line"><span class="attr">Mounted By:</span>    <span class="string">hello-world</span></span><br><span class="line"><span class="attr">Events:</span>        <span class="string">&lt;none&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="3-确认-PVC-关联的-PV-配置"><a href="#3-确认-PVC-关联的-PV-配置" class="headerlink" title="3.确认 PVC 关联的 PV 配置"></a>3.确认 PVC 关联的 PV 配置</h5><p>  通过 <code>kubectl get pv PV名称 -o yaml</code> 确认 PV 的配置，关键字段是 <code>nodeAffinity</code></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">claimRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">pvctest</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/root/test</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span>  <span class="comment"># 使用 local 参数场景下必须指定该参数，这里的条件相当于是 PV 固定调度到 192.168.0.2 这个机器上</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure>

<p>  通过这三步确认出 PV 固定调度到哪些节点上。再通过<code> kubectl get nodes --show-labels | grep 标签键=值</code> 去确认该节点的状态是否正常。如果节点处于 NotReady 状态，可以根据<a target="_blank" rel="noopener" href="https://gdc.lexiangla.com/teams/k100044/docs/842da35cae3511ebb4e66afd0c8a692a?company_from=gdc">《节点 NotReady 处理》</a>来进行问题排查。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">guoltan</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
