<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>guoltan个人博客</title>
  
  
  <link href="https://guoltan.github.io/atom.xml" rel="self"/>
  
  <link href="https://guoltan.github.io/"/>
  <updated>2024-03-25T16:08:15.580Z</updated>
  <id>https://guoltan.github.io/</id>
  
  <author>
    <name>guoltan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://guoltan.github.io/2024/03/25/helm%E5%85%A5%E9%97%A8/"/>
    <id>https://guoltan.github.io/2024/03/25/helm%E5%85%A5%E9%97%A8/</id>
    <published>2024-03-25T15:57:31.056Z</published>
    <updated>2024-03-25T16:08:15.580Z</updated>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><hr><h2 id="title-Helm入门date-2021-01-21-18-00-15tags-“K8S”-”Helm”"><a href="#title-Helm入门date-2021-01-21-18-00-15tags-“K8S”-”Helm”" class="headerlink" title="title: Helm入门date: 2021-01-21 18:00:15tags: [“K8S”,”Helm”]"></a>title: Helm入门<br>date: 2021-01-21 18:00:15<br>tags: [“K8S”,”Helm”]</h2><h1 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h1><h2 id="什么是Helm？"><a href="#什么是Helm？" class="headerlink" title="什么是Helm？"></a>什么是Helm？</h2><p>  <strong>Helm的基本定义</strong></p><p>  根据官网的定义，Helm 是 kubernetes 的包管理器。Helm是用于发现，分享，构建应用程序于 Kubernetes 的最佳方法。</p><p>  可以把 Helm 部署应用的方式比作在 centos 系统上通过 rpm 安装软件一样。使用 rpm 去安装应用时，因为软件开发者已经提前将程序打包成了一个 rpm 包。我们去部署时只需要简单的通过类 rpm -ivh xxx.rpm 来进行安装即可。免去了软件编译安装等动作。 通过 Helm 的方式部署 Kubernetes 应用也类似。 通过 Helm 我们可以把应用构建成 Chart 包。以后在其他 Kubernetes 集群上部署应用时，只需要简单的通过 Helm 进行部署，极大的简化了应用的部署成本。</p><p>  通过 Helm 去部署 Kubernetes 应用，可以将 deployment,svc,pvc,configmap 等等资源整合在一起。不再需要单独的进行类似kubectl apply -f resource.yaml 的动作。而是通过类 helm install stable&#x2F;nginx –set service.type&#x3D;NodePort  的动作来完成应用在  Kubernetes 环境上的部署。</p><p>  使用 Helm 部署 Kubernetes 资源，存在如下好处：</p><ul><li><p>可以实现统一的配置管理，通过 Helm 我们可以将分散的 Kubernetes 资源对象整合成一个整体。通过这个整体入口进行维护，简化管理。</p></li><li><p>可以实现应用模板的复用，编写好。简化后续开发成本。</p></li><li><p>简化部署和学习的成本，想要简单的部署一个应用，只需要学习 Charts 的变量以及基础的理论知识即可。不需要深入的学习配置项。</p><p><strong>Helm 的基本概念</strong></p></li></ul><table><thead><tr><th>术语</th><th>说明</th></tr></thead><tbody><tr><td>Chart</td><td>Kubernetes 应用实例的信息集合，本质上是一堆模板文件，根据环境对内容渲染，从而部署到 Kubernetes 集群上。开发者可以按需求制作 Chart 包。</td></tr><tr><td>Config</td><td>创建发布对象的chart的配置信息。</td></tr><tr><td>Release</td><td>Chart 的在 Kubernetes 上的运行实例。一个 Chart 可以在同一个 Kubernetes 集群上安装多次，每个实例以不同的 Release 做区分，后续通过维护 Release 状态来追踪部署的应用信息。</td></tr><tr><td>Repository</td><td>用于存储 Chart 包的仓库。</td></tr></tbody></table><p>  目前Helm已经发展到V3版本了。本文将统一介绍 Helm v2 和 v3 版本。</p><h2 id="Helm-架构"><a href="#Helm-架构" class="headerlink" title="Helm 架构"></a>Helm 架构</h2><h3 id="Helm-的逻辑架构"><a href="#Helm-的逻辑架构" class="headerlink" title="Helm 的逻辑架构"></a><strong>Helm 的逻辑架构</strong></h3><p>![image-20201204114344460](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201204114344460.png)</p><p>  Helm v2 的逻辑架构如上图，开发者可以根据自己的需求提前将资源文件(类似 svc.yaml,deployment.yaml,pvc.yaml 等等) 通过 package 打包成 Chart 包。 再将 Chart 包 upload 到 Chart Repository 上。此时各个用户就可以通过连接 Chart Repository 去获取 Chart 包了。如果这个时候有个用户想将 Chart 包的应用部署到 Kubernetes 集群上，只需要通过 <code>helm install</code>，将 Chart 包 pull 到本地，或者直接使用本地的某个 Chart 包，再通过 grpc 的方式提交请求和资源文件给 Kubernetes 集群上的 Tiller 组件。 Tiller 组件接收到请求以后，会将其解析成 apiserver 能够识别的指令，再将其提交给 apiserver 。最终让 apiserver 来完成资源的创建。最终创建出的应用会被 Helm 以 release 进行状态追踪。</p><p>![image-20201204115220160](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201204115220160.png)</p><p>  Helm v3 的逻辑架构和 Helm v2 基本一致，它去除了 tiller 组件。直接调用 kube-config 中的配置信息，对集群上资源进行创建和管理。</p><h3 id="Helm-v2-与-Helm-v3-的区别"><a href="#Helm-v2-与-Helm-v3-的区别" class="headerlink" title="Helm v2 与 Helm v3 的区别"></a><strong>Helm v2 与 Helm v3 的区别</strong></h3><p>  Helm v3 去除了 tiller 组件实现了 client-only 架构。使得使用 Helm 进行安装部署更加简单</p><p>  Helm v3 的 release 现在存储在集群的资源对象替换成了 secret。</p><p>  Helm v3 的 release 持久化存储在 release 所运行的命名空间下。而不是固定存放在 kube-system。 这使得它能够支持 release 跨命名空间重名。</p><p>  Helm v3 向下兼容 Helm v2，Helm v3 支持渲染 v2 的 Chart。</p><p>  Helm v3 增加了对 values.yaml 的校验。</p><p>  Helm v3 对一部分 v2 的命令进行了替换和删除。</p><h2 id="如何使用Helm？"><a href="#如何使用Helm？" class="headerlink" title="如何使用Helm？"></a><strong>如何使用Helm？</strong></h2><p>  了解到 Helm 的逻辑架构以及业务流以后，我们知道，想要使用 Helm 大概需要做如下的事情：</p><ul><li>获取 Helm 的部署包。</li><li>Helm 需要安装客户端，客户端用于提交和下载 Chart 包。</li><li>在使用 Helm v2 时，需要部署 Tiller 组件，它运行在 Kubernetes 集群之上，并且需要访问 Apiserver 申请资源。我们需要在 Kubernetes 部署 Tiller 组件，并且还需要给他配置 RBAC 角色。</li><li>我们需要一个 REPOSITORY 的组件用于存储 Chart。</li></ul><h3 id="使用-Helm-v2"><a href="#使用-Helm-v2" class="headerlink" title="使用 Helm v2"></a>使用 Helm v2</h3><h4 id="获取-Helm-v2-的部署包"><a href="#获取-Helm-v2-的部署包" class="headerlink" title="获取 Helm v2 的部署包"></a>获取 Helm v2 的部署包</h4><p>  通过 Github 可以获取最新的 Helm 安装包，链接： <a href="https://github.com/helm/helm/releases%E3%80%82">https://github.com/helm/helm/releases。</a> 由于 GitHub 在外网，在国内访问速度比较慢。笔者将 Helm 包最新的版本下载到本地，并上传到网盘。可以直接获取。获取链接：链接：<a href="https://pan.baidu.com/s/12SgVFA0PpsQB_98NjwiOzA">https://pan.baidu.com/s/12SgVFA0PpsQB_98NjwiOzA</a>  提取码：Helm </p><h4 id="安装-Helm-v2-部署包"><a href="#安装-Helm-v2-部署包" class="headerlink" title="安装 Helm v2 部署包"></a>安装 Helm v2 部署包</h4><p>  将获取到的 helm-v2.16.9-linux-amd64.tar.gz 上传到 Kubernetes 集群机器上。并执行如下动作进行安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar xvf helm-v2.16.9-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">cp -p linux-amd64/helm /usr/local/bin</span><br></pre></td></tr></table></figure><p>  执行完成以后通过 helm version 检验是否已经安装上</p><p>![image-20201130104508620](D:\My\学习\6. 笔记\3. 微服务\k8s\note_picture\helm.asserts\image-20201130104508620.png)</p><p>  此时会提示 <code>Error: could not find a ready tiller pod</code> 表示 tiller 组件未能正确找到。这个时候还需要通过 helm init 去初始化安装 tiller。</p><p>  因为 Kubernetes 1.13 加入了 RBAC 的授权机制，我们需要提前为 tiller 创建 service account。后续才能正常的使用。执行如下命令创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br></pre></td></tr></table></figure><p>  此时通过 helm init 对环境进行初始化，默认会添加 google 的 helm 仓库，使用海外的 tiller 镜像。当前是国内环境，直接拉取海外的镜像源会非常缓慢，所以我们需要手工添加 <code>--stable-repo-url</code> 参数，设置使用国内的 helm 仓库源。添加 <code> --tiller-image</code> 参数，设置 tiller 镜像路径为国内镜像的仓库路径。并通过<code>--service-account</code> 参数指定创建的  service account。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm init --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.9 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account tiller</span><br></pre></td></tr></table></figure><blockquote><p>  注意：如果没有为 tiller 创建 service account ，部署完成以后，执行 helm ls 等需要调用 kubernetes 资源的命令时，会提示如下报错。</p></blockquote><p><img src="C:\Users\89715\AppData\Roaming\Typora\typora-user-images\image-20201202170301217.png" alt="image-20201202170301217"></p><p> 部署完成以后，通过<code> helm ls</code> 来确认基本功能是否正常，此时我们的 kubernetes 集群上并没有部署任何的 Chart 所以返回为空是正常的。</p><p><img src="C:\Users\89715\AppData\Roaming\Typora\typora-user-images\image-20201202173915143.png" alt="image-20201202173915143"> </p><p>通过 helm repo list 命令，我们可以查询到，当前拥有两个 helm 仓库，分别是 stable 和 local。 stable对应的就是我们在部署中添加的 阿里云仓库路径。</p><p><img src="C:\Users\89715\AppData\Roaming\Typora\typora-user-images\image-20201202174245350.png" alt="image-20201202174245350"></p><p>此时可以通过 helm search Chart 包名称，来获取仓库上的 Chart 包。</p><p><img src="C:\Users\89715\AppData\Roaming\Typora\typora-user-images\image-20201202174434422.png" alt="image-20201202174434422"></p><h4 id="使用-Helm-部署一个应用"><a href="#使用-Helm-部署一个应用" class="headerlink" title="使用 Helm 部署一个应用"></a>使用 Helm 部署一个应用</h4><p>  现在已经成功的安装上 Helm ，也可以从 Helm 仓库中获取 Chart 包。接下来我将使用 myapp-1.0.0.tgz 这个包作为演示，将该应用部署到 Kubernetes 集群上。</p><p>  通过 <code>helm install myapp-1.0.0.tgz --name myapp</code> 命令，将该 Chart 包部署到 Kubernetes 集群上。</p><p>![image-20201203145511844](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203145511844.png)</p><p>  此时可以看到，helm 会打印本次部署 myapp-1.0.0.tgz 创建了哪些资源。以及访问该应用的简单示例。根据 NOTES 打印的示例，测试该应用。</p><p>![image-20201203181459561](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203181459561.png)</p><p>  从上图可以看到的是，根据 NOTES 提供的测试方法，可以正确的访问该应用。</p><h4 id="Chart-包的目录结构"><a href="#Chart-包的目录结构" class="headerlink" title="Chart 包的目录结构"></a>Chart 包的目录结构</h4><p>  接下来我们看一下 Chart 包中实际打包了哪些东西。它的内部结构是怎么样的。将 myapp-1.0.0.tgz 包进行解压，通过 <code>tar xvf myapp-1.0.0.tgz</code> 来完成。再通过 <code>tree</code> 查看具体内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── Chart.yaml                         # 用于描述 Chart 的基本信息                </span><br><span class="line">├── templates                         # 该目录上存放的 Chart 是待渲染的模板文件，通过 values.yaml 对内容进行渲染。</span><br><span class="line">│   ├── deployment.yaml             # deployment 资源模板</span><br><span class="line">│   ├── _helpers.tpl                 # 模板帮助器，通常存放全局可重用的变量，根据判断条件生成对应变量的值</span><br><span class="line">│   ├── ingress.yaml                 # ingress 资源模板</span><br><span class="line">│   ├── NOTES.txt                     # Chart的使用说明，在 helm install 以后会自动输出的说明内容</span><br><span class="line">│   ├── serviceaccount.yaml         # service account 资源模板</span><br><span class="line">│   ├── service.yaml                # service 资源模板</span><br><span class="line">│   └── tests                         # 该目录上存放的 Chart 的测试用例，通过 helm test 可以调用</span><br><span class="line">│       └── test-connection.yaml    # 测试用例</span><br><span class="line">└── values.yaml                        # 该文件存储了 Chart 资源清单中使用的默认值，后续会渲染到 templates 的资源模板上。</span><br></pre></td></tr></table></figure><p>  接下来查看 Chart 包中文件的具体内容，通过 <code>cat Chart.yaml </code> 可以看到 Chart.yaml 中有 <code>apiVersion</code> 、 <code>appVersion</code> 、 <code>description</code> 、 <code>name</code> 等字段。主要的内容是描述这个 Chart 包可以干什么，对应的 app 版本，我们在 helm search 的时候查询 Chart 包时，有关包描述的信息就是此处。</p><p>![image-20201203151340282](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203151340282.png)</p><p>  查看 values.yaml 的详细内容，该文件的内容用于渲染 templates 下的模板文件。像是我们对应用部署出的副本数有需求，比如需要使用 3 副本。 或者使用的镜像源来自于某个仓库。这些相对固定的字段都可以在该文件中定义，后续在渲染的时候就可以直接采用这些值进行填充。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default values for myapp.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">replicaCount:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">image:</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">daocloud.io/nginx</span></span><br><span class="line">  <span class="attr">tag:</span> <span class="string">latest</span></span><br><span class="line">  <span class="attr">pullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"></span><br><span class="line"><span class="attr">service:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>  我们可以通过查询 templates&#x2F;deployment.yaml 来查看 values.yaml 里面定义的值是如何渲染到资源模板的。如下所示，我们可以看到像是 <code>replicas: &#123;&#123; .Values.replicaCount &#125;&#125;</code> 这类的字段，代表副本数的具体定义通过 values.yaml 文件来获取。而 <code>&#123;&#123; include "myapp.labels" . &#125;&#125; </code> 这类的字段则是引用 _helpers.tpl 文件中定义的 <code>myapp.name</code> 变量的值。 </p><p>  include 和 template 都可以用于引用变量。但通过 template 导入变量内容时无法统一对数据进行格式化，所以通常会使用 include 来引入。 </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">include</span> <span class="string">&quot;myapp.fullname&quot;</span> <span class="string">.</span> &#125;&#125;</span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">&#123;&#123; <span class="string">include</span> <span class="string">&quot;myapp.labels&quot;</span> <span class="string">.</span> <span class="string">|</span> <span class="string">indent</span> <span class="number">4</span> &#125;&#125;</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> &#123;&#123; <span class="string">.Values.replicaCount</span> &#125;&#125;</span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app.kubernetes.io/name:</span> &#123;&#123; <span class="string">template</span> <span class="string">&quot;myapp.name&quot;</span> <span class="string">.</span> &#125;&#125;</span><br><span class="line">      <span class="attr">app.kubernetes.io/instance:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;</span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app.kubernetes.io/name:</span> &#123;&#123; <span class="string">template</span> <span class="string">&quot;myapp.name&quot;</span> <span class="string">.</span> &#125;&#125;</span><br><span class="line">        <span class="attr">app.kubernetes.io/instance:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;</span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> &#123;&#123; <span class="string">.Chart.Name</span> &#125;&#125;</span><br><span class="line">          <span class="attr">image:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; .Values.image.repository &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.image.tag &#125;&#125;</span>&quot;</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> &#123;&#123; <span class="string">.Values.image.pullPolicy</span> &#125;&#125;</span><br><span class="line">          <span class="attr">lifecycle:</span></span><br><span class="line">            <span class="attr">postStart:</span></span><br><span class="line">              <span class="attr">exec:</span></span><br><span class="line">                <span class="attr">command:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">echo</span> <span class="string">Hello,</span> <span class="string">this</span> <span class="string">is</span> <span class="string">helm</span> <span class="string">example</span> <span class="string">app,</span> <span class="string">version</span> <span class="string">is</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; .Chart.Version &#125;&#125;</span>&quot;</span><span class="string">.</span>  <span class="string">&gt;</span> <span class="string">/usr/share/nginx/html/index.html</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">              <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">              <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">          <span class="attr">livenessProbe:</span></span><br><span class="line">            <span class="attr">httpGet:</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">              <span class="attr">port:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">readinessProbe:</span></span><br><span class="line">            <span class="attr">httpGet:</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">              <span class="attr">port:</span> <span class="string">http</span></span><br></pre></td></tr></table></figure><p>  查看 templates&#x2F;_helpers.tpl 文件的内容，可以看到文件的内容是定义各种可重用的变量。这些变量的值会根据各种条件生成。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#123;<span class="comment">/* vim: set filetype=mustache: */</span>&#125;&#125;</span><br><span class="line">&#123;&#123;<span class="comment">/*</span></span><br><span class="line"><span class="comment">Expand the name of the chart.</span></span><br><span class="line"><span class="comment">*/</span>&#125;&#125;</span><br><span class="line">&#123;&#123;- define <span class="string">&quot;myapp.name&quot;</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- <span class="keyword">default</span> .Chart.Name .Values.nameOverride | trunc <span class="number">63</span> | trimSuffix <span class="string">&quot;-&quot;</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- end -&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;&#123;<span class="comment">/*</span></span><br><span class="line"><span class="comment">Create a default fully qualified app name.</span></span><br><span class="line"><span class="comment">We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).</span></span><br><span class="line"><span class="comment">If release name contains chart name it will be used as a full name.</span></span><br><span class="line"><span class="comment">*/</span>&#125;&#125;</span><br><span class="line">&#123;&#123;- define <span class="string">&quot;myapp.fullname&quot;</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- <span class="keyword">if</span> .Values.fullnameOverride -&#125;&#125;</span><br><span class="line">&#123;&#123;- .Values.fullnameOverride | trunc <span class="number">63</span> | trimSuffix <span class="string">&quot;-&quot;</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- <span class="keyword">else</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- $name := <span class="keyword">default</span> .Chart.Name .Values.nameOverride -&#125;&#125;</span><br><span class="line">&#123;&#123;- <span class="keyword">if</span> contains $name .Release.Name -&#125;&#125;</span><br><span class="line">&#123;&#123;- .Release.Name | trunc <span class="number">63</span> | trimSuffix <span class="string">&quot;-&quot;</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- <span class="keyword">else</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- printf <span class="string">&quot;%s-%s&quot;</span> .Release.Name $name | trunc <span class="number">63</span> | trimSuffix <span class="string">&quot;-&quot;</span> -&#125;&#125;</span><br><span class="line">&#123;&#123;- end -&#125;&#125;</span><br><span class="line">&#123;&#123;- end -&#125;&#125;</span><br><span class="line">&#123;&#123;- end -&#125;&#125;</span><br></pre></td></tr></table></figure><p>  查看 templates&#x2F;NOTES.txt 的内容，可以看到这些内容对应的是 <code>helm install </code> 执行时产生的信息。该文件的内容通常用于提示用户如何访问应用。或者补充 Chart 的一些相关信息。</p><p>![image-20201203181459561](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203181459561.png)</p><p>  查看 templates&#x2F;test&#x2F;test-connection.yaml 文件的内容， 这个资源文件是给 <code>helm test</code> 功能准备的，我们可以通过执行 <code>helm test releasename </code> 来执行该文件中的动作。我们可以将一些测试方法提前写好。用户通过该方法即可验证应用是否可正常使用。 在这里test-connection.yaml 的内容是用于创建一个 Pod 并执行 wget 命令，测试服务的连通性是否正常。 </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; include &quot;myapp.fullname&quot; . &#125;&#125;</span>-test-connection&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">&#123;&#123; <span class="string">include</span> <span class="string">&quot;myapp.labels&quot;</span> <span class="string">.</span> <span class="string">|</span> <span class="string">indent</span> <span class="number">4</span> &#125;&#125;</span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">&quot;helm.sh/hook&quot;:</span> <span class="string">test-success</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">wget</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&#x27;wget&#x27;</span>]</span><br><span class="line">      <span class="attr">args:</span>  [<span class="string">&#x27;<span class="template-variable">&#123;&#123; include &quot;myapp.fullname&quot; . &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.service.port &#125;&#125;</span>&#x27;</span>]</span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure><h4 id="release-的版本控制"><a href="#release-的版本控制" class="headerlink" title="release 的版本控制"></a>release 的版本控制</h4><h5 id="体验版本控制"><a href="#体验版本控制" class="headerlink" title="体验版本控制"></a>体验版本控制</h5><p>  之前提到过，通过 helm 部署的应用会以 release 的形式记录。以追踪它在集群的状态。现在我们对 release 版本进行一次升级。替换 Charts.yaml，将 <code>version 1.0.0</code> 替换为 <code>version 2.0.0</code> 。然后重新执行 <code>helm upgrade release_name chart_path</code> 进行版本升级。（如果不指定 –version 选项，默认升级到最新版本。）</p><p>![image-20201203175511136](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203175511136.png)</p><p>  此时发现 <code>REVISION</code> 的字段变成了 2，并且<code>CHART</code> 字段也变成了 myapp-2.0.0 </p><p>![image-20201203175537768](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203175537768.png)</p><p>  通过 curl 命令进行测试，此时可以看到网页输出的内容也改变了。</p><p>![image-20201203181956303](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203181956303.png)</p><p>  此时我们可以通过 <code>helm history myapp</code> 来获取该 release 的历史版本 可以看到这个 release 的历史记录。 <code>REVISION</code> 代表版本号，该值会随着 Release 的迭代而增加，<code>STATUS</code> 代表当前 Release 的状态，DEPLOYED 代表当前已经部署上。 <code>DESCRIPTION</code> 则是用于表示该版本是通过什么动作产生的。</p><p>![image-20201203182227355](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203182227355.png)</p><p>  如果我们希望回退到其中一个版本，可以执行 <code>helm rollback release_name revision</code> 进行版本回滚。现在通过 <code>helm rollback myapp 1</code> 进行版本回退。回退结束后，通过<code>helm history myapp</code> 可以查询有新的 REVISION 版本生成。</p><p>![image-20201203182938211](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201203182938211.png)</p><p>  回滚结束以后，进行页面访问测试，可以查看页面访问的版本</p><p>![image-20201204104837523](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201204104837523.png)</p><h5 id="版本控制是如何实现的"><a href="#版本控制是如何实现的" class="headerlink" title="版本控制是如何实现的"></a>版本控制是如何实现的</h5><p>  我们知道 helm 通过 release 来实现版本控制的。具体是如何做到的呢？ 实际上我们在进行 <code>helm install </code> 或者 <code>helm upgrade</code> 的时候，每当 release 的版本发生改变，都会在 kube-system 命名空间下新建一个 configmap。通过 <code>kubectl get cm -n kube-system -l &quot;OWNER=TILLER&quot; </code> 命令来获取这些 configmap。</p><p>![image-20201204103012259](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201204103012259.png)</p><p>  编辑其中一个版本，查看其中配置的内容。可以看到该 configmap 存储了 release 的数据。因为有这个 configmap 文件在，所以 helm 才能追踪 release 在集群上的版本。实现版本控制。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">release:</span> <span class="string">H4sIAAAAAAAC...</span>  <span class="comment"># 内容过多，省略，该处是用于存储 release 的 yaml 资源配置。可通过 base64 + gzip 解码</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">&quot;2020-12-02T23:36:10Z&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">MODIFIED_AT:</span> <span class="string">&quot;1606952476&quot;</span></span><br><span class="line">    <span class="attr">NAME:</span> <span class="string">myapp</span></span><br><span class="line">    <span class="attr">OWNER:</span> <span class="string">TILLER</span></span><br><span class="line">    <span class="attr">STATUS:</span> <span class="string">SUPERSEDED</span></span><br><span class="line">    <span class="attr">VERSION:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myapp.v1</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;1975523&quot;</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/kube-system/configmaps/myapp.v1</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">0705b870-4d16-4255-bfdc-a97dc3a0e043</span></span><br></pre></td></tr></table></figure><p>  像是 <code>helm ls</code> 其实就是通过 <code>kubectl get cm -n kube-system -l OWNER=TILLER,STATUS=DEPLOYED</code> 来实现查询的，而 <code>helm history myapp</code> 则是通过 <code>kubectl get cm -n kube-system -l OWNER=TILLER,NAME=myapp</code> 来查询。通过访问这些 configmap 信息，来完成对 release 的追踪。</p><p>![image-20201204104602146](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201204104602146.png)</p><h3 id="使用-Helm-v3"><a href="#使用-Helm-v3" class="headerlink" title="使用 Helm v3"></a>使用 Helm v3</h3><h4 id="安装-Helm-v3-部署包"><a href="#安装-Helm-v3-部署包" class="headerlink" title="安装 Helm v3 部署包"></a>安装 Helm v3 部署包</h4><p>  将软件包上传到环境，执行如下命令，进行 Helm v3 版本的部署。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压软件包</span></span><br><span class="line">tar xvf helm-v3.4.1-linux-amd64.tar.gz</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">拷贝 helm 程序到环境，考虑到后续需要进行Helm v2 到 Helm v3 的迁移。暂时不替换 helm 命令。将其重命名为 helmv3。</span></span><br><span class="line">cp linux-amd64/helm /usr/local/bin/helmv3</span><br></pre></td></tr></table></figure><p>  执行 <code>helmv3 ls</code> 测试 Helm v3 组件的运作。</p><p>![image-20201204110802877](D:\My\学习\6. 笔记\3. 微服务\Helm\picture\helm.asserts\image-20201204110802877.png)</p><h4 id="执行-Helm-v2-迁移到-Helm-v3-版本的工作"><a href="#执行-Helm-v2-迁移到-Helm-v3-版本的工作" class="headerlink" title="执行 Helm v2 迁移到 Helm v3 版本的工作"></a>执行 Helm v2 迁移到 Helm v3 版本的工作</h4><p>  Helm v3 的操作方法和 Helm v2 基本上大同小异，我们现在执行一项动作，将当前 Helm v2 部署的 release 以及环境配置迁移到 Helm v3 环境。该项工作可以使用 Helm 社区提供的 helm 2to3 插件来进行。将 helm-2to3-master 目录上传到环境，执行插件安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helmv3 plugin install https://github.com/helm/helm-2to3</span><br><span class="line">helmv3 plugin list</span><br></pre></td></tr></table></figure><p>  2to3 插件目前提供了三个可用的选项，一个是 convert ，它用于实现 Helm v2 的 release 迁移到 Helm v3 环境。 另一个是 move ， 它用于实现 helm 配置的迁移。将 Helm 的配置文件进行搬迁。以及 cleanup 用于清理 Helm v2 的数据。（包括 tiller，release configmap等）</p><p><strong>执行配置迁移</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行配置迁移</span></span><br><span class="line">[root@k8s-master ~]# helmv3 2to3 move config</span><br><span class="line">2020/12/04 11:50:50 WARNING: Helm v3 configuration may be overwritten during this operation.</span><br><span class="line">2020/12/04 11:50:50</span><br><span class="line">[Move config/confirm] Are you sure you want to move the v2 configuration? [y/N]: y</span><br><span class="line">2020/12/04 11:50:51</span><br><span class="line">Helm v2 configuration will be moved to Helm v3 configuration.</span><br><span class="line">2020/12/04 11:50:51 [Helm 2] Home directory: /root/.helm</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] Config directory: /root/.config/helm</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] Data directory: /root/.local/share/helm</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] Cache directory: /root/.cache/helm</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] Create config folder &quot;/root/.config/helm&quot; .</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] Config folder &quot;/root/.config/helm&quot; created.</span><br><span class="line">2020/12/04 11:50:51 [Helm 2] repositories file &quot;/root/.helm/repository/repositories.yaml&quot; will copy to [Helm 3] config folder &quot;/root/.config/helm/repositories.yaml&quot; .</span><br><span class="line">2020/12/04 11:50:51 [Helm 2] repositories file &quot;/root/.helm/repository/repositories.yaml&quot; copied successfully to [Helm 3] config folder &quot;/root/.config/helm/repositories.yaml&quot; .</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] Create cache folder &quot;/root/.cache/helm&quot; .</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] cache folder &quot;/root/.cache/helm&quot; created.</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] Create data folder &quot;/root/.local/share/helm&quot; .</span><br><span class="line">2020/12/04 11:50:51 [Helm 3] data folder &quot;/root/.local/share/helm&quot; created.</span><br><span class="line">2020/12/04 11:50:51 [Helm 2] starters &quot;/root/.helm/starters&quot; will copy to [Helm 3] data folder &quot;/root/.local/share/helm/starters&quot; .</span><br><span class="line">2020/12/04 11:50:51 [Helm 2] starters &quot;/root/.helm/starters&quot; copied successfully to [Helm 3] data folder &quot;/root/.local/share/helm/starters&quot; .</span><br><span class="line">2020/12/04 11:50:51 Helm v2 configuration was moved successfully to Helm v3 configuration.</span><br></pre></td></tr></table></figure><p><strong>执行 release 迁移</strong></p><p>  再接着，我们将 Helm v2 的 release 信息迁移到 Helm v3 环境上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# helmv3 2to3 convert myapp</span><br><span class="line">2020/12/04 11:57:35 Release &quot;myapp&quot; will be converted from Helm v2 to Helm v3.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] Release &quot;myapp&quot; will be created.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] ReleaseVersion &quot;myapp.v1&quot; will be created.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] ReleaseVersion &quot;myapp.v1&quot; created.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] ReleaseVersion &quot;myapp.v2&quot; will be created.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] ReleaseVersion &quot;myapp.v2&quot; created.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] ReleaseVersion &quot;myapp.v3&quot; will be created.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] ReleaseVersion &quot;myapp.v3&quot; created.</span><br><span class="line">2020/12/04 11:57:35 [Helm 3] Release &quot;myapp&quot; created.</span><br><span class="line">2020/12/04 11:57:35 Release &quot;myapp&quot; was converted successfully from Helm v2 to Helm v3.</span><br><span class="line">2020/12/04 11:57:35 Note: The v2 release information still remains and should be removed to avoid conflicts with the migrated v3 release.</span><br><span class="line">2020/12/04 11:57:35 v2 release information should only be removed using `helm 2to3` cleanup and when all releases have been migrated over.</span><br></pre></td></tr></table></figure><p>  在 myapp 部署对应的命名空间下，通过 <code>kubectl get secret -l owner=helm</code> 命令进行查询，可以看到已经创建出对应 release 的 secret 配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get secret -l owner=helm</span><br><span class="line">NAME                          TYPE                 DATA   AGE</span><br><span class="line">sh.helm.release.v1.myapp.v1   helm.sh/release.v1   1      2m11s</span><br><span class="line">sh.helm.release.v1.myapp.v2   helm.sh/release.v1   1      2m11s</span><br><span class="line">sh.helm.release.v1.myapp.v3   helm.sh/release.v1   1      2m11s</span><br></pre></td></tr></table></figure><p>  此时我们执行 <code>helm ls</code> 会发现，release 信息依旧可以获取到，这是因为 helm v2 的环境信息并没有被清理。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# helmv3 ls</span><br><span class="line">NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION</span><br><span class="line">myapp   default         3               2020-12-02 23:53:07.500259911 +0000 UTC deployed        myapp-1.0.0     1.0</span><br><span class="line">[root@k8s-master ~]# helm ls</span><br><span class="line">NAME    REVISION        UPDATED                         STATUS          CHART           APP VERSION     NAMESPACE</span><br><span class="line">myapp   3               Thu Dec  3 07:53:07 2020        DEPLOYED        myapp-1.0.0     1.0             default</span><br></pre></td></tr></table></figure><p>  执行 <code>helmv3 2to3 cleanup</code> 可以对 helm v2 之前的数据进行清理。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# helmv3 2to3 cleanup</span><br><span class="line">WARNING: &quot;Helm v2 Configuration&quot; &quot;Release Data&quot; &quot;Tiller&quot; will be removed.</span><br><span class="line">This will clean up all releases managed by Helm v2. It will not be possible to restore them if you haven&#x27;t made a backup of the releases.</span><br><span class="line">Helm v2 may not be usable afterwards.</span><br><span class="line"></span><br><span class="line">[Cleanup/confirm] Are you sure you want to cleanup Helm v2 data? [y/N]: y</span><br><span class="line">2020/12/04 12:04:35</span><br><span class="line">Helm v2 data will be cleaned up.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] Releases will be deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] ReleaseVersion &quot;myapp.v1&quot; will be deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] ReleaseVersion &quot;myapp.v1&quot; deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] ReleaseVersion &quot;myapp.v2&quot; will be deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] ReleaseVersion &quot;myapp.v2&quot; deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] ReleaseVersion &quot;myapp.v3&quot; will be deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] ReleaseVersion &quot;myapp.v3&quot; deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] Releases deleted.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] Tiller in &quot;kube-system&quot; namespace will be removed.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] Tiller &quot;deploy&quot; in &quot;kube-system&quot; namespace will be removed.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] Tiller &quot;deploy&quot; in &quot;kube-system&quot; namespace was removed successfully.</span><br><span class="line">2020/12/04 12:04:35 [Helm 2] Tiller &quot;service&quot; in &quot;kube-system&quot; namespace will be removed.</span><br><span class="line">2020/12/04 12:04:36 [Helm 2] Tiller &quot;service&quot; in &quot;kube-system&quot; namespace was removed successfully.</span><br><span class="line">2020/12/04 12:04:36 [Helm 2] Tiller in &quot;kube-system&quot; namespace was removed.</span><br><span class="line">2020/12/04 12:04:36 [Helm 2] Home folder &quot;/root/.helm&quot; will be deleted.</span><br><span class="line">2020/12/04 12:04:36 [Helm 2] Home folder &quot;/root/.helm&quot; deleted.</span><br><span class="line">2020/12/04 12:04:36 Helm v2 data was cleaned up successfully.</span><br></pre></td></tr></table></figure><h3 id="Helm的常用命令集合"><a href="#Helm的常用命令集合" class="headerlink" title="Helm的常用命令集合"></a>Helm的常用命令集合</h3><p>  有关 Helm 命令的常用说明</p><table><thead><tr><th>选项</th><th>说明</th></tr></thead><tbody><tr><td>create</td><td>创建 Chart 开发模板</td></tr><tr><td>env</td><td>打印 Helm client 的环境信息</td></tr><tr><td>plugin</td><td>管理 Helm plugin 功能</td></tr><tr><td>get</td><td>获取 Release 信息</td></tr><tr><td>status</td><td>显示 Release 的资源在集群中的状态</td></tr><tr><td>test</td><td>执行 Release 的测试用例</td></tr><tr><td>install</td><td>安装 Chart 包</td></tr><tr><td>upgrade</td><td>升级 Release 版本</td></tr><tr><td>rollback</td><td>回退 Release 版本</td></tr><tr><td>uninstall</td><td>删除 Release 版本</td></tr><tr><td>history</td><td>列出 Release 的历史版本</td></tr><tr><td>package</td><td>将某目录打包为 Chart 包</td></tr><tr><td>lint</td><td>检查 Chart 模板语法是否合规</td></tr><tr><td>repo</td><td>管理 Chart 仓库</td></tr><tr><td>pull</td><td>将 Chart 包下载到本地</td></tr><tr><td>search</td><td>在 Chart 仓库中搜索匹配条件的 Chart 包</td></tr><tr><td>show</td><td>显示某个 Chart 的信息</td></tr><tr><td>version</td><td>打印 Helm client 版本信息</td></tr></tbody></table><p><strong>常用的 Helm 命令</strong></p><p>列出当前所有 repo</p><p><code>helm repo list</code></p><p>添加名为 internal 的 repo ，仓库地址为internal&#x2F;Charts</p><p><code>helm add internal internal/Charts</code></p><p>在 repo 中搜索名称为 mysql 的 Chart 包</p><p><code>helm search repo mysql</code></p><p>下载 Chart 包到本地</p><p><code>helm pull stable/mysql</code></p><p>安装 Chart 包到集群，将 release 命名为 instance01</p><p><code>helm install stable/mysql --name instance01</code></p><p>查询 myapp 这个 release 的状态<br><code>helm status myapp</code></p><p>列出 myapp 的历史 release 版本</p><p><code>helm history myapp</code></p><p>将 myapp 这个 release 的版本回退到版本 2</p><p><code>helm rollback myapp 2</code></p><p>将 myapp 这个 release 升级到版本 4</p><p><code>helm upgrade myapp --version 4</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;title-Helm入门date-2021-01-21-18-00-15tags-“K8S”-”Helm”&quot;&gt;&lt;a href=&quot;#tit</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>同时启用 tcp_timestamp 和 tcp_tw_recycle 导致业务不通排查记录</title>
    <link href="https://guoltan.github.io/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/"/>
    <id>https://guoltan.github.io/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/</id>
    <published>2024-02-28T07:11:25.000Z</published>
    <updated>2024-03-14T03:33:38.913Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-问题背景"><a href="#1-问题背景" class="headerlink" title="1. 问题背景"></a>1. 问题背景</h1><p>  经项目同学反馈，有个环境出现了一个怪异的问题，环境中有三台机器，一台一体机服务器(172.16.1.201)，一台 Linux 客户端(172.16.1.202)，一台 Windows 客户端(172.16.1.204)。在一体机服务器上有个 web-server 的应用，通过 nodePort 在机器上暴露了端口。让外部通过该端口访问这个应用。但奇怪的现象是，一体机和 Linux 客户端都可以通过 nodePort 发布的端口正常连通该机器的业务端口，但是 Windows 客户端不可以。整体关系如下图：</p><img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/1.jpg" class="" title="1.jpg"><h1 id="2-排查过程"><a href="#2-排查过程" class="headerlink" title="2. 排查过程"></a>2. 排查过程</h1><h2 id="2-1-测试业务配置问题"><a href="#2-1-测试业务配置问题" class="headerlink" title="2.1. 测试业务配置问题"></a>2.1. 测试业务配置问题</h2><p>  此类问题应该优先确认网络侧的信息，而当时忘记使用 netstat -s 直接查看网络丢包信息。所以先选择通过查看应用配置的方式去确认原因。当时假设出现的问题可能如下：</p><ul><li><p>因为反馈 linux 客户端可以正常连通，但是 windows 客户端不可以，可能是 windows 发送的数据包有某些字段服务器接受不了（从抓包结果看有带 EW flags）</p></li><li><p>业务构建的 nginx 镜像有一些特殊配置，或者镜像使用的函数库有一些 BUG 导致 windows 发的包无法接收。</p></li><li><p>某个资源定义的配置影响到业务</p></li></ul><h3 id="2-1-1-排查服务端不接收-ECN-包导致丢包"><a href="#2-1-1-排查服务端不接收-ECN-包导致丢包" class="headerlink" title="2.1.1. 排查服务端不接收 ECN 包导致丢包"></a>2.1.1. 排查服务端不接收 ECN 包导致丢包</h3><p>  因为项目侧同学提供的抓包看，windows 发的包是有带 EW 字段的，该字段用于 ECN 功能。如果服务端不接受 ECN 功能，有导致异常的可能。所以先检查了该配置。</p><p>cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_ecn</p><ul><li><p>如果返回 0，代表不支持 tcp_ecn 的发送和接收。</p></li><li><p>如果返回 1，代表支持接收但不会主动发送这类报文。</p></li><li><p>如果返回 2，代表可以发也可以收 ECN 类的报文。</p></li></ul><p>  经确认现场的业务容器里面输出结果是 2，也就是环境未触发该问题。</p><h3 id="2-1-2-排查业务镜像问题"><a href="#2-1-2-排查业务镜像问题" class="headerlink" title="2.1.2. 排查业务镜像问题"></a>2.1.2. 排查业务镜像问题</h3><p>  先通过普通的 nginx 镜像重新下发一个 deployment，并配置 nodePort 的 service 进行连通性测试。此时发现 windows 客户端和 nginx 的业务连通性是正常的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 deployment 测试</span></span><br><span class="line">kubectl create deploy --image nginx:latest testweb --target-port 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 service 测试</span></span><br><span class="line">kubectl expose deploy testweb --port 80 --target-port 80 --name testweb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改类型为 NodePort，将 ClusterIP 替换为 NodePort，并在 Port 下追加一个 nodePort: 31805 的字段（测试端口，环境未使用）</span></span><br><span class="line">kubectl edit svc testweb</span><br></pre></td></tr></table></figure><p>  验证后发现基本连通性正常，此时怀疑是否和镜像有关，将镜像更新成 web-server 服务的镜像，并配置 nodePort 的 service 进行连通性测试。此时发现 windows 客户端和 nginx 的业务连通性还是正常的。到此可以推测问题可能和业务镜像无关。</p><h3 id="2-1-3-排查-deployment-配置问题"><a href="#2-1-3-排查-deployment-配置问题" class="headerlink" title="2.1.3. 排查 deployment 配置问题"></a><strong>2.1.3. 排查 deployment 配置问题</strong></h3><p>  由于镜像无误，此时尝试复制 web-server 的 YAML，重新配置一份副本。进行业务测试。此时复现了 windows 客户端请求业务不通的问题。</p><p>  由此现象可以得知，有个配置影响到了业务连通性，于是乎进行了配置删除的测试。测试的类型和结果如下：</p><ul><li><p>dnsConfig 配置  – 业务依旧不通</p></li><li><p>volumes,volumeMount – 业务依旧不通</p></li><li><p>SecurityContext – 业务依旧不通</p></li><li><p>LivenessProbe,ReadnessProbe – 业务正常连通。</p></li></ul><p>  经测试，基本上可以确认去除探针配置以后，业务就能够正常了。目前表现看是探针配置引发了业务异常。此时我先假设问题可能是：</p><ul><li><p>业务 BUG？</p></li><li><p>Kubelet 探针的 BUG？</p></li></ul><p>  顺着这个思路进行问题搜索，网上没有相关的案例。此时思路转变为追加探针以后，实际上 kubelet 会一直发送请求来测试业务的健康性。本质上存在建立连接的动作。此时定位的思路转变为如何确认建立多个连接下，新连接会丢包的原因。</p><h2 id="2-2-确认容器丢包原因"><a href="#2-2-确认容器丢包原因" class="headerlink" title="2.2. 确认容器丢包原因"></a><strong>2.2. 确认容器丢包原因</strong></h2><p>  基于 2.1 的排查结果，最后确认的思路是需要确认什么时候建立多个 TCP 连接会导致业务丢包。为此刚好记起通过 netstat -s 通常情况下可以获取到业务丢包的原因。需要基于该指令进行进一步的问题范围收敛。重新为测试 Pod 添加回探针以后，在 windows 客户端上进行连通性测试。在对应 Pod 容器进行 netstat -s ，结果如下：</p><p><strong>在该容器的网络命名空间查询</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker inspect -f &#x27;&#123;&#123; .State.Pid &#125;&#125;&#x27;</span><br><span class="line">nsenter -t PID -n netstat -s</span><br></pre></td></tr></table></figure><p><strong>输出的结果</strong></p><img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/2.jpg" class="" title="2.jpg"><p>从截图来看，已经可以找到丢包的原因，就是因为 timestamps 的问题导致业务丢包。此时问题已经比较明确了。此时通过搜索关键字，发现同时启用 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps 会导致业务丢包。（具体原因分析参考 3 原因分析章节）</p><h2 id="2-3-解决方法"><a href="#2-3-解决方法" class="headerlink" title="2.3. 解决方法"></a><strong>2.3. 解决方法</strong></h2><p>  通过原因分析，可以得知问题是因为同时启用 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps，此时只能开启一个，从优先级判断，tcp_tw_recycle 优先级更低，需要将其关闭。</p><p>  由于业务容器内没有 &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_tw_recycle 参数，于是乎选择在宿主机上关闭 tcp_tw_recycle 并重启业务进行修复。最终效果如下</p><img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/3.jpg" class="" title="3.jpg"><h1 id="3-原因分析"><a href="#3-原因分析" class="headerlink" title="3. 原因分析"></a><strong>3. 原因分析</strong></h1><h2 id="3-1-TimeStamps-用途"><a href="#3-1-TimeStamps-用途" class="headerlink" title="3.1. TimeStamps 用途"></a><strong>3.1. TimeStamps 用途</strong></h2><p>  timestamps 机制的用处：</p><ul><li><p>防止上一个TCP连接的延迟的数据包，影响到新的 TCP 连接。</p></li><li><p>提供更为精确的 RTT 计算能力。</p></li></ul><p>  timestamps 的组成</p><img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/4.jpg" class="" title="4.jpg"><p>  其中，timestamp value（简称 tsval）用于存储当前请求报文的时间戳值，timestamp echo reply（简称 TSecr）用于存储最近接收到的对端所发送报文的 TSopt 选项中包含的 TSval 时间戳值。</p><p>  那 timestamps 的值 tsval 是怎么获取的，实际上是通过 CPU tick 来获取的（也就是说每个机器的 CPU tick 是很难保持相同的）。</p><h2 id="3-2-windows-机访问-web-server-为什么会导致丢包？"><a href="#3-2-windows-机访问-web-server-为什么会导致丢包？" class="headerlink" title="3.2. windows 机访问 web-server 为什么会导致丢包？"></a><strong>3.2. windows 机访问 web-server 为什么会导致丢包？</strong></h2><h3 id="3-2-1-tcp-timestamps-和-tcp-tw-recycle-的作用"><a href="#3-2-1-tcp-timestamps-和-tcp-tw-recycle-的作用" class="headerlink" title="3.2.1. tcp_timestamps 和 tcp_tw_recycle 的作用"></a><strong>3.2.1. tcp_timestamps 和 tcp_tw_recycle 的作用</strong></h3><p>  tcp_timestamps 用于启用时间戳特性以提供更加精确的 RTT 计算以及额外特性功能的支持（如 PAWS），而 tcp_tw_recycle 用于配置 TIME_WAIT 快速回收以增快资源回收。</p><p>  tcp_timestamps 和 tcp_tw_recycle 选项开启之后，自动开启 per-host 的 PAWS 机制，也就是会对对端 IP 进行 PAWS，而不是 IP + 端口的四元组进行 PAWS 检查。</p><h3 id="3-2-2-同时开启-tcp-timestamps-和-tcp-tw-recycle-导致丢包的场景"><a href="#3-2-2-同时开启-tcp-timestamps-和-tcp-tw-recycle-导致丢包的场景" class="headerlink" title="3.2.2. 同时开启 tcp_timestamps 和 tcp_tw_recycle 导致丢包的场景"></a><strong>3.2.2. 同时开启 tcp_timestamps 和 tcp_tw_recycle 导致丢包的场景</strong></h3><p>  根据内核参数的解释，如果同时开启 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps 时，此时服务端在处理数据包时会要求报文中的 timestamps 字段是递增的，如果服务端运行在 NAT 环境下，处理来自不同客户端的请求时会有可能触发丢包。下图以一个 CLB 的场景为例进行解释：</p><img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/5.jpg" class="" title="5.jpg"><p>  当客户端 1 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME-WAIT 状态的连接后，客户端 2 也通过 NAT 网关和服务器建立 TCP 连接，注意客户端 1 和 客户端 2 因为通过 CLB 的 NAT，所以对服务端来说是使用相同的 IP 地址建立 TCP 连接，如果客户端 2 的 timestamp 比 客户端 1 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 2 发来的 SYN 包。</p><h3 id="3-2-3-我们的-web-server-应用是怎么满足了-tcp-tw-recycle-丢包场景？"><a href="#3-2-3-我们的-web-server-应用是怎么满足了-tcp-tw-recycle-丢包场景？" class="headerlink" title="3.2.3. 我们的 web-server 应用是怎么满足了 tcp_tw_recycle 丢包场景？"></a><strong>3.2.3. 我们的 web-server 应用是怎么满足了 tcp_tw_recycle 丢包场景？</strong></h3><p>  <strong>根据上述的环境总结出的丢包触发规律如下：</strong></p><ul><li><p>tcp_timestamps 和 tcp_tw_recycle 处于启用状态。</p></li><li><p>服务端运行在 NAT 环境下，此时服务端接收到请求的客户端 IP 是相同的。</p></li><li><p>客户端 IP 实际上对应不同的客户端，短时间（60秒）内存在多个 TCP 请求业务。</p></li><li><p>多个客户端的 timestamps 存在偏差，报文收到的 timestamps opt 中的 tsval 时间要小于 peer 保存的 timestamp 时间。timestamp 来自 CPU Tick，非同一个节点很难保证相同的，所以这个时候时间戳基本上很难保证的线性递增的。</p></li></ul><p>  实际上 web-server 应用是通过 nodePort 类型的 service 进行发布的，而 kube-proxy 无论是 iptables 还是 ipvs 模式，他们都是通过 NAT 的方式来实现网络，此时服务端就满足了 NAT 环境的要素。而我们业务配置了探针，这就满足了多个客户端在短时间内存在多个 TCP 请求业务的场景。此时 windows 客户端发包时，就会被进行报文时间戳的比较。如果报文低于已经存在 peer 最近一次时间戳，就会满足丢包的条件。</p><p>  <strong>本次案例 windows 访问不通的大体总结如下：</strong></p><p> 1. 一体机的 kubelet 进程发起 TCP 连接 web-server 的 80 端口进行探测。然后关闭连接。由于开启了 recycle 此时 time_wait 的连接会被快速回收掉，但 web-server 容器环境会记录一体机这个 peer 的时间戳（可以通过 ip tcp_metrics show 的方式去查 看，其中 tw_ts 字段缓存了该信息）也就是连接虽然释放了，但是 peer 信息还在系统中留存。</p><p> 2. 此时 windows 客户端发起请求，请求发送到一体机暴露的 30180 端口以后，会被进行 NAT 转换，再传递到后端 web-server 容器。</p><p> 3. 此时 web-server 容器环境因为配置了 tcp_timestamp 和 tcp_tw_recycle 这时候是通过 per-host 的PAWS 进行检查。此时会将 windows 新发的报文提取 tsval 和 peer 里面留存的 timestamps 进行比较。</p><p> 4. 由于 windows 客户端和一体机所在节点的 CPU Tick 不同，导致他们生成的 timestamp 也不相同。只要 windows CPU tick 计算出的时间比一体机的小，这时候就会导致 web-server 将 windows 客户端发起的 SYN 进行丢弃。造成业务不通。</p><h3 id="3-2-4-关闭-tcp-tw-recycle-以后，为什么会正常？"><a href="#3-2-4-关闭-tcp-tw-recycle-以后，为什么会正常？" class="headerlink" title="3.2.4. 关闭 tcp_tw_recycle 以后，为什么会正常？"></a>3.2.4. 关闭 tcp_tw_recycle 以后，为什么会正常？</h3><p>  一旦关闭 tcp_tw_recycle ，PAWS 就不会 per-host 去检查，而是以四元组的方式进行检查，这样一体机节点和 Windows 就算发起连接会被 NAT 成相同的 IP。也会因为不同的源端口而被区分。</p><h3 id="3-2-5-为什么现场说-Linux-客户端可以正常的联通业务？"><a href="#3-2-5-为什么现场说-Linux-客户端可以正常的联通业务？" class="headerlink" title="3.2.5. 为什么现场说 Linux 客户端可以正常的联通业务？"></a>3.2.5. 为什么现场说 Linux 客户端可以正常的联通业务？</h3><p>  我对此问题的假设是满足了下面其中一个条件，所以没有触发丢包问题：</p><ul><li><p>这台 Linux 客户端和服务端拥有相同的 CPU Clock 或者比一体机大</p></li><li><p>Linux 客户端节点已关闭 timestamps</p></li></ul><p>参考连接</p><p><a href="https://blog.csdn.net/Ivan_Wz/article/details/112250255">踩坑内核参数tcp_tw_recycle_Ivan_Wz的博客-CSDN博客</a> - 踩坑内核参数tcp_tw_recycle</p><p><a href="https://www.cnblogs.com/sunsky303/p/12818009.html">不要启用 net.ipv4.tcp_tw_recycle - sunsky303 - 博客园</a> - 不要启用 net.ipv4.tcp_tw_recycle</p><p><a href="https://blog.csdn.net/thehunters/article/details/122240272">客户端第二次连接失败，SYN包发了，没有收到服务端回 SYN+ACK ，SYN包被丢弃了_服务器句柄资源不足 丢弃 syn包_thehunters的博客-CSDN博客</a> – 客户端第二次连接失败，SYN包发了，没有收到服务端回 SYN+ACK ，SYN包被丢弃了</p><p><a href="https://zhuanlan.zhihu.com/p/64690239">https://zhuanlan.zhihu.com/p/64690239</a> – Linux内核协议栈丢弃SYN报文的主要场景剖析</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-问题背景&quot;&gt;&lt;a href=&quot;#1-问题背景&quot; class=&quot;headerlink&quot; title=&quot;1. 问题背景&quot;&gt;&lt;/a&gt;1. 问题背景&lt;/h1&gt;&lt;p&gt;  经项目同学反馈，有个环境出现了一个怪异的问题，环境中有三台机器，一台一体机服务器(172.16.1.</summary>
      
    
    
    
    
    <category term="排障" scheme="https://guoltan.github.io/tags/%E6%8E%92%E9%9A%9C/"/>
    
  </entry>
  
  <entry>
    <title>某环境接口调用偶现 502 问题分析</title>
    <link href="https://guoltan.github.io/2024/02/15/%E6%9F%90%E7%8E%AF%E5%A2%83%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8%E5%81%B6%E7%8E%B0%20502%20%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/"/>
    <id>https://guoltan.github.io/2024/02/15/%E6%9F%90%E7%8E%AF%E5%A2%83%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8%E5%81%B6%E7%8E%B0%20502%20%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</id>
    <published>2024-02-15T11:00:25.000Z</published>
    <updated>2024-03-13T16:04:50.956Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a><strong>1. 概述</strong></h1><p>  本文的内容主要是对 TI361演示环境偶发 502 该问题的处理进行一次记录。其中章节 2 - 章节 3 主要讲述了问题现象、整个问题的排查过程、以及最后的解决方法。章节 4 主要是记录本次排查问题中对于某些问题的思考。<strong>章节 5</strong> 主要是对 2 ~ 3 章节的排查过程进行一次思路整理。如果需要定位类似问题，可以直接参考<strong>章节 5</strong> 的内容操作。</p><h1 id="2-问题现象"><a href="#2-问题现象" class="headerlink" title="2. 问题现象"></a><strong>2. 问题现象</strong></h1><p>  用户反馈 361 演示环境存在异常，在使用的过程中偶发出现 Bad Gateway 的提示，接口返回 502。并且出现异常的接口不限。具体现象如下图：</p><h1 id="3-排查过程"><a href="#3-排查过程" class="headerlink" title="3. 排查过程"></a><strong>3. 排查过程</strong></h1><h2 id="3-1-确认故障点"><a href="#3-1-确认故障点" class="headerlink" title="3.1. 确认故障点"></a><strong>3.1. 确认故障点</strong></h2><p>  出现该问题后，根据研发同学最初提供了思路，建议先去排查下 ti-gateway 的日志看看是否存在错误，以及集群机器负载是否正常。</p><h3 id="3-1-1-确认集群负载情况以及-Pod-状态"><a href="#3-1-1-确认集群负载情况以及-Pod-状态" class="headerlink" title="3.1.1. 确认集群负载情况以及 Pod 状态"></a><strong>3.1.1. 确认集群负载情况以及 Pod 状态</strong></h3><p>  首先确认机器负载，通过 grafana 查看各个 Node 的机器负载，目前 CPU&#x2F;内存的使用率没有特别高的，属于正常。该问题可能不是机器负载导致的。</p><h3 id="3-1-2-确认-ti-gateway-workload-状态"><a href="#3-1-2-确认-ti-gateway-workload-状态" class="headerlink" title="3.1.2. 确认  ti-gateway-workload 状态"></a><strong>3.1.2. 确认  ti-gateway-workload 状态</strong></h3><p>  查看 gateway 的状态，需要考虑的点：</p><p>· gateway的状态是否正常，是否有出现过重启的问题（OOM或者程序异常退出）</p><p>· 是否有异常的服务日志</p><p>· 如何确认流量是否达到了 ti-gateway</p><h4 id="3-1-2-1-确认-gateway-pod-状态"><a href="#3-1-2-1-确认-gateway-pod-状态" class="headerlink" title="3.1.2.1. 确认 gateway pod 状态"></a><strong>3.1.2.1. 确认 gateway pod 状态</strong></h4><p>  查看 gateway 的状态， Pod 未发生过重启，此时可以说明该问题不会是 gateway 程序异常退出或者 OOM 引发的。 </p><h4 id="3-1-2-2-确认-gateway-的错误日志"><a href="#3-1-2-2-确认-gateway-的错误日志" class="headerlink" title="3.1.2.2. 确认 gateway 的错误日志"></a><strong>3.1.2.2. 确认 gateway 的错误日志</strong></h4><p>  进入 ti-gateway-workload 服务，查看 &#x2F;usr&#x2F;local&#x2F;apisix&#x2F;logs 目录下的 error.log，没发现相关调用接口的异常日志。</p><h4 id="3-1-2-3-排查请求是否有达到-gateway"><a href="#3-1-2-3-排查请求是否有达到-gateway" class="headerlink" title="3.1.2.3. 排查请求是否有达到 gateway"></a><strong>3.1.2.3. 排查请求是否有达到 gateway</strong></h4><p>  从 gateway 的状态和服务日志来看，请求可能没有达到 gateway。需要一些方法去界定流量是否有抵达过 gateway。参考研发同学的 iwiki <a href="/pages/viewpage.action?pageId=596973203"><u><span class="16">Ti网关服务排查手册</span></u></a> 提供的思路进行排查。里面提到一个标准的错误场景，调用接口 5xx 的思路。如下图</p><p>  通过该说法，可以推断，如果流量经过网关，那网关响应会重写一个响应头部 <strong>X-TiGateway-Upstream-Status</strong> 通过该头部我们可以去判断流量有没有经过 ti-gateway。如果未产生该头部，则代表流量没达到 gateway 则可能流量没有达到 gateway。当前的响应头部没有  <strong>X-TiGateway-Upstream-Status</strong> 此时可以推断流量没有抵达 gateway，需要向上排查。</p><h3 id="3-1-3-确认-alb-状态"><a href="#3-1-3-确认-alb-状态" class="headerlink" title="3.1.3. 确认 alb 状态"></a><strong>3.1.3. 确认 alb 状态</strong></h3><p>  gateway 的上一层应该是 ingress-controller，环境使用的是灵雀 TKE 作为底座，此时需要对 alb 进行排查。</p><p>  查看 alb 的状态，需要考虑的点和 gateway 类似：</p><p>· gateway的状态是否正常，是否有出现过重启的问题（OOM或者程序异常退出）</p><p>· 是否有异常的服务日志，请求日志可以通过 &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log（错误日志） 以及 &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;logs&#x2F;access.log（访问日志）</p><p>· 如何确认流量是否达到了 alb2</p><h4 id="3-1-3-1-确认-alb-pod-状态"><a href="#3-1-3-1-确认-alb-pod-状态" class="headerlink" title="3.1.3.1. 确认 alb pod 状态"></a><strong>3.1.3.1. 确认 alb pod 状态</strong></h4><p>  查看 alb 状态，发现 global-alb2 虽然状态都是 Running 的，但是 192.168.25.19 以及 192.168.25.5 节点上的 global-alb2 分别出现过重启。</p><p>  从容器的状态来看 192.168.25.19 的 alb 是出现过 Exit Code 137（OOM）的，但触发的事件节点是 2022.7.17 的 19:38。而我们当前出现故障的时间节点是 2022.7.18 的下午。初步可以推断不是由于该原因引起的。</p><p>  192.168.25.5 的容器也出现过退出，但同样的，时间节点对不上。</p><p>通过上述的分析，引发该问题的原因应该不是 alb 程序退出或者 OOM 等原因</p><h4 id="3-1-3-2-确认-alb-的错误日志"><a href="#3-1-3-2-确认-alb-的错误日志" class="headerlink" title="3.1.3.2. 确认 alb 的错误日志"></a><strong>3.1.3.2. 确认 alb 的错误日志</strong></h4><p>  此时需要查询该时间节点下，alb 是否有产生过相关日志，进入 global 分别执行过过滤指令，得到的结果如下。（注：当时排查时未截图，其中 10:18:32 是当时请求失败的时间节点。+8 的话就是 CST 时间的 18:18）</p><p>&#x2F;var&#x2F;log&#x2F;nginx # grep “10:18:” * | grep 07&#x2F;18</p><p>error.log:2022&#x2F;07&#x2F;18 10:18:01 [warn] 53#53: *142405699 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647558, client: 159.75.199.13, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p><p>error.log:2022&#x2F;07&#x2F;18 10:18:21 [warn] 53#53: *142407716 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647559, client: 159.75.198.73, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p><p>error.log:2022&#x2F;07&#x2F;18 10:18:42 [warn] 53#53: *142408115 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647560, client: 159.75.199.142, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p><p>error.log:2022&#x2F;07&#x2F;18 10:18:54 [warn] 53#53: *142408413 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647561, client: 159.75.198.73, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p><p>&#x2F;var&#x2F;log&#x2F;nginx #</p><p>  通过查询，没有在 2022&#x2F;07&#x2F;18 10:18:32 这个时间看到明显的报错，相关时间给出的都是一些告警信息。同样的命令在其他节点也查询过，得不到关键的信息。</p><h4 id="3-1-3-3-分析请求是否有达到-alb"><a href="#3-1-3-3-分析请求是否有达到-alb" class="headerlink" title="3.1.3.3. 分析请求是否有达到 alb"></a><strong>3.1.3.3. 分析请求是否有达到 alb</strong></h4><p>  将所有节点的 access.log 导出进行过滤，在 2022&#x2F;07&#x2F;18 10:18:32 这个时间没有看到请求。推测这个请求并没有达到平台。</p><p>[root@VM-25-41-tlinux ~&#x2F;log&#x2F;alb_log]# grep DescribeGroupDevices * | grep 10:18</p><p>192.168.25.19_access.log:192.168.25.41 - - [18&#x2F;Jul&#x2F;2022:10:18:57 +0000] “POST &#x2F;gateway?action&#x3D;DescribeGroupDevices HTTP&#x2F;1.1” 200 678 “<a href="https://demo-v36.tiplatform.qq.com/tim/device">https://demo-v36.tiplatform.qq.com/tim/device</a>“ “Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;103.0.0.0 Safari&#x2F;537.36”</p><p>192.168.25.41_access.log:159.75.198.50 - - [18&#x2F;Jul&#x2F;2022:10:18:05 +0000] “POST &#x2F;gateway?action&#x3D;DescribeGroupDevices HTTP&#x2F;1.1” 200 654 “<a href="https://demo-v36.tiplatform.qq.com/tim/Application/deviceControl">https://demo-v36.tiplatform.qq.com/tim/Application/deviceControl</a>“ “Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;103.0.0.0 Safari&#x2F;537.36”</p><p>192.168.25.5_access.log:192.168.25.41 - - [18&#x2F;Jul&#x2F;2022:10:18:07 +0000] “POST &#x2F;gateway?action&#x3D;DescribeGroupDevices HTTP&#x2F;1.1” 200 678 “<a href="https://demo-v36.tiplatform.qq.com/tim/device">https://demo-v36.tiplatform.qq.com/tim/device</a>“ “Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;103.0.0.0 Safari&#x2F;537.36”</p><h3 id="3-1-4-分析问题是否由-WAF-引发"><a href="#3-1-4-分析问题是否由-WAF-引发" class="headerlink" title="3.1.4. 分析问题是否由 WAF 引发"></a><strong>3.1.4. 分析问题是否由 WAF 引发</strong></h3><p>  经过上述的排查，怀疑请求没有达到平台，而是在 WAF 侧直接返回的失败。通过联系腾讯云助手（cloud IT support）咨询。将情况描述以后，得到的确认是该请求有到 WAF，并且 WAF 也提交给了后端，但是后端得到的返回是 502。也就是说该问题<strong>还有节点被遗漏</strong>了，需要继续排查。</p><h2 id="3-2-重新梳理流量路径"><a href="#3-2-重新梳理流量路径" class="headerlink" title="3.2. 重新梳理流量路径"></a><strong>3.2. 重新梳理流量路径</strong></h2><p>  基于上述的排查进展，梳理出的情况是有部分流量节点没有去定位，通过（学习）查看 WAF 关联的节点配置，找到了其关联的源站 IP。通过该 IP 成功找到了关联的后端。发现它实际上是关联的 HA VIP。也就是说实际上环境还存在一个 keepalived 的组件在提供 VIP 功能。还需要排查此节点。</p><p>  基于上述排查过程，简单总结出的流量链路如下</p><p>浏览器 -&gt; DNS -&gt; WAF -&gt; EIP -&gt; HA VIP(Keepalived) -&gt; alb -&gt; ti-gateway -&gt; 业务pod</p><p>  每个节点角色功能如下</p><p>· 浏览器 – 请求发起者</p><p>· DNS – 提供站点域名解析，将请求引入 WAF</p><p>· WAF – 提供 WEB 应用防火墙功能，负责拦截 Web 攻击，过滤流量后将请求转发给后端服务</p><p>· EIP 公网 IP – 提供公网访问功能</p><p>· HA VIP – 虚拟 IP，提供网络层高可用功能结合 TKE 自建 VIP 的 keepalived 使用。实现平台网络层高可用。</p><p>· alb – 灵雀自研的 ingress-controller，基于 URL 将请求分发到对应的后端业务 Pod。</p><p>· ti-gateway – TI平台自研的网关组件，负责将 TI 业务接口的调用分发到指定的业务 Pod。</p><p>· 业务 Pod – 提供业务处理逻辑的进程。</p><h2 id="3-3-分析-keepalived-异常"><a href="#3-3-分析-keepalived-异常" class="headerlink" title="3.3. 分析 keepalived 异常"></a><strong>3.3. 分析 keepalived 异常</strong></h2><h3 id="3-3-1-排查-keepalived-日志"><a href="#3-3-1-排查-keepalived-日志" class="headerlink" title="3.3.1. 排查 keepalived 日志"></a><strong>3.3.1. 排查 keepalived 日志</strong></h3><p>  重新梳理流量路径后，需要对 keepalived 进行排查，通过 kubectl logs -n kube-system keepalived-x.x.x.x &gt; keepalived-x.x.x.x.log 获取日志，发现了两个比较醒目的错误，内容如下：</p><h1 id="频繁出现的日志，表示网络环境中有重复的-VRID"><a href="#频繁出现的日志，表示网络环境中有重复的-VRID" class="headerlink" title="频繁出现的日志，表示网络环境中有重复的 VRID"></a>频繁出现的日志，表示网络环境中有重复的 VRID</h1><p>Mon Jul 18 10:41:52 2022: (VI_1) ip address associated with VRID 10 not present in MASTER advert : 192.168.25.13</p><p>Mon Jul 18 10:41:52 2022: (VI_1) ip address associated with VRID 10 not present in MASTER advert : 192.168.25.13</p><h1 id="这一段日志表示出现过切主的情况"><a href="#这一段日志表示出现过切主的情况" class="headerlink" title="这一段日志表示出现过切主的情况"></a>这一段日志表示出现过切主的情况</h1><p>Mon Jul 18 10:41:52 2022: (VI_1) Entering MASTER STATE</p><p>Mon Jul 18 10:41:52 2022: (VI_1) ip address associated with VRID 10 not present in MASTER advert : 192.168.25.13</p><p>Mon Jul 18 10:41:52 2022: (VI_1) Received advert from 192.168.25.5 with lower priority 100, ours 100, forcing new election</p><p>Mon Jul 18 10:41:52 2022: (VI_1) Master received advert from 192.168.25.41 with same priority 100 but higher IP address than ours</p><p>Mon Jul 18 10:41:52 2022: (VI_1) Entering BACKUP STATE</p><p>  由于日志中发生过切主，而切主的过程中业务是会出现闪断的。推测问题与 keepalived 切主有关系。接下来需要一些方法去验证。</p><h3 id="3-3-2-验证问题是否和-keepalived-频繁切主有关"><a href="#3-3-2-验证问题是否和-keepalived-频繁切主有关" class="headerlink" title="3.3.2. 验证问题是否和 keepalived 频繁切主有关"></a><strong>3.3.2. 验证问题是否和 keepalived 频繁切主有关</strong></h3><h4 id="3-3-2-1-通过日志确认"><a href="#3-3-2-1-通过日志确认" class="headerlink" title="3.3.2.1. 通过日志确认"></a><strong>3.3.2.1. 通过日志确认</strong></h4><p>  以下图请求为例，于 GMT 时间的 Mon, 18 Jul 2022 15:12:07 进行的请求失败了。我在同一个时间段内，我通过这个时间节点去查询 gateway、alb 都无法查询到记录。也就是说请求在进入 alb 之前已经丢失了。</p><p>  通过查询 keepalived 的日志，可以发现，这个时间段发生了重新选举，也就是切主。这说明切主的过程中可能会造成请求异常。</p><h4 id="3-3-2-2-通过请求对比"><a href="#3-3-2-2-通过请求对比" class="headerlink" title="3.3.2.2. 通过请求对比"></a><strong>3.3.2.2. 通过请求对比</strong></h4><p>  通过比较访问 HAVIP 的情况下（就是直接访问 <a href="https://demo-v36.tiplatform.qq.com/"><u><span class="16">https://demo-v36.tiplatform.qq.com/</span></u></a> ）以及不访问 HAVIP（也就是直接访问对应节点的公网IP），发现同一时间段内，走 HA VIP 的请求是不通的。绕过 HA VIP 的请求是可以正常通的。</p><p><strong>正常情况下的请求比较</strong></p><p><strong>异常情况下的请求比较</strong></p><p>异常情况下有出现强制选举日志</p><p>  经此对比，基本上可以实锤问题就是 keepalived 切主引发的。</p><h3 id="3-3-3-排查-Keepalived-切主原因"><a href="#3-3-3-排查-Keepalived-切主原因" class="headerlink" title="3.3.3. 排查 Keepalived 切主原因"></a><strong>3.3.3. 排查 Keepalived 切主原因</strong></h3><p>  为了排查切主原因，此时查看 Keepalived 的配置、日志，得出信息如下：</p><p>· 自建 VIP 的 Keepalived 配置是使用了非抢占模式，每个 keepalived 配置相同优先级。在初步启动时协商 Keepalived Master，在成员相同优先级的情况下，IP 地址越大，优先级越高。</p><p>· Keepalived 使用 &#x2F;etc&#x2F;kubernetes&#x2F;keepalived&#x2F;check.sh 作为 track 条件。目前该脚本的执行情况来看，除非执行时间过长（超出计时器配置），不然是不会返回 1 的。</p><p>· 从日志中只能看到切主，并没有提到是因为 track 导致优先级下降导致切主。应该不是 track 的原因引发的。</p><p>· 日志时刻在刷新 vrid 冲突问题。理论上 vrid 冲突确实会影响业务。</p><p>  基于初步诊断的结果，目前怀疑导致切主的问题是因为 vrid 冲突，需要先解决该问题后持续观察业务是否恢复正常。</p><h2 id="3-4-修复-vrid-冲突并验证功能"><a href="#3-4-修复-vrid-冲突并验证功能" class="headerlink" title="3.4. 修复 vrid 冲突并验证功能"></a><strong>3.4. 修复 vrid 冲突并验证功能</strong></h2><h3 id="3-4-1-找到-vrid-冲突源"><a href="#3-4-1-找到-vrid-冲突源" class="headerlink" title="3.4.1. 找到 vrid 冲突源"></a><strong>3.4.1. 找到 vrid 冲突源</strong></h3><p>  刚开始想通过 subnet 判断冲突源，通过查看环境清单，只有一套环境处于同一个 subnets。推测修改该环境 vrid 就可以解决问题。</p><p>  此时通过在集群任意节点上，执行 tcpdump vrrp 指令，这时候发现冲突的 vrid 有很多套环境。。。。所以这个时候还是得修改演练环境的 vrid。</p><h3 id="3-4-2-修改-keepalived-的-vrid"><a href="#3-4-2-修改-keepalived-的-vrid" class="headerlink" title="3.4.2. 修改 keepalived 的 vrid"></a><strong>3.4.2. 修改 keepalived 的 vrid</strong></h3><p>  在 global 3 台 master 分别修改配置文件 vim &#x2F;etc&#x2F;kubernetes&#x2F;keepalived&#x2F;keepalived.conf</p><p>vrrp_instance VI_1 {</p><p>    state BACKUP</p><p>    interface eth0</p><p>    virtual_router_id 250 #将此处的 virtual_router_id 从 10 替换成 250.</p><p>    priority 100</p><p>    advert_int 1</p><p>    authentication {</p><p>        auth_type PASS</p><p>        auth_pass 1111</p><p>    }</p><p>    virtual_ipaddress {</p><p>        192.168.25.13</p><p>    }</p><p>    track_script {</p><p>        check</p><p>    }</p><p>}</p><p>  重启 keepalived 服务</p><h1 id="获取-docker-id"><a href="#获取-docker-id" class="headerlink" title="获取 docker id"></a>获取 docker id</h1><p>[root@vm-25-19-tlinux ~]# docker ps -a | grep keepalived</p><p>417092ae5316   c3efc585784a                                                          “&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;kee…”   4 months ago   Up 31 hours                           k8s_keepalived_keepalived-192.168.25.19_kube-system_c915363bfce6a644ac7c8e054aeae619_7</p><p>4f22d8a93303   192.168.25.13:60080&#x2F;tkestack&#x2F;pause:3.2                                “&#x2F;pause”                 4 months ago   Up 4 months                           k8s_POD_keepalived-192.168.25.19_kube-system_c915363bfce6a644ac7c8e054aeae619_1</p><h1 id="重启-keepalived-容器"><a href="#重启-keepalived-容器" class="headerlink" title="重启 keepalived 容器"></a>重启 keepalived 容器</h1><p>docker restart 417092ae5316</p><p>  重启服务后，查看 keepalived 日志，未出现过异常日志。</p><h1 id="4-排查过程中的一些思考"><a href="#4-排查过程中的一些思考" class="headerlink" title="4. 排查过程中的一些思考"></a><strong>4. 排查过程中的一些思考</strong></h1><h2 id="4-1-怎么快速抓-VRRP-包，查看-vrid-冲突源？"><a href="#4-1-怎么快速抓-VRRP-包，查看-vrid-冲突源？" class="headerlink" title="4.1. 怎么快速抓 VRRP 包，查看 vrid 冲突源？"></a><strong>4.1. 怎么快速抓 VRRP 包，查看 vrid 冲突源？</strong></h2><p>通过在集群任意节点上，执行 tcpdump vrrp 指令即可。如果出现多个IP使用了相同的 vrid，代表他们之间 vrid 是冲突的。类似输出如下图。</p><p>如果希望统计有哪些环境出现了冲突，可以将捕获结果导入到 wireshark，通过 统计 -&gt; Ipv4 Staticitsc -&gt; All Adress 可以获取本次捕获所有包的 IP 地址列表。通过显示过滤器，使用过滤条件 vrrp.virt_rtr_id &#x3D;&#x3D; VRID 可以筛选出需要确认冲突的 IP。类似如下的输出。</p><p>  基于 VRRP 的原理，只有 Master 节点会主动给成员发心跳报文。而一个 VRRP 实例代表一个 Keepalived，也就是一套环境。通过这个方法就可以得出有哪些环境存在冲突需要修复。</p><h2 id="4-2-为什么-ifconfig-看不到-keepalived-这个设备？"><a href="#4-2-为什么-ifconfig-看不到-keepalived-这个设备？" class="headerlink" title="4.2. 为什么 ifconfig 看不到 keepalived 这个设备？"></a><strong>4.2. 为什么 ifconfig 看不到 keepalived 这个设备？</strong></h2><p>  在验证问题时想查看 keepalived 网口的信息，此时发现了一个现象，通过 ifconfig 是没有办法直接显示 keepalived 的。但是 ifconfig keepalived 可以输出结果。输出如下：</p><h2 id="4-3-为什么在-VRID-冲突的情况下，环境依然可用？"><a href="#4-3-为什么在-VRID-冲突的情况下，环境依然可用？" class="headerlink" title="4.3. 为什么在 VRID 冲突的情况下，环境依然可用？"></a><strong>4.3. 为什么在 VRID 冲突的情况下，环境依然可用？</strong></h2><p>  keepalived 是怎么知道 vrid 冲突的，VRRP报文里面有携带 vrid 和 vip 字段，通过判断本地配置文件的 vrid 和 vip 和报文中是否出现 vrid 一样但 vip 不同的报文即可。</p><p>  理论上 keepalived 的虚拟 MAC 地址是通过 vrid 去决定的，如果两组 vrrp 实例使用了相同的 vrid，那么结果就是他们协商通过以后会发送免费 ARP 报文，MAC 地址相同。此时对于物理网络来说应该会产生 MAC 地址漂移的问题。对外体现就是网络不稳定，通信时断时续。</p><p>  目前原因还在确认，从自己抓包测试的结果初步推断和腾讯云实现虚拟网络的机制(vpc.ko) 有关，目前还在确认中。中间可能有一层网络封装屏蔽了 MAC 冲突的问题。最后呈现的结果就是虽然有 VRID 冲突，但是业务还是可用。</p><h2 id="4-4-VRID-冲突会导致切主？"><a href="#4-4-VRID-冲突会导致切主？" class="headerlink" title="4.4. VRID 冲突会导致切主？"></a><strong>4.4. VRID 冲突会导致切主？</strong></h2><p>  当前通过配置修改验证，解决了 VRID 冲突问题以后，偶发切主现象确实消失了。具体原因还没有理出来。</p><h2 id="4-5-请求回包的过程中，keepalived-切主是否会有影响？"><a href="#4-5-请求回包的过程中，keepalived-切主是否会有影响？" class="headerlink" title="4.5. 请求回包的过程中，keepalived 切主是否会有影响？"></a><strong>4.5. 请求回包的过程中，keepalived 切主是否会有影响？</strong></h2><p>  当前环境 keepalived 的 ipvs 工作模式是 nat 模式，进出流量都会经过 keepalived 配置的 ipvs。在进行转发作业的时候 lvs 会维护一张连接表。如果说中途出现 keepalaived 切主的情况。新的主 keepalived 对这些连接信息存在丢失。此时请求&#x2F;响应链路应该会中断一会。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ipvs 配置</span></span><br><span class="line">virtual_server 192.168.25.90 60080 &#123;</span><br><span class="line">    delay_loop 10</span><br><span class="line">    lb_algo rr</span><br><span class="line">    lb_kind NAT  <span class="comment"># 使用了 NAT 模式</span></span><br><span class="line">    protocol TCP</span><br><span class="line">    real_server 192.168.25.127 60080 &#123;</span><br><span class="line">        weight 1</span><br><span class="line">        HTTP_GET &#123;</span><br><span class="line">            url &#123;</span><br><span class="line">                path /</span><br><span class="line">            &#125;</span><br><span class="line">            connect_timeout 10</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>  实际上我在排查过程中，是遇到了两种场景，一种是请求没有到 gateway（如上文 2~3 的过程，另一种情况是请求已经到了 gateway 并且 gateway 也转给了业务 Pod，业务 Pod 也有正常返回。但最终结果也是 502。也就是说切主确实会对回程流量造成影响。</p><h2 id="4-6-WAF-是怎么工作的？"><a href="#4-6-WAF-是怎么工作的？" class="headerlink" title="4.6. WAF 是怎么工作的？"></a><strong>4.6. WAF 是怎么工作的？</strong></h2><p>  具体可参考腾讯云官网的示例 <a href="https://cloud.tencent.com/document/product/627/39843">Web 应用防火墙 产品分类-产品简介-文档中心-腾讯云</a>，当前演示环境配置的是 SaaS 型的，总体逻辑如下</p><p><img src="C:\Users\89715\AppData\Roaming\marktext\images\2023-08-26-19-36-48-1693049798627.jpg"></p><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h1><h2 id="5-1-思路"><a href="#5-1-思路" class="headerlink" title="5.1. 思路"></a><strong>5.1.</strong> <strong>思路</strong></h2><p>  后续如果遇到类似问题，可优先尝试以下思路去定位：</p><ul><li><p>可以先通过响应头部是否有携带 <strong>X-ABC-Upstream-Status</strong> 来定界下范围，如果有携带那应该是后端服务有异常，优先查看后端服务。如果没有携带可能是请求没抵达，也可能是中途因为其他原因连接丢失。</p></li><li><p>如果组件找不到请求的日志，或者有收到请求但是有正常响应，页面接口报 502，大概率是中间某个节点触发了主从切换。可以往带主从切换的组件优先进行排查。</p></li><li><p>梳理网络拓扑，确认入&#x2F;出流量的路径。再基于每个流量节点进行排查。类似场景可参考 5.2、5.3 的方法进行确认。</p></li></ul><h2 id="5-2-演示环境网络路径"><a href="#5-2-演示环境网络路径" class="headerlink" title="5.2. 演示环境网络路径"></a><strong>5.2.</strong> <strong>演示环境网络路径</strong></h2><p>  <strong>拓扑</strong></p><p>  <strong>每个节点角色功能如下</strong></p><ul><li><p>浏览器 – 请求发起者</p></li><li><p>DNS – 提供站点域名解析，将请求引入 WAF</p></li><li><p>WAF – 提供 WEB 应用防火墙功能，负责拦截 Web 攻击，过滤流量后将请求转发给后端服务</p></li><li><p>EIP 公网 IP – 提供公网访问功能</p></li><li><p>HA VIP – 虚拟 IP，提供网络层高可用功能结合 TKE 自建 VIP 的 keepalived 使用。实现平台网络层高可用。</p></li><li><p>alb –  ingress-controller，基于 URL 将请求分发到对应的后端业务 Pod。</p></li><li><p>内部 gateway – 自研的网关组件，负责将业务接口的调用分发到指定的业务 Pod。</p></li><li><p>业务 Pod – 提供业务处理逻辑的进程。</p></li></ul><h2 id="5-3-各环节排查思路-处理方法"><a href="#5-3-各环节排查思路-处理方法" class="headerlink" title="5.3. 各环节排查思路&#x2F;处理方法"></a><strong>5.3.</strong> <strong>各环节排查思路&#x2F;处理方法</strong></h2><p>  每个环节的排查&#x2F;处理思路</p><table><thead><tr><th><strong>排查节点</strong></th><th><strong>排查以及处理思路</strong></th></tr></thead><tbody><tr><td>客户端浏览器&#x2F;Postman 等程序</td><td>1. 如果是接口偶现 502，可能是类似的问题，需要先确认下 Response Headers 有没有带 <strong>X-ABC-Upstream-Status</strong>。2. 也有可能是客户端自身问题，比如配置或者网络环境质量不好。如果有多个用户反馈类似的情况。这种场景应该是平台侧的问题。需要从平台侧找原因。</td></tr><tr><td>DNS</td><td>1. 通常情况下不会是它的原因。域名访问不通，平台是无法访问的。2. 如果出现了异常，可以确认近期是否有进行过配置变更 3. 联系腾讯云小助手支持</td></tr><tr><td>WAF</td><td>1. 同 DNS 的处理思路，如果需要访问日志等信息可联系腾讯云小助手支持。联系支持时需要提供接口调用的具体时间、请求的 URL。小助手可以提供对应的访问日志。</td></tr><tr><td>EIP</td><td>1. 同 DNS 的处理思路。通常情况下应该不是它的原因</td></tr><tr><td>HA VIP</td><td>1. 查看日志，相关接口报错的时间点内是否发生过切主。可通过 kubectl logs -n kube-system keepalived-x.x.x.x 获取日志。2. 日志中是否有产生重复 vrid 的提示，如果有则需要修复。此时可以通过 tcpdump vrrp 的方法快速确认冲突源。</td></tr><tr><td>alb（ingress-controller）</td><td>1. 确认 alb 状态是否正常，近期状态是否有异常退出、重启等情况2. 确认 alb 的请求日志和错误日志。进入 alb 容器访问请求日志路径</td></tr><tr><td>内部 gateway</td><td>1. 组件找不到请求的日志，应该是上面其他环节存在异常。2. 组件可以找到请求日志，从后端服务也能找到正常处理的日志，此时可能是上面有个环节存在异常。比如 keepalived 切主等。建议在上面其他环节进行排查。</td></tr><tr><td>业务 Pod</td><td>1. 确认业务 Pod 状态是否为 Running 并就绪。2. 确认业务 Pod 是否有产生过重启记录（重启计数器不为 0），重启的时间节点以及退出码。</td></tr><tr><td>主机</td><td>1. 通过 grafana 查看监控，集群各主机状态是否正常，负载是否正常。</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;1. 概述&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;  本文的内容主要是对 TI361演示环境偶发 502 该问题的处理进行一次记录。其中</summary>
      
    
    
    
    
    <category term="排障" scheme="https://guoltan.github.io/tags/%E6%8E%92%E9%9A%9C/"/>
    
  </entry>
  
  <entry>
    <title>边缘计算杂记一：概念</title>
    <link href="https://guoltan.github.io/2024/01/31/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%9D%82%E8%AE%B0%E4%B8%80%EF%BC%9A%E6%A6%82%E5%BF%B5/"/>
    <id>https://guoltan.github.io/2024/01/31/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%9D%82%E8%AE%B0%E4%B8%80%EF%BC%9A%E6%A6%82%E5%BF%B5/</id>
    <published>2024-01-31T15:19:11.000Z</published>
    <updated>2024-01-31T15:20:04.449Z</updated>
    
    <content type="html"><![CDATA[<p><strong>什么是边缘计算？它具备什么价值？</strong></p><p>  边缘计算（Edge Computing）是一种分布式计算的架构，将计算和数据处理的能力推向离数据源和用户更近的边缘设备上。边缘设备可以是工控机、路由器、网关、传感器等，这些设备都有一定的计算、存储和网络连接能力。</p><p>  边缘计算的价值在于：</p><ul><li><p>减少数据传输延迟：边缘设备位于数据源和用户之间，可以避免长距离的数据传输，减少了数据传输延迟。</p></li><li><p>提供实时决策和响应：由于边缘设备的计算和数据处理能力强大，可以实时对数据进行处理和分析，对数据进行过滤、聚合、计算等，快速做出决策和响应。</p></li><li><p>提高网络安全性：由于数据处理在边缘设备上进行，有助于减少对云端和网络的依赖性，从而提高系统的安全性和可靠性。</p></li><li><p>降低云计算成本：通过使用边缘计算，可以将计算和数据处理从云端下移，减少云计算的负载和成本。</p></li><li><p>支持物联网应用：边缘计算的应用场景非常适合物联网领域，因为物联网涉及到大量的传感器和设备，需要对这些设备产生的数据进行实时监控和分析，并做出实时决策和调整。</p><p>边缘计算的典型应用包括自动驾驶、智能家居、智能制造、智慧城市等领域，在这些应用场景中，边缘计算可以快速响应用户需求，处理实时数据并进行决策。这种分布式计算架构的优势在于提高了系统的性能和效率，并能够更好地满足复杂的实时业务需求。</p></li></ul><p><strong>边缘计算的架构大概是怎么样的？</strong></p><p>  边缘计算的架构通常包括以下几个主要组件：</p><ol><li><p>边缘设备（Edge Devices）：边缘设备是指部署在离数据源和用户更近的位置上的设备，例如传感器、工控机、路由器、智能手机等。这些设备具备一定的计算、存储和网络连接能力，能够处理和分析数据，并执行一些简单的计算任务。</p></li><li><p>边缘节点（Edge Nodes）：边缘节点是一组连接在一起的边缘设备，它们协同工作，共同处理和管理数据。边缘节点可以是一个集群、网关或者边缘服务器等。边缘节点负责接收、存储、处理和转发来自边缘设备的数据，并与云端或中心节点进行通信。</p></li><li><p>云端或中心节点（Cloud or Central Nodes）：云端或中心节点是位于数据中心或云平台的中央服务器，负责接收、存储和管理从边缘节点传输过来的数据。云端或中心节点具备强大的计算和存储能力，可以执行复杂的数据分析和处理任务。</p></li><li><p>网络连接（Network Connectivity）：边缘计算依赖于可靠的网络连接，用于连接边缘设备、边缘节点和云端或中心节点之间的数据传输。可以利用有线网络（如以太网）或者无线网络（如Wi-Fi、蜂窝网络等）来实现连接。</p></li><li><p>边缘计算软件平台（Edge Computing Software Platform）：边缘计算软件平台提供了管理和协调边缘设备、边缘节点和云端节点之间通信和数据处理的功能。它包括边缘节点操作系统、容器化技术、边缘计算框架等组件，帮助实现数据的分发、处理、存储和安全管理。</p></li></ol><p>在边缘计算架构中，边缘设备收集和生成数据，通过边缘节点进行本地处理和决策，然后将关键数据传输到云端或中心节点进行进一步的分析和计算。这种分布式计算架构能够提高系统的性能和效率，减少数据传输延迟，并支持实时的决策和响应。</p><p><strong>边缘计算和 Kubernetes 存在什么关联？</strong></p><ol><li><p>边缘计算是一种分布式计算模型，它将计算、存储和网络功能推近到数据产生源头和最终用户之间，以提供更低延迟、更高带宽和更好的用户体验。边缘计算可以将计算资源放置在离用户或设备更近的位置，例如工厂、物流中心、智能城市等地方。</p></li><li><p>Kubernetes（简称K8s）是一个用于自动化容器化应用程序部署、扩展和管理的开源平台。它提供了管理容器化应用程序所需的资源调度、服务发现、负载均衡、弹性伸缩等功能。Kubernetes通过使用容器技术，如Docker，可以简化应用程序的部署和管理，并提供高可用性和弹性的运行环境。</p></li><li><p>边缘计算和Kubernetes结合在一起，可以实现在边缘设备上部署和管理容器化应用程序的能力。通过在边缘节点上运行Kubernetes集群，可以将容器化的应用程序部署到边缘设备中，实现边缘计算的灵活性和可扩展性。</p></li><li><p>在边缘计算环境中，通过Kubernetes可以实现容器化应用程序的自动部署、资源调度和服务发现。Kubernetes可以根据边缘设备的资源状况进行自动伸缩，确保应用程序在边缘节点上的高可用性和性能。</p></li></ol><p><strong>边缘计算和 CDN 的关系？</strong></p><p>  个人理解边缘计算和 CDN 都属于解决方案，它们之间非常相似。下面是我找到的一个 CDN 与边缘计算联合使用的案例：</p><p>  假设有一家电子商务公司，他们的在线平台需要快速响应用户的请求并提供高质量的服务。用户遍布全球各地，访问平台的内容包括网页、图片、视频等。这家电子商务公司决定采用CDN来优化内容传输和加速用户访问体验。他们与CDN服务提供商合作，在全球范围内建立了许多节点服务器，这些节点服务器位于离用户最近的位置。</p><p>  当用户请求访问该电子商务平台时，CDN会根据用户的位置选择距离最近的节点服务器来提供所需内容。CDN节点服务器具有缓存功能，如果所请求的内容已经存在于某个节点服务器上，CDN可以直接从该节点服务器中返回内容，而无需通过远程的主要服务器。这样可以大大减少数据传输的延迟，加快内容的加载速度。</p><p>  此外，这些CDN节点服务器也可以提供边缘计算的能力。例如，当用户请求访问一个动态生成的页面时，CDN节点服务器可以在接收到请求后执行一些简单的计算任务，如数据处理、模板渲染等，以生成所需的页面，并将结果返回给用户。这样，不仅减少了从主要服务器到用户的数据传输量，还可以提供更快速的响应时间。</p><p>  CDN作为边缘计算节点发挥了双重作用。它们不仅通过缓存和分发内容来加速内容传输，提高用户访问体验，而且通过执行一些简单的边缘计算任务，减少了数据传输延迟，并提供更快速的响应。</p><p>边缘计算平台，大概需要解决哪些问题？</p><p>  如果应用需要在边缘计算的场景下使用，对提供边缘计算能力的平台需要解决下面这几个问题：</p><ul><li><p>云边通信，边端通常没有固定IP，云端怎么去管控它们？云端又怎么去访问边端上的服务？</p></li><li><p>边缘自治，边端和云端连通性出现异常时，边端怎么保障还能正常的对外提供服务？</p></li><li><p>边缘部署，边缘节点间可能存在差异（如ARM、x86 或者服务配置有一些不同），怎么快速把应用发布在边缘节点。</p></li></ul><p>那这里可以通过什么技术或者手段去实现这些能力？</p><p><strong>云边通信</strong></p><p>  云边通信需要解决的问题是边端没有固定IP，这使得传统的 ssh 主动去连接边缘的机器是无法做到的。边端虽然没办法被云端主动去连接，但可以反过来，让边端主动去连接云端，这时候就可以实现云边通信了。</p><p>  所以这里可以设计一个程序，实现这个反向隧道。来解决掉云端和边端之间的通信问题。如 superedge 就是通过 tunnel-cloud、tunnel-edge 组件来实现的反向隧道。让云端可以实现边端管理。</p><p><strong>边缘自治</strong></p><p>  边缘自治边缘自治，笔者的理解是边缘节点能够自主的去进行决策和协作。实际上这里的实现需要和边缘上的应用实现相关联。我这里只讨论边缘计算的平台（基于 Kubernetes）要怎么实现基础的边缘自治能力。</p><p>  如果只考虑到 Kubernetes 这一层次，假设云端和边端节点之间的网络不稳定，出现了几分钟断连的情况，这时候基于 Kubernetes 自身的机制，节点需要每隔一段时间进行上报的动作。当上报失败次数到某个次数，Kubernetes 会认为节点不健康，将状态设置为 NotReady，并驱逐节点上的所有服务。在其他节点上拉起新的工作副本。</p><p>  对于边缘计算的场景下，这个设计其实是不可靠的，毕竟边端节点可能没有异常，它只是和云端存在网络通信问题。边端节点自身还是能够给边缘设备提供访问能力的。所以这时候边端节点不能只依赖 Kubernetes 原生的健康检查机制。还需要依赖一些分布式的检查方式。比如某个站点下的边缘节点异常了。那该站点下的其他节点可以和它进行连通性测试。确保该节点只是和云端通信存在异常。本身能力还是可以继续对外提供的。不进行驱逐。</p><p>  另外当云端无法访问了，边端节点如果依赖云端的一些个性化数据无法获取到，也可能会造成业务影响，所以边端节点自身也需要去实现一层类似缓存、同步的机制。比如在本地拉起一个简易的数据库程序。将平常访问到云端的一些数据、在脱离云端通信过程中执行的一些决策数据等缓存到本地数据库中。等恢复后再执行同步。</p><p><strong>边缘部署</strong></p><p>  基于实际环境应用去考虑，边缘部署可能要解决这几个点：</p><ul><li><p>环境可能存在站点的概念，比如环境下存在非常多的节点。某些边缘节点属于一个站点，某些节点属于另一个站点，我一个边缘应用在同时发布在若干个站点。怎么快速去部署？</p></li><li><p>部署过程中可能存在差异化的配置，比如边缘节点的 CPU 架构可能是不一致的，比如同时存在 x86 和 arm 架构，那这个时候部署的时候怎么去快速渲染差异化的配置。</p><p>那这里就是说要解决站点间部署的差异性，以及快速将应用部署在若干站点。这里就需要 Kubernetes 这一层再去设计一些额外的 CR 资源来实现，以 superedge 为例子。它设计了 NodeUnit、NodeGroup、ServiceGroup 的概念。</p><p>我们可以将具备相同属性（例子中就是同一站点）的节点配置划分到 NodeUnit。这样不同站点间它们就归属在不同的 NodeUnit，所有的 NodeUnit 都归属于一个 NodeGroup。然后基于 NodeGroup 可以创建对应的 deploymentGrid。在 deploymentGrid 中可以配置 poolTemplate，将每个不同 NodeUnit 的差异化配置进行设置。实现差异化部署。之后再基于 NodeGroup 可以创建 ServiceGroup。去自动给这些 NodeUnit 部署的应用创建 service。来实现对外服务。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;什么是边缘计算？它具备什么价值？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;  边缘计算（Edge Computing）是一种分布式计算的架构，将计算和数据处理的能力推向离数据源和用户更近的边缘设备上。边缘设备可以是工控机、路由器、网关、传感器等，这些设备都有一定的计</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
    <category term="边缘计算" scheme="https://guoltan.github.io/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes go开发环境安装</title>
    <link href="https://guoltan.github.io/2024/01/26/go%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E7%8E%AF%E5%A2%83/"/>
    <id>https://guoltan.github.io/2024/01/26/go%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E7%8E%AF%E5%A2%83/</id>
    <published>2024-01-26T12:11:25.000Z</published>
    <updated>2024-01-26T15:12:52.709Z</updated>
    
    <content type="html"><![CDATA[<h2 id="go-安装"><a href="#go-安装" class="headerlink" title="go 安装"></a>go 安装</h2><p>wget <a href="https://dl.google.com/go/go1.21.6.linux-amd64.tar.gz">https://dl.google.com/go/go1.21.6.linux-amd64.tar.gz</a> tar xvf go1.21.6.linux-amd64.tar.gz<br>mv go &#x2F;usr&#x2F;local&#x2F;go<br>echo “PATH&#x3D;PATH:&#x2F;usr&#x2F;local&#x2F;go&#x2F;bin” &gt;&gt; &#x2F;etc&#x2F;profile<br>source &#x2F;etc&#x2F;profile</p><h2 id="使用国内代理"><a href="#使用国内代理" class="headerlink" title="使用国内代理"></a>使用国内代理</h2><p>go env -w GOPROXY&#x3D;<a href="https://goproxy.cn,direct/">https://goproxy.cn,direct</a></p><p>go env -w GO111MODULE&#x3D;auto</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;go-安装&quot;&gt;&lt;a href=&quot;#go-安装&quot; class=&quot;headerlink&quot; title=&quot;go 安装&quot;&gt;&lt;/a&gt;go 安装&lt;/h2&gt;&lt;p&gt;wget &lt;a href=&quot;https://dl.google.com/go/go1.21.6.linux-amd6</summary>
      
    
    
    
    
    <category term="go" scheme="https://guoltan.github.io/tags/go/"/>
    
  </entry>
  
  <entry>
    <title>about</title>
    <link href="https://guoltan.github.io/2024/01/25/about/"/>
    <id>https://guoltan.github.io/2024/01/25/about/</id>
    <published>2024-01-25T15:08:00.000Z</published>
    <updated>2024-01-25T15:08:00.896Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>基于资源对应 pod 批量执行 kubectl logs 采集工具</title>
    <link href="https://guoltan.github.io/2024/01/23/kubectl%20logs%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E5%B7%A5%E5%85%B7/"/>
    <id>https://guoltan.github.io/2024/01/23/kubectl%20logs%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E5%B7%A5%E5%85%B7/</id>
    <published>2024-01-23T12:11:25.000Z</published>
    <updated>2024-01-26T15:12:52.709Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>  在交付运维阶段，有时候在日志平台还没有就绪，或者日志平台有问题情况下，我们需要对 Pod 进行诊断，这时候可能需要去读取 pod 的标准输出。但是一般情况下环境的 Pod 都是多副本高可用的，服务的请求可能会随机落到任意 Pod。对此通常的解决方法是拉多个窗口去观测，这种方式不大方便。这里就通过开发一个简易日志采集脚本来简化改工作。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>  实际上我只需要获取 pod 的清单，然后挨个 pod 执行 kubectl logs 的指令，并挂在后台。然后再通过 tail 命令去获取所有 log 的输出即可。本身 tail 对多个文件的输出是自带了 title。可以简化工作量。</p><p>  获取 pod 清单的方式是让使用者输入 pod 控制器的类型（deployment、daemonset等等或者简写），命名空间，对应控制器资源的名称，每个控制器都是通过标签选择来判断 pod 是否在集群中满足数量。所以这里通过获取控制器的 matchlabels 来生成判断条件。后续通过 kubectl get pod -l 标签的方式就能直接拿到清单了。</p><p>  考虑到日志的展示本身就是需要生成临时日志文件，并事后可能需要保存，或者多次执行，脚本会自动创建一个命名空间+资源名称的目录，来存储日志文件。并加入一些删除的逻辑，强制退出脚本或者启动脚本时都会检查 kubectl logs 的进程还有没有运行在环境，如果有就删除残留的进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">print_log()</span><br><span class="line">&#123;</span><br><span class="line">    log_level=$1</span><br><span class="line">    log_msg=$2</span><br><span class="line">    currentTime=&quot;$(date &#x27;+%F %T&#x27;)&quot;</span><br><span class="line">    echo &quot;$currentTime    [$log_level]    $log_msg&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">clear() &#123;</span><br><span class="line">    if [ -f clearlist ]; then</span><br><span class="line">        while IFS= read -r line; do</span><br><span class="line">          if [ $(ps aux | grep &quot;$&#123;line&#125;&quot; | wc -l) -gt 1 ] ; then</span><br><span class="line">            ps aux | grep &quot;$&#123;line&#125;&quot; | grep -v grep | awk &#x27;&#123; print $2 &#125;&#x27; | xargs kill</span><br><span class="line">          fi</span><br><span class="line">        done &lt; clearlist</span><br><span class="line">    fi</span><br><span class="line">    rm -f clearlist</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ctrl_c() &#123;</span><br><span class="line">    clear</span><br><span class="line">    print_log &quot;INFO&quot; &quot;Is k8s_log_collector.sh: line xx:  xxxx Terminated appeared, please ingore.&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">trap ctrl_c INT</span><br><span class="line"></span><br><span class="line">baseName=$(basename &quot;$0&quot;)</span><br><span class="line"></span><br><span class="line">if [ $# -lt 3 ]; then</span><br><span class="line">    print_log &quot;INFO&quot; &quot;usage: $baseName resource ns name&quot;</span><br><span class="line">    print_log &quot;INFO&quot; &quot;Example:&quot;</span><br><span class="line">    print_log &quot;INFO&quot; &quot;  $baseName ds kube-flannel kube-flannel-ds&quot;</span><br><span class="line">    print_log &quot;INFO&quot; &quot;  $baseName deploy default nginx&quot;</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if ! kubectl get $1 -n $2 $3 &amp;&gt; /dev/null; then</span><br><span class="line">    echo &quot;resource not exits.&quot;</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">log_path=log_collect/$2/$1_$2_$3/</span><br><span class="line">if [ -d log_collect/$2 ]; then</span><br><span class="line">    clear</span><br><span class="line">    mv log_collect/$2 log_collect/$2_$(date +%Y%d%H%M%T)</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">mkdir -p $&#123;log_path&#125;</span><br><span class="line"></span><br><span class="line">labels=$(kubectl get $1 -n $2 $3 -o jsonpath=&quot;&#123;.spec.selector.matchLabels&#125;&quot;  | sed &#x27;s/&#123;//&#x27; | sed &#x27;s/&#125;//&#x27; | sed &#x27;s/&quot;//g&#x27; | sed &#x27;s/:/=/g&#x27;)</span><br><span class="line"></span><br><span class="line">podlist=$(kubectl get pod -n $2 -l $&#123;labels&#125; --no-headers | awk &#x27;&#123; print $1 &#125;&#x27;)</span><br><span class="line"></span><br><span class="line">for pod in $&#123;podlist&#125;</span><br><span class="line">do</span><br><span class="line">    echo &quot;kubectl logs -n $2 $&#123;pod&#125;&quot; &gt;&gt; clearlist</span><br><span class="line">    kubectl logs -n $2 $&#123;pod&#125; -f &amp;&gt; $&#123;log_path&#125;/$&#123;pod&#125;.log &amp;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">print_log &quot;INFO&quot; &quot;Log collection started, wait for 2 seconds.&quot;</span><br><span class="line">sleep 2</span><br><span class="line"></span><br><span class="line">tail -f $&#123;log_path&#125;/*</span><br></pre></td></tr></table></figure><p>  效果预览：</p><img src="/2024/01/23/kubectl%20logs%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E5%B7%A5%E5%85%B7/1.png" class="" title="1.png"><p>  目前初版只是简单的实现了功能，整体逻辑和功能还比较搓。后续代码在 orca-tools 中继续维护。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;  在交付运维阶段，有时候在日志平台还没有就绪，或者日志平台有问题情况下，我们需要对 Pod 进行诊断，这时候可能需要去读取 pod 的标准</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
    <category term="工具" scheme="https://guoltan.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 快速拉起单节点集群</title>
    <link href="https://guoltan.github.io/2024/01/16/kubernetes%20%E5%8D%95%E8%8A%82%E7%82%B9%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/"/>
    <id>https://guoltan.github.io/2024/01/16/kubernetes%20%E5%8D%95%E8%8A%82%E7%82%B9%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/</id>
    <published>2024-01-16T10:11:25.000Z</published>
    <updated>2024-01-24T03:06:44.677Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>  考虑到近期会做多 Kubernetes 集群相关的一些验证，需要频繁创建单机版的 K8S 集群。需要编写一个简单的自动化 K8S 安装脚本。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>  基于社区 Kubernetes 提供的安装步骤，做了一下简单的脚本自动化封装。初版内容如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用于实现单节点部署 K8S，仅用于部署调试、开发环境，验证于 CentOS 7.9 环境</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">部署后使用默认 K8S CIDR，机器主机名称修改为 K8S-Master01，并且安装 flannel。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">适用于国区</span></span><br><span class="line"></span><br><span class="line">init_repo() &#123;</span><br><span class="line">    cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key</span><br><span class="line">EOF</span><br><span class="line">    yum -y install yum-utils</span><br><span class="line">    yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">install_pkg() &#123;</span><br><span class="line">    yum -y install kubelet kubeadm containerd kubectl</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">config_ntp() &#123;</span><br><span class="line">    yum -y install chrony</span><br><span class="line">    sed -i &#x27;/server/ d&#x27; /etc/chrony.conf</span><br><span class="line">    echo &quot;server ntp1.aliyun.com iburst&quot; &gt;&gt; /etc/chrony.conf</span><br><span class="line">    systemctl enable chronyd</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">crictl_init() &#123;</span><br><span class="line">    cat &gt; /etc/crictl.yaml &lt;&lt;EOF</span><br><span class="line">runtime-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">image-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">timeout: 0</span><br><span class="line">debug: false</span><br><span class="line">pull-image-on-create: false</span><br><span class="line">EOF</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">init_containerd() &#123;</span><br><span class="line">    containerd config default &gt; /etc/containerd/config.toml</span><br><span class="line">    sed -i &#x27;s/registry.k8s.io/registry.aliyuncs.com\/google_containers/&#x27; /etc/containerd/config.toml</span><br><span class="line">    systemctl restart containerd</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">service_onboot() &#123;</span><br><span class="line">    systemctl enable containerd</span><br><span class="line">    systemctl enable kubelet</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">disable_swap() &#123;</span><br><span class="line">    swapoff -a</span><br><span class="line">    sed -i &#x27;/swap/ s/^/#/g&#x27;  /etc/fstab</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据默认网关出接口的 IP 配置 hosts</span></span><br><span class="line">init_etchosts() &#123;</span><br><span class="line">    netif=$(ip route show | grep default | awk &#x27;&#123; print $5 &#125;&#x27;)</span><br><span class="line">    ipaddr=$(ip addr show $&#123;netif&#125; | grep -w inet | awk &#x27;&#123;print $2&#125;&#x27; | awk -F&#x27;/&#x27; &#x27;&#123; print $1 &#125;&#x27;)</span><br><span class="line">    echo &#x27;$&#123;ipaddr&#125; k8s-master01&#x27; &gt;&gt; /etc/hosts</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">init_sysctl() &#123;</span><br><span class="line">  cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf</span><br><span class="line">overlay</span><br><span class="line">br_netfilter</span><br><span class="line">EOF</span><br><span class="line">    modprobe overlay</span><br><span class="line">    modprobe br_netfilter</span><br><span class="line">  cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-iptables  = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.ip_forward                 = 1</span><br><span class="line">EOF</span><br><span class="line">    sysctl --system</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k8s_install() &#123;</span><br><span class="line">    kubeadm init --pod-network-cidr=10.244.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --node-name k8s-master01</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k8s_post() &#123;</span><br><span class="line">    mkdir -p /root/.kube</span><br><span class="line">    cp -a /etc/kubernetes/admin.conf /root/.kube/config</span><br><span class="line">    kubectl taint node k8s-master01 node-role.kubernetes.io/control-plane-</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">install_flannel() &#123;</span><br><span class="line">    wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class="line">    kubectl apply -f kube-flannel.yml</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">init() &#123;</span><br><span class="line">    init_repo</span><br><span class="line">    install_pkg</span><br><span class="line">    crictl_init</span><br><span class="line">    disable_swap</span><br><span class="line">    service_onboot</span><br><span class="line">    init_containerd</span><br><span class="line">    init_sysctl</span><br><span class="line">    init_etchosts</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">install() &#123;</span><br><span class="line">    k8s_install</span><br><span class="line">    k8s_post</span><br><span class="line">    install_flannel</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main() &#123;</span><br><span class="line">    init</span><br><span class="line">    install</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main</span><br></pre></td></tr></table></figure><p>  填入文件 k8s_install.sh，直接执行脚本即可完成安装。执行 bash k8s_install.sh，就会自动创建出一个单节点的。</p><p>  目前实现的非常简单，不考虑任何变化，逻辑判断也比较宽松，后续变动继续在 orca-tools 维护。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;  考虑到近期会做多 Kubernetes 集群相关的一些验证，需要频繁创建单机版的 K8S 集群。需要编写一个简单的自动化 K8S 安装脚</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>如何查看进程属于哪个容器</title>
    <link href="https://guoltan.github.io/2023/09/11/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E8%BF%9B%E7%A8%8B%E5%B1%9E%E4%BA%8E%E5%93%AA%E4%B8%AA%E5%AE%B9%E5%99%A8/"/>
    <id>https://guoltan.github.io/2023/09/11/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E8%BF%9B%E7%A8%8B%E5%B1%9E%E4%BA%8E%E5%93%AA%E4%B8%AA%E5%AE%B9%E5%99%A8/</id>
    <published>2023-09-11T12:11:25.000Z</published>
    <updated>2024-01-29T10:15:01.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a><strong>1.</strong> 背景</h2><p> 项目上可能会遇到一种场景，运维同学反馈某几个进程存在异常（CPU、内存占用率过高），希望定位出这些进程是哪些业务去创建的此时可以根据本文的处理方法进行诊断。</p><h2 id="2-处理方法"><a href="#2-处理方法" class="headerlink" title="2. 处理方法"></a>2. 处理方法</h2><h3 id="2-1-方法一：查看各个容器环境中，是否有对应进程名称在运行"><a href="#2-1-方法一：查看各个容器环境中，是否有对应进程名称在运行" class="headerlink" title="2.1. 方法一：查看各个容器环境中，是否有对应进程名称在运行"></a>2.1. 方法一：查看各个容器环境中，是否有对应进程名称在运行</h3><p>执行如下指令获取各个容器中运行的进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for id in $(docker ps -q | xargs); do echo echo &quot;----$id----&quot; ; docker top $id | grep 进程名称 ; done</span><br></pre></td></tr></table></figure><p>将获取到有回显的结果，进一步执行 docker ps -a | grep $id 来确认容器名称</p><p> </p><p>kubernetes 的容器命名规则： <code>k8s_&#123;containerName&#125;_&#123;podFullName&#125;_&#123;namespace&#125;_&#123;podUID&#125;_&#123;podrestartCount&#125;</code></p><p>通过 docker ps 获取到的容器名称，根据 podFullName 和 namespace 的命名，我们就能判断这个容器属于哪个命名空间下。</p><p> </p><h3 id="2-2-方法二：通过进程当前的-cgroup-判断它属于哪个容器或者-Pod"><a href="#2-2-方法二：通过进程当前的-cgroup-判断它属于哪个容器或者-Pod" class="headerlink" title="2.2. 方法二：通过进程当前的 cgroup 判断它属于哪个容器或者 Pod"></a><strong>2.2.</strong> 方法二：通过进程当前的 cgroup 判断它属于哪个容器或者 Pod</h3><p>通过查看 &#x2F;proc&#x2F;PID&#x2F;cgroup 文件，通过其中的pod字段来确认进程是否属于容器环境</p><p>cat &#x2F;proc&#x2F;xxxx&#x2F;cgroup</p><h2 id="3-示例：获取某个进程的-PID，该-PID-属于运行在容器环境"><a href="#3-示例：获取某个进程的-PID，该-PID-属于运行在容器环境" class="headerlink" title="3. 示例：获取某个进程的 PID，该 PID 属于运行在容器环境"></a>3. 示例：获取某个进程的 PID，该 PID 属于运行在容器环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]# cat /proc/7644/cgroup</span><br><span class="line"></span><br><span class="line">11:cpuset:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">10:perf_event:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">9:memory:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">8:hugetlb:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">7:devices:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">6:pids:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">5:freezer:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">4:net_prio,net_cls:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">3:blkio:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">2:cpuacct,cpu:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">1:name=systemd:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br></pre></td></tr></table></figure><p>其中 podda498366ad0b5d5f487968aaa924c6ab 就是该进程所属的 pod，此时通过 docker ps -a | grep da4983 可以确认进程所属容器</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.&lt;/strong&gt; 背景&lt;/h2&gt;&lt;p&gt; 项目上可能会遇到一种场景，运维同学反馈某几个进程存在异常（CPU、内存占用率过高），</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>helm list 显示不出资源原因分析</title>
    <link href="https://guoltan.github.io/2023/08/26/helm%20list%20%E6%98%BE%E7%A4%BA%E4%B8%8D%E5%87%BA%E8%B5%84%E6%BA%90%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/"/>
    <id>https://guoltan.github.io/2023/08/26/helm%20list%20%E6%98%BE%E7%A4%BA%E4%B8%8D%E5%87%BA%E8%B5%84%E6%BA%90%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/</id>
    <published>2023-08-26T10:11:25.000Z</published>
    <updated>2024-03-25T15:03:51.069Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p> 部分项目由客户运维，有些客户对 helm 指令不熟悉，客户误操作查询不出 release 会引起一些歧义。本文基于过去的一些处理的案例，对 helm list 不出资源的可能原因进行说明，并解释 helm list -a 和 -A 的区别。</p><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><h2 id="helm-list-不出东西怎么办"><a href="#helm-list-不出东西怎么办" class="headerlink" title="helm list 不出东西怎么办"></a>helm list 不出东西怎么办</h2><p>  helm list 是用于列举特定命名空间下的 release。本质上 release 是存储在 secret 的（Helm v2 存储在 configmap）。所以 helm list 这个指令实际上可类比为 kubectl get secret -n 命名空间 -l owner&#x3D;helm。如果说 helm list 找不到东西，可能原因如下：</p><ul><li><p>helm list 没有加 -n 选项，没有在指定命名空间下查询 release</p></li><li><p>资源已被全部删除</p></li><li><p>资源没有被删除，但是用于追踪状态的 secret 被误删除。</p></li><li><p>错误的使用 helm ls -a 去获取集群当前所有的 release。</p></li></ul><p>基于原因，可检查项：</p><ul><li><p>helm ls 有没有加上 -n 命名空间是否正确</p></li><li><p>资源是否本身已被删除过了，通过 kubectl get pod -A 是否能看到相关业务 Pod。</p></li><li><p>kubectl get secret -A | grep sh.helm.release 检查集群中的保存 release 状态的 secret 是否还存在。</p></li><li><p>是否使用了 helm ls -a 选项。需要修改成 helm ls -A</p></li></ul><h2 id="helm-list-A-和-a-有什么区别？"><a href="#helm-list-A-和-a-有什么区别？" class="headerlink" title="helm list -A 和 -a 有什么区别？"></a>helm list -A 和 -a 有什么区别？</h2><p>helm list –help 中，给出 -a 、-A 的区别如下</p><p> -a, –all show all releases without any filter applied</p><p> -A, –all-namespaces list releases across all namespaces</p><p>-a 简单翻译就是显示未被筛选的所有 release 版本。对于某些中间状态，默认情况下 helm list 是不会显示的，如果要显示的话，就可以增加 -a 的选项。</p><p>-A 简单的说就是列出集群所有命名空间下的 release。如果对 release 配置在哪个命名空间不大清楚，是可以通过 -A 选项去查询的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt; 部分项目由客户运维，有些客户对 helm 指令不熟悉，客户误操作查询不出 release 会引起一些歧义。本文基于过去的一些处理的案例，对</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
    <category term="Helm" scheme="https://guoltan.github.io/tags/Helm/"/>
    
  </entry>
  
  <entry>
    <title>搭建 Kubernetes 私有化镜像仓库 registry</title>
    <link href="https://guoltan.github.io/2023/08/15/%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89%E5%8C%96%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%20registry/"/>
    <id>https://guoltan.github.io/2023/08/15/%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89%E5%8C%96%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%20registry/</id>
    <published>2023-08-15T10:11:25.000Z</published>
    <updated>2024-01-28T12:22:18.244Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h1><p>  在使用 Kubernetes 平台过程中，常常会有搭建私有镜像仓库的需求。而本文主要是记录如何在 Kubernetes 集群上，以 staticpod 的方式快速拉起一个单实例的 registry。以提供容器镜像存储使用。</p><h1 id="2-部署步骤"><a href="#2-部署步骤" class="headerlink" title="2. 部署步骤"></a>2. 部署步骤</h1><h2 id="2-1-创建-registry-需要使用的证书"><a href="#2-1-创建-registry-需要使用的证书" class="headerlink" title="2.1. 创建 registry 需要使用的证书"></a>2.1. 创建 registry 需要使用的证书</h2><p>  docker 提供的 registry 默认情况下需要配置 HTTPS 证书，所以我们在搭建前需要先获取 HTTPS 证书。而证书的获取则是通过自签名证书的方式实现。值得注意的是，新版本 go 1.15 以上对证书字段存在要求，必须使用” SAN 字段”。否则在使用过程中，可能会触发下面的错误：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">error: failed to solve: registry.sealos.hub:5000/library/nginx:latest: failed to do request:</span> <span class="string">Head</span> <span class="attr">&quot;https://registry.sealos.hub:5000/v2/library/nginx/manifests/latest&quot;:</span> <span class="attr">tls: failed to verify certificate: x509:</span> <span class="string">certificate</span> <span class="string">relies</span> <span class="string">on</span> <span class="string">legacy</span> <span class="string">Common</span> <span class="string">Name</span> <span class="string">field,</span> <span class="string">use</span> <span class="string">SANs</span> <span class="string">instead</span></span><br></pre></td></tr></table></figure><p>以下是自签名证书实现的过程。</p><h3 id="2-1-1-创建证书信息文件"><a href="#2-1-1-创建证书信息文件" class="headerlink" title="2.1.1. 创建证书信息文件"></a>2.1.1. 创建证书信息文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 创建证书信息文件 openssl.cnf</span><br><span class="line"># 其中 alt_names 的 IP、DNS 字段可以根据实际需要去增加和修改。</span><br><span class="line"></span><br><span class="line">[req]</span><br><span class="line">req_extensions = v3_req</span><br><span class="line">distinguished_name = req_distinguished_name</span><br><span class="line"></span><br><span class="line">[req_distinguished_name]</span><br><span class="line">countryName = Country Name (2 letter code)</span><br><span class="line">countryName_default = XX</span><br><span class="line">stateOrProvinceName = State or Province Name (full name)</span><br><span class="line">stateOrProvinceName_default = YourState</span><br><span class="line">localityName = Locality Name (eg, city)</span><br><span class="line">localityName_default = YourCity</span><br><span class="line">organizationName = Organization Name (eg, company)</span><br><span class="line">organizationName_default = YourCompany</span><br><span class="line">commonName = Common Name (e.g. server FQDN or YOUR name)</span><br><span class="line">commonName_max = 64</span><br><span class="line"></span><br><span class="line">[v3_req]</span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line"></span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1 = sealos.hub</span><br><span class="line">DNS.2 = registry.sealos.hub</span><br><span class="line">IP.1 = 192.168.214.101</span><br><span class="line">IP.2 = 192.168.214.1</span><br></pre></td></tr></table></figure><h3 id="2-1-2-创建自签名证书"><a href="#2-1-2-创建自签名证书" class="headerlink" title="2.1.2. 创建自签名证书"></a>2.1.2. 创建自签名证书</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成私钥文件</span></span><br><span class="line">openssl genpkey -algorithm RSA -out registry.key</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成证书签名（CSR）文件，一直回车</span></span><br><span class="line">openssl req -new -key registry.key -out registry.csr -config openssl.cnf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用私钥和配置文件生成自签名证书</span></span><br><span class="line">openssl x509 -req -signkey registry.key -<span class="keyword">in</span> registry.csr -out registry.crt -extensions v3_req -extfile openssl.cnf -days 36500</span><br></pre></td></tr></table></figure><h2 id="2-2-创建-registry-的-staticPod"><a href="#2-2-创建-registry-的-staticPod" class="headerlink" title="2.2. 创建 registry 的 staticPod"></a>2.2. 创建 registry 的 staticPod</h2><p>  新建 registry.yaml 文件，存放在 &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F; 目录下，内容如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">registry</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">registry</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">registry:2</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">private-repository-k8s</span></span><br><span class="line">      <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REGISTRY_HTTP_TLS_CERTIFICATE</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;/certs/registry.crt&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REGISTRY_HTTP_TLS_KEY</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;/certs/registry.key&quot;</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5000</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">certs-vol</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/certs</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registry-vol</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/registry</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">certs-vol</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/opt/certs</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registry-vol</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/opt/registry</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line"><span class="attr">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>  此处的 volumes 字段，certs-vol、registry-vol 的 path 路径可以根据实际存储位置进行调整。</p><p>  执行完成以后，通过 kubectl get pod -n kube-system -o wide | grep registry，就可以看到创建出的 registry pod。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master01certs]<span class="comment"># kubectl get pod -n kube-system -o wide | grep registry</span></span><br><span class="line">registry-k8s-master01                      1/1     Running   2          4h36m   192.168.214.101   k8s-master01   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="2-3-如何解决客户端不信任-registry-提供自签名证书"><a href="#2-3-如何解决客户端不信任-registry-提供自签名证书" class="headerlink" title="2.3. 如何解决客户端不信任 registry 提供自签名证书"></a>2.3. 如何解决客户端不信任 registry 提供自签名证书</h2><p>  经过上述步骤以后，此时可以通过 nerdctl push 测试镜像推送，如果是自签名证书，此时可能会触发如下报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tls: failed to verify certificate: x509: certificate signed by unknown authority</span><br></pre></td></tr></table></figure><p>  这是因为客户端没有信任 registry 提供的 HTTPS 证书导致的，对于这种问题，可解决的方法：</p><ul><li><p>关闭客户端的 HTTPS 校验功能，忽略不受信任。– 对于测试、学习环境，可以通过该方式快速解决。</p></li><li><p>通过配置客户端，将自签名证书添加到受信任的证书列表。 – 虽然操作起来比较复杂，但这种方式相对安全的多。</p><p>需要提醒的是笔者使用的是 containerd 而非 docker，所以操作方式以 containerd 为例。</p></li></ul><h3 id="2-3-1-关闭-HTTPS-校验"><a href="#2-3-1-关闭-HTTPS-校验" class="headerlink" title="2.3.1. 关闭 HTTPS 校验"></a>2.3.1. 关闭 HTTPS 校验</h3><p>  参考如下方法来完成 HTTPS 校验关闭的动作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑 /etc/containerd/config.toml 追加如下配置</span></span><br><span class="line">    [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry]</span><br><span class="line">      [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.configs.<span class="string">&quot;registry.sealos.hub:5000&quot;</span>.tls]</span><br><span class="line">        insecure_skip_verify = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境可能默认没有 /etc/containerd/config.toml 文件，可以通过 containerd config dump &gt; /etc/containerd/config.toml 去生成</span></span><br></pre></td></tr></table></figure><h3 id="2-3-2-为-containerd-添加-HTTPS-证书"><a href="#2-3-2-为-containerd-添加-HTTPS-证书" class="headerlink" title="2.3.2. 为 containerd 添加 HTTPS 证书"></a>2.3.2. 为 containerd 添加 HTTPS 证书</h3><p>   我们可以将 HTTPS 证书放置在 &#x2F;etc&#x2F;containerd&#x2F;certs.d 目录下，需要在所有 node 上执行这个动作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果没有 /etc/containerd/certs.d 目录，则需要创建</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /etc/containerd/certs.d</span><br><span class="line"><span class="comment"># 将 2.1.2. 生成的上传证书文件到 /etc/containerd/certs.d/ 目录下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果环境配置了 buildkitd，此时也需要追加证书</span></span><br><span class="line">修改 buildkitd 配置文件的内容 /etc/buildkit/buildkitd.toml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加如下内容，将对应镜像仓库信息、证书路径写入</span></span><br><span class="line">[registry.<span class="string">&quot;registry.sealos.hub:5000&quot;</span>]</span><br><span class="line">  http = <span class="literal">false</span></span><br><span class="line">  insecure = <span class="literal">false</span></span><br><span class="line">  ca=[<span class="string">&quot;/opt/certs/registry.crt&quot;</span>]</span><br><span class="line">  [[registry.<span class="string">&quot;registry.sealos.hub:5000&quot;</span>.keypair]]</span><br><span class="line">    key=<span class="string">&quot;/opt/certs/registry.key&quot;</span></span><br><span class="line">    cert=<span class="string">&quot;/opt/certs/registry.crt&quot;</span></span><br><span class="line">[worker.containerd]</span><br><span class="line">  namespace = <span class="string">&quot;k8s.io&quot;</span></span><br></pre></td></tr></table></figure><h2 id="2-4-验证上传下载功能"><a href="#2-4-验证上传下载功能" class="headerlink" title="2.4. 验证上传下载功能"></a>2.4. 验证上传下载功能</h2><p>  经过上述的步骤以后，就可以在客户端侧进行镜像的推拉测试，测试无误后。至此镜像仓库搭建已完成。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从公网拉取镜像</span></span><br><span class="line"><span class="comment"># 修改 tag</span></span><br><span class="line">nerdctl --namespace k8s.io tag daocloud.io/nginx:latest registry.sealos.hub:5000/library/nginx:latest</span><br><span class="line"><span class="comment"># 推送</span></span><br><span class="line">nerdctl --namespace k8s.io push registry.sealos.hub:5000/library/nginx:latest</span><br><span class="line"><span class="comment"># 拉取镜像</span></span><br><span class="line">nerdctl --namespace k8s.io registry.sealos.hub:5000/library/nginx:latest</span><br></pre></td></tr></table></figure><p>参考链接：</p><p><a href="https://github.com/moby/buildkit/blob/master/docs/buildkitd.toml.md">https://github.com/moby/buildkit/blob/master/docs/buildkitd.toml.md</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h1&gt;&lt;p&gt;  在使用 Kubernetes 平台过程中，常常会有搭建私有镜像仓库的需求。而本文主要是记录如何在 Kubernete</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>ServiceCatalog 概念</title>
    <link href="https://guoltan.github.io/2023/02/10/ServiceCatalog%E6%A6%82%E5%BF%B5/"/>
    <id>https://guoltan.github.io/2023/02/10/ServiceCatalog%E6%A6%82%E5%BF%B5/</id>
    <published>2023-02-10T12:11:25.000Z</published>
    <updated>2024-03-25T15:35:14.414Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>  服务目录（Service Catalog）可看作是一堆服务的集合，这些服务是由管理员对若干资源打包集成后结果。用户可以查阅这个服务目录获取到环境可被申请、使用的服务。通过调用申请服务按需获取自己需要的能力。其中用户不需要关心具体底层的实现以及其实现的过程。<br>  可参考的例子如公有云，管理员将底层资源池的资源进行抽象，将其整合为一个一个的服务，并在用户端体现一个可用的服务目录（列表），比如常见的云服务器、云硬盘、虚拟私有云等资源。用户访问公有云提供的服务目录，按需申请自己需要的资源。而用户不需要关心购买的服务对应的服务器是如何接入供电，如何上架到机房，如何部署上软件，只需要关注最终提供的可用的实例。<br>  那 Kubernetes 服务目录又是怎么样的东西？是为了解决什么场景的问题？实际上和上面公有云的例子进行关联，不难发现 Kubernetes 服务目录也是用于实现自助服务能力。可以先假设一个场景，用户业务运行在 Kubernetes 集群上，它们需要在 Kubernetes 集群使用数据库，需要集群管理员进行提供。这时候管理员可以怎么做？</p><ul><li><p>接收到用户的工单，然后管理员部署一套数据库集群，然后将数据库里面的一些鉴权、服务地址等信息提供给用户。用户将信息写入到 Secret 声明，然后在对应的 Pod 里面引用。</p></li><li><p>管理员提前抽象出数据库资源，实现按需部署，用户申请数据库资源，管理员审批后环境自动部署数据库实例。并将数据库的信息提供给用户。用户将信息写入到 Secret 声明，然后在对应的 Pod 里面引用。</p><p>从上面的实现方式来看，如果管理员实现了自助服务会简化一定程度上的工作。减少运维人员的工作量。我们可以基于Kubernetes ServiceCatalog 的功能去构建这个能力，将公有云上的服务、或者基于 Kubernetes 集群部署的服务，又或者是机房某个物理机上部署的服务，通过 Service Catalog 以服务代理的形式，将资源封装成服务用来提供给用户自助服务。</p></li></ul><p><code>笔者在编写到一半的时候，发现此项目已经不属于孵化项目，变成退休项目了。相关代码不再维护，镜像、chart 也无法被获取。导致没有办法继续编写下面的内容。。。不过 ServiceCatalog 这种提供自主服务集成的能力，对于某些场景下实际上还是很有用的。待后续了解其他替代方案再进行沉淀记录。</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;p&gt;  服务目录（Service Catalog）可看作是一堆服务的集合，这些服务是由管理员对若干资源打包集成后结果。用户可以查阅这个服务目录获</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>Redis响应慢导致接口超时问题分析</title>
    <link href="https://guoltan.github.io/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/"/>
    <id>https://guoltan.github.io/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</id>
    <published>2022-11-10T09:11:25.000Z</published>
    <updated>2024-03-14T03:20:00.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h1><p>  业务分析视频时，发现人物标签丢失图片。经研发定位发现业务的 CreateDownloadURL 接口会出现超时的问题，而该接口主要是使用了 Redis 服务，怀疑是 Redis 服务存在故障导致业务异常。</p><img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/1.jpg" class="" title="1.jpg"><h1 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h1><p>  通过查看平台 Redis 监控，发现 Redis 某个实例存在大量的慢查询记录（1K）。怀疑该 Redis 实例存在问题。</p><img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/2.jpg" class="" title="2.jpg"><p>  登录到该 Redis 实例，通过 slowlog get 指令，获取当前的慢查询记录，通过查询发现基本所有的慢查询记录都是查询 某个 key 触发的。并且查询的方式是 keys 指令（复杂度较高，指令执行时间较长）。</p><p>  通过 dbsize 查询，发现当前 Redis 集群的键数量已经达到 96W。在这个数量下执行 keys 类的指令（复杂度高），会拖慢 Redis 的处理效率（ Redis 单线程处理，其他指令会阻塞）。导致 Redis 性能下降，这样调用 Redis 的服务性能就会下降。出现异常。</p><img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/3.jpg" class="" title="3.jpg"><p>  此时需要理清楚为什么集群中存在大量的 Key，以及某个 Key 存在大量查询的原因。</p><p>  某个 key 产生大量查询的原因。经咨询是研发的代码逻辑缺陷，查询方式使用了 keys 并且查询间隔非常短（毫秒级的轮询）而环境存在大量的 key，使用这种方式就会一直产生慢查询记录，影响 Redis 性能。该问题需要研发修复。</p><p>  存在大量 Key 的原因，需要先查询有哪些 key，研发侧怀疑是某个服务存量的 key 数量会比较多。因为当前该 key 为了满足下载有效期的时间，配置了 30 天的有效期。而近期环境一直在执行压测作业。集群可能会积压该服务的 key。</p><p>  通过查询，该服务的 key 的数量达到了 95W。也就是集群中基本上所有 key 都是由该服务产生的。这也符合了我们的推测，由于近期压测次数较多产生了大量的 key，key 的 TTL 过长导致集群积压大量 key，redis 性能下降导致服务超时。对于这个服务的处理，当前的策略是先调整 TTL 为 1 天，避免集群产生太多 Key。</p><img src="/2022/11/10/Redis%E5%93%8D%E5%BA%94%E6%85%A2%E5%AF%BC%E8%87%B4%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/4.jpg" class="" title="4.jpg"><p>  当前需要核实 key 数量对 Redis 实例性能的影响，需要先删除掉该服务相关的 key。执行如下命令清理：</p><p>清理掉所有 abc:token: 开头的 key</p><p>redis-cli -a 密码 keys “abc:token:*” | xargs redis-cli -a 密码 del</p><p>  清理完成以后，经研发反馈，性能恢复正常，接口响应变得非常快。</p><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><h2 id="同等配置下，如果用-Keys-方式查询，集群-Key-达到多少会开始产生慢查询记录？"><a href="#同等配置下，如果用-Keys-方式查询，集群-Key-达到多少会开始产生慢查询记录？" class="headerlink" title="同等配置下，如果用 Keys 方式查询，集群 Key 达到多少会开始产生慢查询记录？"></a>同等配置下，如果用 Keys 方式查询，集群 Key 达到多少会开始产生慢查询记录？</h2><p>  通过 redis-benchmark 写入 key，大约 8W 左右就会开始产生慢查询记录，能说明 keys 这种方式非常不适用于生产环境。应该要严格限制业务侧去使用 keys 等方式去查询 redis 上面的数据。</p><h2 id="Redis-后续可改进点？"><a href="#Redis-后续可改进点？" class="headerlink" title="Redis 后续可改进点？"></a>Redis 后续可改进点？</h2><p>  需要把监控报警能力覆盖进去，通过告警通知就可以快速获知这个现象，减少一些人工判断的动作。</p><p>  需要在 CI 的代码检测阶段引入相关的 keys 检测，及时提醒用户修正使用方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;&lt;strong&gt;背景&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;  业务分析视频时，发现人物标签丢失图片。经研发定位发现业务的 CreateDownloadURL 接口会</summary>
      
    
    
    
    
    <category term="排障" scheme="https://guoltan.github.io/tags/%E6%8E%92%E9%9A%9C/"/>
    
    <category term="Redis" scheme="https://guoltan.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>如何查看某进程建立的连接信息</title>
    <link href="https://guoltan.github.io/2022/09/10/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%9F%90%E8%BF%9B%E7%A8%8B%E5%BB%BA%E7%AB%8B%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/"/>
    <id>https://guoltan.github.io/2022/09/10/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%9F%90%E8%BF%9B%E7%A8%8B%E5%BB%BA%E7%AB%8B%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/</id>
    <published>2022-09-10T12:11:25.000Z</published>
    <updated>2024-01-27T15:05:54.159Z</updated>
    
    <content type="html"><![CDATA[<p>  有些场景下，我们需要了解某个进程正在和哪些服务建立连接。本文主要是记录了捕获进程连接信息的几个实现方法。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="ss-指令"><a href="#ss-指令" class="headerlink" title="ss 指令"></a>ss 指令</h3><p>最先想到的方法是通过 netstat 或者 ss 的工具去持续查看机器的连接建立情况，并查看 PID 或者进程名称，参见如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取 sshd 进程的连接情况</span></span><br><span class="line">[root@10 ~]# ss -t  -p | grep sshd</span><br><span class="line">ESTAB      0      0      10.0.2.15:ssh                  10.0.2.15:42700                 users:((&quot;sshd&quot;,pid=2615,fd=3))</span><br><span class="line">ESTAB      0      0      192.168.214.254:ssh                  192.168.214.1:56070                 users:((&quot;sshd&quot;,pid=2863,fd=3))</span><br><span class="line">ESTAB      0      36     192.168.214.254:ssh                  192.168.214.1:56068                 users:((&quot;sshd&quot;,pid=2859,fd=3))考虑</span><br></pre></td></tr></table></figure><p>考虑到进程的连接信息是会持续变化的，可通过 while 的方法循环去查看某个进程的连接情况，实现类似 top 的效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">while true ; do clear ; ss -t  -p | grep sshd ; sleep 1; done</span><br></pre></td></tr></table></figure><h3 id="audit-日志"><a href="#audit-日志" class="headerlink" title="audit 日志"></a>audit 日志</h3><p>  ss 指令的局限在于它没办法回溯历史，你只能查看到当前正在建立的连接信息。但是某些场景下，我们可能需要知道某个进程曾经和哪些机器建立过。这时候就可以借助 audit 日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@10 ~]# tail /var/log/audit/audit.log</span><br><span class="line">type=CRYPTO_KEY_USER msg=audit(1703823116.926:813): pid=2872 uid=0 auid=0 ses=89 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg=&#x27;op=destroy kind=server fp=SHA256:83:79:c8:32:03:bd:25:7b:54:54:f7:b9:99:2b:3d:c4:0d:d9:55:ed:6c:11:a2:8f:e9:b8:ba:0f:4c:18:8c:6f direction=? spid=2872 suid=0  exe=&quot;/usr/sbin/sshd&quot; hostname=10.0.2.15 addr=? terminal=pts/1 res=success&#x27;</span><br></pre></td></tr></table></figure><p>  可以看到 audit 的审计是可以记录进程的相关信息，此处记录了进程和谁互联。当然 audit 使用的前提是系统打开了该功能。</p><h3 id="perf-工具"><a href="#perf-工具" class="headerlink" title="perf 工具"></a>perf 工具</h3><p>  perf 工具可以用来追踪特定进程建立连接的信息，参考如下方法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">捕获信息</span></span><br><span class="line">perf record -e &#x27;syscalls:sys_enter_connect&#x27; -p &lt;进程名称&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看捕获结果</span></span><br><span class="line">perf report</span><br></pre></td></tr></table></figure><h3 id="lsof-工具"><a href="#lsof-工具" class="headerlink" title="lsof 工具"></a>lsof 工具</h3><p>  lsof 工具可以列出指定进程正在和谁建立连接，和 ss 类似</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2615 是 PID，根据实际情况调整</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">也可以用 -c 参数替代，-c 参数为追踪的 <span class="built_in">command</span></span></span><br><span class="line">[root@10 ~]# lsof -i -a -p 2615</span><br><span class="line">COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</span><br><span class="line">sshd    2615 root    3u  IPv4  32428      0t0  TCP 10.0.2.15:ssh-&gt;10.0.2.15:42700 (ESTABLISHED)</span><br></pre></td></tr></table></figure><p>  同样可以通过 while 循环实现持续查看状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">while true ; do clear ; lsof -i -a -p 2615 ; sleep 1; done</span><br></pre></td></tr></table></figure><h3 id="strace-工具"><a href="#strace-工具" class="headerlink" title="strace 工具"></a>strace 工具</h3><p>  strace 工具是非常强大的链路追踪工具，可以通过 strace 对运行中的进程或者某个命令的执行过程进行追踪。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 trace=network 可以列出网络相关的追踪信息</span></span><br><span class="line">[root@10 ~]# strace -e trace=network curl www.baidu.com</span><br><span class="line">socket(AF_INET6, SOCK_DGRAM, IPPROTO_IP) = 3</span><br><span class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0</span><br><span class="line">setsockopt(3, SOL_TCP, TCP_KEEPIDLE, [60], 4) = 0</span><br><span class="line">setsockopt(3, SOL_TCP, TCP_KEEPINTVL, [60], 4) = 0</span><br><span class="line">connect(3, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(&quot;120.232.145.144&quot;)&#125;, 16) = -1 EINPROGRESS (Operation now in progress)</span><br><span class="line">getsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0</span><br><span class="line">getpeername(3, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(&quot;120.232.145.144&quot;)&#125;, [128-&gt;16]) = 0</span><br><span class="line">getsockname(3, &#123;sa_family=AF_INET, sin_port=htons(43260), sin_addr=inet_addr(&quot;10.0.2.15&quot;)&#125;, [128-&gt;16]) = 0</span><br><span class="line">sendto(3, &quot;GET / HTTP/1.1\r\nUser-Agent: curl&quot;..., 77, MSG_NOSIGNAL, NULL, 0) = 77</span><br><span class="line">recvfrom(3, &quot;HTTP/1.1 200 OK\r\nAccept-Ranges: &quot;..., 16384, 0, NULL, NULL) = 2781</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class=&quot;bg s_ipt_wr&quot;&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus&gt;&lt;/span&gt;&lt;span class=&quot;bg s_btn_wr&quot;&gt;&lt;input type=submit id=su value=百度一下 class=&quot;bg s_btn&quot;&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;新闻&lt;/a&gt; &lt;a href=http://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;地图&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;视频&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;贴吧&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;登录&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write(&#x27;&lt;a href=&quot;http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=&#x27;+ encodeURIComponent(window.location.href+ (window.location.search === &quot;&quot; ? &quot;?&quot; : &quot;&amp;&quot;)+ &quot;bdorz_come=1&quot;)+ &#x27;&quot; name=&quot;tj_login&quot; class=&quot;lb&quot;&gt;登录&lt;/a&gt;&#x27;);&lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style=&quot;display: block;&quot;&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;关于百度&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;©2017 Baidu &lt;a href=http://www.baidu.com/duty/&gt;使用百度前必读&lt;/a&gt;  &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt; 京ICP证030173号  &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;</span><br><span class="line">+++ exited with 0 +++</span><br></pre></td></tr></table></figure><p>  另一个示例，使用 strace 追踪已运行的进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# strace -p 903 -e trace=network</span><br><span class="line">strace: Process 903 attached</span><br><span class="line">--- SIGCHLD &#123;si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=18908, si_uid=0, si_status=255, si_utime=0, si_stime=0&#125; ---</span><br><span class="line">accept(3, &#123;sa_family=AF_INET, sin_port=htons(34004), sin_addr=inet_addr(&quot;192.168.214.254&quot;)&#125;, [128-&gt;16]) = 5</span><br><span class="line">socketpair(AF_UNIX, SOCK_STREAM, 0, [8, 9]) = 0</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>  实际上可使用的方法不止上文提及的几种手段，也可以通过 ebpf 等方式来实现进程连接信息的捕获。本文仅列出了常见的集中可用手段。</p><p>  最后对这几个方法进行简单的小结，见下表</p><table><thead><tr><th>方法</th><th>示例</th><th>特点</th></tr></thead><tbody><tr><td>ss 指令</td><td>ss -t  -p | grep &lt;进程ID&gt;</td><td>需要借助 while 的方法实现实时查看<br>精确度不能完全保证，查询之间存在间隔<br>不能查看历史的连接信息<br>系统默认会有 ss 指令，不需要额外去安装</td></tr><tr><td>audit 日志</td><td>tail &#x2F;var&#x2F;log&#x2F;audit&#x2F;audit.log</td><td>可查阅历史连接信息<br>依赖环境打开 audit 功能</td></tr><tr><td>perf 工具</td><td>perf record -e ‘syscalls:sys_enter_connect’ -p &lt;进程ID&gt;</td><td>精确度有保障，可实时捕获数据并写入到一个结果文件，便于事后分析。<br>同样的不支持查看历史的连接信息</td></tr><tr><td>lsof 工具</td><td>lsof -i -a -p &lt;进程ID&gt;</td><td>基本同 ss 指令</td></tr><tr><td>strace 工具</td><td>strace -e trace&#x3D;network -p &lt;进程ID&gt;</td><td>精确度有保障，可实时捕获数据<br>支持单条命令调试，也支持加载现有进程。<br>同样的不支持查看历史的连接信息。</td></tr></tbody></table><p> 基于上述的汇总，可以看出这些指令直接是可以取合集做一个工具实现更好的信息捕获的。后续可以实现一个自动化的工具，加入到 orca-tools 里面。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  有些场景下，我们需要了解某个进程正在和哪些服务建立连接。本文主要是记录了捕获进程连接信息的几个实现方法。&lt;/p&gt;
&lt;h2 id=&quot;实现&quot;&gt;&lt;a href=&quot;#实现&quot; class=&quot;headerlink&quot; title=&quot;实现&quot;&gt;&lt;/a&gt;实现&lt;/h2&gt;&lt;h3 id=&quot;ss-</summary>
      
    
    
    
    
    <category term="Linux" scheme="https://guoltan.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>排查 kubectl edit 保存失败方法</title>
    <link href="https://guoltan.github.io/2022/08/23/%E6%8E%92%E6%9F%A5%20kubectl%20edit%20%E4%BF%9D%E5%AD%98%E5%A4%B1%E8%B4%A5%E6%80%9D%E8%B7%AF/"/>
    <id>https://guoltan.github.io/2022/08/23/%E6%8E%92%E6%9F%A5%20kubectl%20edit%20%E4%BF%9D%E5%AD%98%E5%A4%B1%E8%B4%A5%E6%80%9D%E8%B7%AF/</id>
    <published>2022-08-23T12:11:25.000Z</published>
    <updated>2024-01-26T15:14:01.260Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-常见原因"><a href="#1-常见原因" class="headerlink" title="1. 常见原因"></a><strong>1. 常见原因</strong></h2><p>保存失败的常见原因如下：</p><ul><li><p>使用 tab 产生空白字符。</p></li><li><p>使用了无效的选项、参数等。</p></li><li><p>value 未使用字符串。</p></li><li><p>环境的 kubernetes 配置了 webhook，提交 YAML 修改到 apiserver 以后需要完成校验，如果校验失败或 webhook 的组件故障，此时 kubectl edit 保存会失败。</p></li></ul><h2 id="2-排查思路"><a href="#2-排查思路" class="headerlink" title="2. 排查思路"></a><strong>2. 排查思路</strong></h2><p>  通过 kubectl edit 保存配置时，如果使用了无效的参数或者语法错误。将会打印相印的错误提示，错误提示的行首有 # 的字符。根据这些提示即可找到故障原因。</p><p>  另一种问题是，保存 YAML 时未产生错误提示，但是保存后产生了类似如下的错误提示 dial tcp: lookup ti-resource-server.ti-base.svc.cluster.local on [::1]:53: dial udp [::1]:53: connect: network is unreachable。这种场景通常是因为 webhook 组件故障引起的。需要解决 webhook 组件故障。</p><h2 id="3-常见错误原因"><a href="#3-常见错误原因" class="headerlink" title="3. 常见错误原因"></a><strong>3. 常见错误原因</strong></h2><h3 id="3-1-did-not-find-expected-key"><a href="#3-1-did-not-find-expected-key" class="headerlink" title="3.1 did not find expected key"></a><strong>3.1 did not find expected key</strong></h3><p>  如果看见 did not find expected key 的错误，通常是空格数量不正确导致的。上下级选项需要保持两个空格。</p><h3 id="3-2-The-edited-file-failed-validation"><a href="#3-2-The-edited-file-failed-validation" class="headerlink" title="3.2 The edited file failed validation"></a><strong>3.2 The edited file failed validation</strong></h3><p>  如果看见类似 invalid value: “The edited file failed validation”: ValidationError(Deployment.spec): unknown field “12345” in io.k8s.api.apps.v1.DeploymentSpec 的错误，通常是使用了错误的选项。可以通过 kubectl explain 资源.spec.选项名称 来获取有效合法的 key、values 值。</p><h3 id="3-3-cannot-convert-int64-to-string"><a href="#3-3-cannot-convert-int64-to-string" class="headerlink" title="3.3 cannot convert int64 to string"></a><strong>3.3 cannot convert int64 to string</strong></h3><p>  未使用”” 将整型数据括起来。常见于添加 nodeSelector 未给 value 配置 “”。</p><h3 id="3-4-no-matches-for-kind-“xxxx”-in-version-“xxxx”"><a href="#3-4-no-matches-for-kind-“xxxx”-in-version-“xxxx”" class="headerlink" title="3.4 no matches for kind “xxxx” in version “xxxx”"></a><strong>3.4 no matches for kind “xxxx” in version “xxxx”</strong></h3><p>  如果看见类似 The edited file had a syntax error: unable to recognize “edited-file”: no matches for kind “Deployment” in version “extension&#x2F;v1beta1” 的错误，通常是 YAML 编辑的资源使用了错误的 apiVersion 导致的。同样可以通过 kubectl explain 指令来获取对应资源的 apiVersion 可用版本。</p><h3 id="3-5-provided-port-is-not-in-the-valid-range-The-range-of-valid-ports-is-xxxx-xxxx"><a href="#3-5-provided-port-is-not-in-the-valid-range-The-range-of-valid-ports-is-xxxx-xxxx" class="headerlink" title="3.5 provided port is not in the valid range. The range of valid ports is xxxx-xxxx"></a><strong>3.5 provided port is not in the valid range. The range of valid ports is xxxx-xxxx</strong></h3><p>  只存在于 service 修改的场景，如果看见类似于 * spec.ports[0].nodePort: Invalid value: 10508: provided port is not in the valid range. The range of valid ports is 30000-32767 的操作，代表修改 service 配置的 NodePort 超出了 kube-apiserver 中 –service-node-port-range 定义的范围，这个范围默认值是 30000-32767，也就是说 NodePort 配置的端口不能超出这个范围。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-常见原因&quot;&gt;&lt;a href=&quot;#1-常见原因&quot; class=&quot;headerlink&quot; title=&quot;1. 常见原因&quot;&gt;&lt;/a&gt;&lt;strong&gt;1. 常见原因&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;保存失败的常见原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;使用 tab</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>Pod 内无法解析域名排查思路</title>
    <link href="https://guoltan.github.io/2022/02/01/Pod%20%E5%86%85%E6%97%A0%E6%B3%95%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF/"/>
    <id>https://guoltan.github.io/2022/02/01/Pod%20%E5%86%85%E6%97%A0%E6%B3%95%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF/</id>
    <published>2022-02-01T12:01:15.000Z</published>
    <updated>2024-03-25T16:06:40.274Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h1><h2 id="Pod-内域名解析失败的现象"><a href="#Pod-内域名解析失败的现象" class="headerlink" title="Pod 内域名解析失败的现象"></a><strong>Pod 内域名解析失败的现象</strong></h2><p>  域名解析失败的现象，通过 ping、curl 或者服务日志中，可以看到域名解析失败的错误，关键字是 Could not resolve host 或者 Name or service not known。如下图所示</p><p>  可能故障的原因如下：</p><ul><li><p>客户端请求了错误的域名。</p></li><li><p>客户端未配置正确的 DNS 服务器。</p></li><li><p>coredns 服务故障。</p></li><li><p>网络插件存在异常，Pod 无法通过 kube-dns 这个服务访问 coredns。</p></li><li><p>客户端请求解析外部域名，coredns 无法连通外部 DNS 服务器，导致解析失败。</p></li></ul><h2 id="域名解析请求图"><a href="#域名解析请求图" class="headerlink" title="域名解析请求图"></a><strong>域名解析请求图</strong></h2><p>  客户端在请求域名解析前会先去查询 &#x2F;etc&#x2F;hosts 中是否有配置域名解析记录。此处可以通过在 Pod.Spec 中使用 hostAliases 参数来添加。如果 &#x2F;etc&#x2F;hosts 没有解析记录。它会再通过 &#x2F;etc&#x2F;resolv.conf 里面配置的参数，将域名解析请求转发给指定的服务器。使用非 HostNetwork 模式的 Pod 会将请求转发给 coredns。</p><p>  默认情况下，coredns 只存在 cluster.local 域的解析记录。默认情况下，每条解析记录都是以 <strong>服务名称.命名空间名称.svc.cluster.local</strong> 的形式存在的。对于非 cluster.local 域的解析请求，会通过 forward 的配置参数进行转发。默认情况下，会将请求转发到容器内配置的 &#x2F;etc&#x2F;resolv.conf 中指定的 nameserver。</p><h1 id="Pod-内无法解析域名的排查思路"><a href="#Pod-内无法解析域名的排查思路" class="headerlink" title="Pod 内无法解析域名的排查思路"></a><strong>Pod 内无法解析域名的排查思路</strong></h1><p>  排查场景分为内部服务域名解析和外部服务域名解析，内部服务域名指的是 kubernetes 内部 service 的域名，他的完整域名形式通常为 <strong>服务名称.命名空间名称.svc.cluster.local</strong>，如：mysql.inf.svc.cluster.local。而外部域名则是非 kubernetes 内部 service 的域名，例如：<a href="http://api.weixin.qq.com/"><u><span class="16">api.weixin.qq.com</span></u></a>、redcs.tencent.com等等。</p><p>  如果是内部服务域名解析的问题，需要优先确认一下客户端请求的域名是否存在，coredns 服务是否正常，pod 和 coredns 之间的服务连通性是否正常。</p><p>  如果是外部服务域名解析的问题，基于内部服务域名解析的排查项还需要确认一下 coredns 和外部 DNS 之间的连通性是否正常，是否配置了正确的外部 DNS。</p><h2 id="内部服务域名解析失败排查"><a href="#内部服务域名解析失败排查" class="headerlink" title="内部服务域名解析失败排查"></a><strong>内部服务域名解析失败排查</strong></h2><h3 id="确认-Pod-内的-etc-resolv-conf-配置"><a href="#确认-Pod-内的-etc-resolv-conf-配置" class="headerlink" title="确认 Pod 内的 &#x2F;etc&#x2F;resolv.conf 配置"></a><strong>确认 Pod 内的 &#x2F;etc&#x2F;resolv.conf 配置</strong></h3><p>  <strong>通过 kubectl 指令进入容器，查询 &#x2F;etc&#x2F;resolv.conf 配置</strong></p><p>    可以通过 kubectl exec -it -n 命名空间 Pod名称 &#x2F;bin&#x2F;sh 的指令，进入的容器中，然后通过 cat &#x2F;etc&#x2F;resolv.conf 的指令获取 DNS 服务器配置。如果输出的内容如下示例，代表该容器的域名解析会提交给 coredns。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver 10.96.0.10</span><br><span class="line">search ti-inf.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure><p>    如果输出的内容如下示例，说明该容器可能是直接使用的宿主机 &#x2F;etc&#x2F;resolv.conf 配置，容器可能是 hostNetwork 模式的。这个时候他请求域名解析的记录不会发送到 coredns。<strong>如果直接请求 kubernetes 内部服务名称，则会失败。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nameserver 183.60.83.19</span><br><span class="line">nameserver 183.60.82.98</span><br></pre></td></tr></table></figure><p>  <strong>通过登录 Pod 宿主机，查看容器工作目录的方式查询 &#x2F;etc&#x2F;resolv.conf 的配置</strong></p><p>    某些容器内部无法使用 &#x2F;bin&#x2F;sh 或者 &#x2F;bin&#x2F;bash，这个时候如果要确认 resolv.conf 的配置，可以通过登录宿主机，然后执行 <code>docker ps -a | grep Pod 名称</code>，获取容器的 ID，再通过 <code>docker inspect 容器ID -f &#39;&#123;&#123;.ResolvConfPath&#125;&#125;&#39;</code> 的指令获取当前 Pod resolv.conf 在宿主机上的路径。期望的获取结果如下示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/docker/containers/a9691be1de0edd7d8edaf83b046e84594486ffbdd9bfb0a113d51d986830fbb1/resolv.conf</span><br></pre></td></tr></table></figure><h3 id="确认-coredns-服务是否正常"><a href="#确认-coredns-服务是否正常" class="headerlink" title="确认 coredns 服务是否正常"></a><strong>确认 coredns 服务是否正常</strong></h3><p>  通过 <strong>kubectl get pod -n kube-system -o wide | grep coredns</strong> 的指令获取当前 coredns 的信息，期望输出内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment"># kubectl get pod -n kube-system -o wide | grep coredns</span></span><br><span class="line">coredns-6d8cbdcf64-gkhcg               1/1     Running   0          14d   10.199.0.7     172.27.0.9                </span><br><span class="line">coredns-6d8cbdcf64-ht2vz               1/1     Running   0          14d   10.199.0.8     172.27.0.9</span><br></pre></td></tr></table></figure><p>  如果 coredns 状态为非 Running，代表 coredns 服务未正常允许，可以参考 Pod 排查指引来处理。</p><h3 id="确认内部域名是否合法"><a href="#确认内部域名是否合法" class="headerlink" title="确认内部域名是否合法"></a><strong>确认内部域名是否合法</strong></h3><p>  合法的 kubernetes 内部域名应该可以通过 <strong>kubectl get svc -n 命名空间 服务名称</strong> 来获取到的。比如说 ti-mysql.ti-inf.svc.cluster.local. 这个域名对应的服务应该是能够通过 kubectl get svc -n ti-inf ti-mysql 的指令获取到。</p><h3 id="收集-coredns-服务的日志，查看解析记录"><a href="#收集-coredns-服务的日志，查看解析记录" class="headerlink" title="收集 coredns 服务的日志，查看解析记录"></a><strong>收集 coredns 服务的日志，查看解析记录</strong></h3><p>  通过 kubectl logs -n kube-system coredns-xxxx-xxx 的指令可以获取 coredns 的日志输出，如果域名无法解析，需要转发给上级 DNS 服务器，类似输出的内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:46205-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:56372-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:36434-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. AAAA: <span class="built_in">read</span> udp 10.199.0.8:46337-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. AAAA: <span class="built_in">read</span> udp 10.199.0.8:41195-&gt;183.60.82.98:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. AAAA: <span class="built_in">read</span> udp 10.199.0.8:51501-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:54489-&gt;183.60.82.98:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:51802-&gt;183.60.82.98:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:48109-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br><span class="line">[ERROR] plugin/errors: 2 ti-mysql.ti-inf.svc.cluster.local.qqq. A: <span class="built_in">read</span> udp 10.199.0.8:36779-&gt;183.60.83.19:53: i/o <span class="built_in">timeout</span></span><br></pre></td></tr></table></figure><p>  如果内部域名解析请求需要提交给上级 DNS 服务器来处理，则可能是该域名不合法，在环境中应该不存在对应的 service。如果在 coredns 未能查询到相关的日志，需要通过抓包确认流量是否已经到达 coredns。</p><h3 id="抓取-coredns-的报文，查看请求域名解析的流量是否已经到达"><a href="#抓取-coredns-的报文，查看请求域名解析的流量是否已经到达" class="headerlink" title="抓取 coredns 的报文，查看请求域名解析的流量是否已经到达"></a><strong>抓取 coredns 的报文，查看请求域名解析的流量是否已经到达</strong></h3><p>  通过如下步骤抓取报文，查看报文中是否有相应域名解析的记录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1. 确认 Pod 当前运行节点</span><br><span class="line">  kubectl get pod -n kube-system -o wide | grep coredns</span><br><span class="line"></span><br><span class="line">2. 登录到 coredns 所在的节点，查找 coredns 对应的容器 ID</span><br><span class="line"> docker ps -a | grep coredns</span><br><span class="line"></span><br><span class="line">3. 获取容器的 PID</span><br><span class="line">  docker inspect -f &#123;&#123;.State.Pid&#125;&#125;</span><br><span class="line"></span><br><span class="line">4. 进入容器的网络命名空间</span><br><span class="line">  nsenter -t PID -n</span><br><span class="line"></span><br><span class="line">5. 抓取报文</span><br><span class="line">  tcpdump -i eth0</span><br></pre></td></tr></table></figure><h3 id="确认-coredns-配置，是否使用了-hosts-插件，但未配置-fallthrough"><a href="#确认-coredns-配置，是否使用了-hosts-插件，但未配置-fallthrough" class="headerlink" title="确认 coredns 配置，是否使用了 hosts 插件，但未配置 fallthrough"></a><strong>确认 coredns 配置，是否使用了 hosts 插件，但未配置 fallthrough</strong></h3><p>  coredns 默认情况下启用了多个插件，请求被当前插件以 Fallthrough 的形式处理，如果请求在该插件处理过程中有可能会跳转到下一个插件，这个过程被称为 fallthrough。根据 fallthrough 配置来控制插件是否能够跳转到下一个插件，比如启用了 hosts 插件，但是 hosts 解析域名失败，如果不配置 fallthrough，它将不会继续跳转到下一个插件。导致解析失败。</p><p>  于是乎我们还可以检查下 coredns 是否出现配置了 hosts 插件，但是未启用 fallthrough。参考如下配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    .:53 &#123;</span></span><br><span class="line"><span class="string">        errors</span></span><br><span class="line"><span class="string">        health</span></span><br><span class="line"><span class="string">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span></span><br><span class="line"><span class="string">           pods insecure</span></span><br><span class="line"><span class="string">           upstream</span></span><br><span class="line"><span class="string">           fallthrough in-addr.arpa ip6.arpa</span></span><br><span class="line"><span class="string">           ttl 30</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        hosts tke.cn &#123; #此处代表使用了 hosts 插件</span></span><br><span class="line"><span class="string">          172.27.0.123 a.tke.cn</span></span><br><span class="line"><span class="string">          fallthrough  # 观察此处是否配置了 fallthrough，如果启用 hosts 插件但是未配置该选项，会导致业务解析功能异常</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        prometheus :9153</span></span><br><span class="line"><span class="string">        forward . /etc/resolv.conf</span></span><br><span class="line"><span class="string">        cache 30</span></span><br><span class="line"><span class="string">        loop</span></span><br><span class="line"><span class="string">        reload</span></span><br><span class="line"><span class="string">        loadbalance</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br></pre></td></tr></table></figure><h2 id="外部服务域名解析失败排查"><a href="#外部服务域名解析失败排查" class="headerlink" title="外部服务域名解析失败排查"></a><strong>外部服务域名解析失败排查</strong></h2><h3 id="确认上级转发-DNS-的配置是否正确"><a href="#确认上级转发-DNS-的配置是否正确" class="headerlink" title="确认上级转发 DNS 的配置是否正确"></a><strong>确认上级转发 DNS 的配置是否正确</strong></h3><p>  通过 2.1.4 的步骤获取到 coredns 的服务日志时，如果刚好看到类似 read udp 10.199.0.8:46205-&gt;183.60.83.19:53: i&#x2F;o timeout 的错误，此时上级 DNS 就是 183.60.83.19。这种错误通常可能的原因是无法连通上级 DNS。现场可以根据实际环境对比一下日志打印的上级 DNS 服务器的地址是否是预期的地址。</p><p>  上级 DNS 是通过 coredns 中指定 forward 配置项来设置的，通过 kubectl get cm -n kube-system coredns -o yaml 可以查看 coredns 的配置，示例输出如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">.:53</span> &#123;</span><br><span class="line">    <span class="string">errors</span></span><br><span class="line">    <span class="string">health</span> &#123;</span><br><span class="line">       <span class="string">lameduck</span> <span class="string">5s</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">ready</span></span><br><span class="line">    <span class="string">kubernetes</span> <span class="string">cluster.local</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span> &#123;</span><br><span class="line">       <span class="string">pods</span> <span class="string">insecure</span></span><br><span class="line">       <span class="string">fallthrough</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span></span><br><span class="line">       <span class="string">ttl</span> <span class="number">30</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">prometheus</span> <span class="string">:9153</span></span><br><span class="line">    <span class="string">forward</span> <span class="string">.</span> <span class="string">/etc/resolv.conf</span> &#123;</span><br><span class="line">       <span class="string">max_concurrent</span> <span class="number">1000</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">    <span class="string">loop</span></span><br><span class="line">    <span class="string">reload</span></span><br><span class="line">    <span class="string">loadbalance</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  如果 forward 的参数只指定了 &#x2F;etc&#x2F;resolv.conf 则代表会将请求转发 &#x2F;etc&#x2F;resolv.conf 中配置的 nameserver。值得注意的一点是，coredns 容器内的 &#x2F;etc&#x2F;resolv.conf 是最初从宿主机的 &#x2F;etc&#x2F;resolv.conf 继承过来的。如果使用 <strong>forward .  &#x2F;etc&#x2F;resolv.conf</strong> 的方式指定上级 DNS，后续需要修改 coredns 的上级 DNS 服务器则需要修改 coredns 宿主机的 &#x2F;etc&#x2F;resolv.conf，并重启 coredns 生效。</p><h3 id="确认上级转发-DNS-连通性是否正常"><a href="#确认上级转发-DNS-连通性是否正常" class="headerlink" title="确认上级转发 DNS 连通性是否正常"></a><strong>确认上级转发 DNS 连通性是否正常</strong></h3><p>  只需要登录到 coredns 所在的宿主机去 ping 上级 DNS 的地址或者通过在宿主机上使用 <strong>dig @服务器地址</strong> 域名 来测试连通性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h1&gt;&lt;h2 id=&quot;Pod-内域名解析失败的现象&quot;&gt;&lt;a href=&quot;#Pod-内域名解析失败的现象&quot; class=</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>Pod 状态 Pending 处理思路</title>
    <link href="https://guoltan.github.io/2022/01/20/Pending/"/>
    <id>https://guoltan.github.io/2022/01/20/Pending/</id>
    <published>2022-01-20T10:00:15.000Z</published>
    <updated>2024-03-25T15:35:44.503Z</updated>
    
    <content type="html"><![CDATA[<h1 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h1><p>  如果 Pod 处于 Pending 状态，这意味着当前集群不存在满足 Pod 运行条件的节点，需要通过<code> kubectl describe pod名称 -n 命名空间</code> 获取该 Pod 相关的 <code>Events</code>。基于 <code>Events</code> 给出的信息来处理。</p><p>  对于 Pod 一直处于 Pending 状态的问题，通常可能的原因如下：</p><ul><li>集群节点资源不足（CPU、内存、GPU和其他）</li><li>不满足亲和性、反亲和性规则（nodeSelector、Affinity）</li><li>节点存在污点，Pod 无容忍（节点异常被动添加污点、管理员手动添加污点）</li><li>调度器存在 BUG 或者当前调度器存在故障</li></ul><h1 id="排查思路"><a href="#排查思路" class="headerlink" title="排查思路"></a>排查思路</h1><h2 id="节点资源类"><a href="#节点资源类" class="headerlink" title="节点资源类"></a>节点资源类</h2><h3 id="CPU-或内存不足"><a href="#CPU-或内存不足" class="headerlink" title="CPU 或内存不足"></a>CPU 或内存不足</h3><h4 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> Insufficient cpu, Insufficient memory.</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">Type</span>     <span class="string">Reason</span>            <span class="string">Age</span>                <span class="string">From</span>               <span class="string">Message</span></span><br><span class="line">  <span class="string">----</span>     <span class="string">------</span>            <span class="string">----</span>               <span class="string">----</span>               <span class="string">-------</span></span><br><span class="line">  <span class="string">Warning</span>  <span class="string">FailedScheduling</span>  <span class="string">10s</span> <span class="string">(x2</span> <span class="string">over</span> <span class="string">10s)</span>  <span class="attr">default-scheduler  0/4 nodes are available:</span> <span class="number">4</span> <span class="string">Insufficient</span> <span class="string">cpu,</span> <span class="number">3</span> <span class="string">Insufficient</span> <span class="string">memory.</span></span><br></pre></td></tr></table></figure><h4 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该类错误是由于集群的机器资源不足，无法满足待调度 Pod 最低运行要求。对于此类问题需要先获取待调度 Pod 的资源需求，通过查看 cpu、memory 剩余的可分配资源量，对比 Pod 申请的资源量，我们就可以知道节点资源和 Pod 申请量的差值。再根据实际情况去降低 Pod 的资源要求或者在节点上释放其他不需要的资源、扩容节点资源等方式处理。</p><p>  使用 volcano 作为调度器的场景，资源不足类的错误统一为 <code>all nodes are unavailable: 10 node(s) resource fit failed.</code> 如果看到类似报错，也适用下面的操作方法来排查问题。</p><h4 id="处理步骤"><a href="#处理步骤" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="获取待调度-Pod-的资源需求"><a href="#获取待调度-Pod-的资源需求" class="headerlink" title="获取待调度 Pod 的资源需求"></a>获取待调度 Pod 的资源需求</h5><p>​    通过 <code>kubectl get pod -n 命名空间 Pod名称</code> 可以查询 Pod 的配置，查询关键字段是 containers 下的 resources</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span>  <span class="comment"># 运行该 Pod 的最低资源需求，调度也是根据 requests 的配置来的。</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;50&quot;</span>  <span class="comment"># 代表运行该 Pod 的节点至少需要 50U 可供使用，此处可能是以其他方式表示，如 50000m。</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">50Gi</span> <span class="comment"># 代表运行该 Pod 的节点至少需要 50Gi 资源，此处可能是以其他方式表示，如 50000Mi。</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>​    也可以通过 <code>kubectl</code> 提取关键字段的方式获取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例指令</span></span><br><span class="line">kubectl get pod hello-world -o jsonpath=&#x27;&#123;.spec.containers[*].resources.requests&#125;&#x27;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出</span></span><br><span class="line">map[cpu:50 memory:50Gi]</span><br></pre></td></tr></table></figure><h5 id="获取节点可分配的资源量"><a href="#获取节点可分配的资源量" class="headerlink" title="获取节点可分配的资源量"></a>获取节点可分配的资源量</h5><p>​    通过 <code>kubectl describe node 节点名称 | grep -A20 &quot;Allocated resources:&quot;</code> 可以获取节点当前资源分配的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Allocated resources:</span><br><span class="line">  (Total limits may be over 100 percent, i.e., overcommitted.)</span><br><span class="line">  Resource                 Requests       Limits</span><br><span class="line">  --------                 --------       ------</span><br><span class="line">  cpu                      10331m (21%)   37 (77%)</span><br><span class="line">  memory                   20796Mi (21%)  60363Mi (63%)</span><br><span class="line">  ephemeral-storage        0 (0%)         0 (0%)</span><br><span class="line">  hostdev.k8s.io/dev_fuse  0              0</span><br><span class="line">  hostdev.k8s.io/dev_mem   0              0</span><br><span class="line">Events:                    &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="GPU-资源不足"><a href="#GPU-资源不足" class="headerlink" title="GPU 资源不足"></a>GPU 资源不足</h3><h4 id="故障现象-1"><a href="#故障现象-1" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> Insufficient tencent.com/vcuda-core, Insufficient tencent.com/vcuda-memory.</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                From               Message</span><br><span class="line">  ----     ------            ----               ----               -------</span><br><span class="line">  Warning  FailedScheduling  10s (x2 over 10s)  default-scheduler  0/4 nodes are available: 4 Insufficient tencent.com/vcuda-core, 3 Insufficient tencent.com/vcuda-memory.</span><br></pre></td></tr></table></figure><h4 id="解决思路-1"><a href="#解决思路-1" class="headerlink" title="解决思路"></a>解决思路</h4><p>  此类问题的可能原因如下：</p><ul><li><p>gpu-manager 故障，或 GPU 节点未配置 gpu 标签。</p></li><li><p>gpu-manager 发现不到 GPU 资源，通常是 GPU 驱动未安装正确导致的。</p></li><li><p>同 <strong>CPU 或内存不足</strong> 的案例，可能是节点资源不足。</p></li><li><p>如果是节点资源充足，但是无法创建，可能原因是 kube-scheduler 的 BUG，或者是当前环境存在 GPU 碎片。</p><p>首先要确保集群内存在 GPU 资源，并且 node 信息上已经识别出 GPU 资源。然后要确保调度器已支持 GPU 类资源的使用，满足这两个条件环境才能实现 GPU 资源的调度。</p><p>GPU 资源是通过 device-plugins 的机制去发现的，如果说我们发现 GPU 节点上没有发现任何可用的 GPU 资源，这时候先要去检查  gpu-manager 的组件运行是否正常。</p><p>如果节点存在可用的资源，但是无法调用成功。此时可以参考 <strong>CPU 或内存不足</strong> 的处理方法，确认集群资源是否足够。此外还存在一种场景，就是如果环境同时去调度大量 Pod，这些 Pod 申请的 GPU 资源量较小也可能会导致 Pod 调度失败。原理可以参考<a href="https://docs.qq.com/doc/DV2pJVUJVTmdycnFq">《GPU碎片》</a>，解决思路可以参考<a href="https://gdc.lexiangla.com/teams/k100044/docs/9261de46c75511eba30996d948652e79?company_from=gdc">《云智天枢-单节点vGPU调度优化》</a> ，以及<a href="https://gdc.lexiangla.com/teams/k100044/docs/a17471765a6511eb832766820c31d5db?company_from=gdc">《云智天枢-TKE 2.9.4+TIMatrix 2.5.1-vGPU调度失败问题》</a>。</p></li></ul><h4 id="处理步骤-1"><a href="#处理步骤-1" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="获取待调度-Pod-的资源需求-1"><a href="#获取待调度-Pod-的资源需求-1" class="headerlink" title="获取待调度 Pod 的资源需求"></a>获取待调度 Pod 的资源需求</h5><p>​    通过 <code>kubectl get pod -n 命名空间 Pod名称</code> 可以查询 Pod 的配置，查询关键字段是 containers 下的 resources</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span>  <span class="comment"># 运行该 Pod 的最低资源需求，调度也是根据 requests 的配置来的。</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-core:</span> <span class="string">&quot;50&quot;</span>  <span class="comment"># 代表 Pod 运行最低需要使用到的 GPU 卡的计算资源，50 代表 0.5 卡，当配置为 100 时将会独占单卡资源。</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-memory:</span> <span class="string">&quot;20&quot;</span> <span class="comment"># 代表 Pod 运行最低需要使用到的 GPU 卡显存资源，1 等同于 256 Mi 显存，配置为 20 代表需要使用 5120Mi 的显存。</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>​    也可以通过 <code>kubectl</code> 提取关键字段的方式获取信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例指令</span></span><br><span class="line">kubectl get pod hello-world -o jsonpath=&#x27;&#123;.spec.containers[*].resources.requests&#125;&#x27;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出</span></span><br><span class="line">map[cpu:50 memory:50Gi tencent.com/vcuda-core:50 tencent.com/vcuda-memory:20]</span><br></pre></td></tr></table></figure><h5 id="获取节点可分配的资源量-1"><a href="#获取节点可分配的资源量-1" class="headerlink" title="获取节点可分配的资源量"></a>获取节点可分配的资源量</h5><p>​    通过 <code>kubectl describe node 节点名称 | grep -A20 &quot;Capacity&quot;</code> 可以获取节点是否已成功获取到 GPU 资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Capacity:</span><br><span class="line"> cpu:                       20</span><br><span class="line"> ephemeral-storage:         515927276Ki</span><br><span class="line"> hostdev.k8s.io/dev_fuse:   128</span><br><span class="line"> hostdev.k8s.io/dev_mem:    128</span><br><span class="line"> hugepages-1Gi:             0</span><br><span class="line"> hugepages-2Mi:             0</span><br><span class="line"> memory:                    82319956Ki</span><br><span class="line"> nvidia.com/gpu:            1</span><br><span class="line"> pods:                      255</span><br><span class="line"> tencent.com/vcuda-core:    100    # 此处不能为 0，如果为 0 代表 gpu-manager 运行异常</span><br><span class="line"> tencent.com/vcuda-memory:  59     # 此处不能为 0，如果为 0 代表 gpu-manager 运行异常</span><br><span class="line">Allocatable:</span><br><span class="line"> cpu:                       19800m</span><br><span class="line"> ephemeral-storage:         475478576775</span><br><span class="line"> hostdev.k8s.io/dev_fuse:   128</span><br><span class="line"> hostdev.k8s.io/dev_mem:    128</span><br><span class="line"> hugepages-1Gi:             0</span><br><span class="line"> hugepages-2Mi:             0</span><br><span class="line"> memory:                    81193556Ki</span><br><span class="line"> nvidia.com/gpu:            1</span><br></pre></td></tr></table></figure><p>​    通过 <code>kubectl describe node 节点名称 | grep -A20 &quot;Allocated resources:&quot;</code> 可以获取节点当前资源分配的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Allocated resources:</span><br><span class="line">  (Total limits may be over 100 percent, i.e., overcommitted.)</span><br><span class="line">  Resource                  Requests     Limits</span><br><span class="line">  --------                  --------     ------</span><br><span class="line">  cpu                       3314m (16%)  26900m (135%)</span><br><span class="line">  memory                    3427Mi (4%)  35936Mi (45%)</span><br><span class="line">  ephemeral-storage         0 (0%)       0 (0%)</span><br><span class="line">  hostdev.k8s.io/dev_fuse   0            0</span><br><span class="line">  hostdev.k8s.io/dev_mem    0            0</span><br><span class="line">  nvidia.com/gpu            0            0</span><br><span class="line">  tencent.com/vcuda-core    100          100</span><br><span class="line">  tencent.com/vcuda-memory  59           59</span><br><span class="line">Events:                     &lt;none&gt;</span><br></pre></td></tr></table></figure><h5 id="检查-gpu-manager-组件状态是否可用"><a href="#检查-gpu-manager-组件状态是否可用" class="headerlink" title="检查 gpu-manager 组件状态是否可用"></a>检查 gpu-manager 组件状态是否可用</h5><p>​    如果在 <strong>获取节点可分配的资源量</strong> 步骤中发现节点无法获取到 GPU 资源。此时可以通过 <code>kubectl get pod -o wide -A | grep gpu-manager | grep 节点地址</code> 可以获取节点上是否正确运行 gpu-manager</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出如下，类似输出代表 gpu-manager 已正常运行</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get pod -o wide -A | grep gpu-manager | grep 192.168.0.57</span><br><span class="line">timaker        gpu-manager-9g9ps                                                1/1     Running     0          9h      10.199.5.181    192.168.0.57   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>  如果 gpu-manager 正常运行，但是 <code>tencent.com/vcuda-core</code> 依然为 0，需要通过 <code>kubectl logs -n 命名空间 Pod名称</code> 来收集组件的日志，提安灯工单给后端同学确认问题点。</p><p>  如果在指定节点上找不到 gpu-manager 这个 Pod，通常是由于节点不满足 gpu-manager 创建条件导致的。可能是节点未正确配置 gpu 的标签，通过 <code>kubectl get nodes --show-labels | grep 节点名称 | grep nvidia-device-enable</code> 来检查 GPU 标签。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出，注意 TKE、TIM 场景下，GPU 标签 key 固定为 nvidia-device-enable，value 为 <span class="built_in">enable</span> 代表该节点需要启动 gpu-manager。</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get nodes --show-labels | grep 192.168.0.57 | grep nvidia-device-enable</span><br><span class="line">192.168.0.57   Ready    &lt;none&gt;   19d   v1.16.15-alauda.3   GPUName=T4,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.57,kubernetes.io/os=linux,nvidia-device-enable=enable</span><br></pre></td></tr></table></figure><h3 id="端口不可用"><a href="#端口不可用" class="headerlink" title="端口不可用"></a>端口不可用</h3><h4 id="故障现象-2"><a href="#故障现象-2" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> didn&#39;t have free ports for the requested pod ports.</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">Type</span>     <span class="string">Reason</span>            <span class="string">Age</span>                <span class="string">From</span>               <span class="string">Message</span></span><br><span class="line">  <span class="string">----</span>     <span class="string">------</span>            <span class="string">----</span>               <span class="string">----</span>               <span class="string">-------</span></span><br><span class="line">  <span class="string">Warning</span>  <span class="string">FailedScheduling</span>  <span class="string">10s</span> <span class="string">(x2</span> <span class="string">over</span> <span class="string">10s)</span>  <span class="attr">default-scheduler  0/4 nodes are available:</span> <span class="number">4</span> <span class="string">node(s)</span> <span class="string">didn&#x27;t</span> <span class="string">have</span> <span class="string">free</span> <span class="string">ports</span> <span class="string">for</span> <span class="string">the</span> <span class="string">requested</span> <span class="string">pod</span> <span class="string">ports.</span></span><br></pre></td></tr></table></figure><h4 id="解决思路-2"><a href="#解决思路-2" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该问题出现在 Pod 使用了 <code>hostNetwork: true</code> 配置的场景，使用该参数 Pod 将会和宿主机网络命名空间，如果宿主机已监听了 Pod 运行需要使用的端口，此时将会触发这个错误。</p><p>  比较常见遇到该类问题的场景是更新 deployment 的配置，每当我们去修改 deployment 配置时，控制器就会自动创建新的 replicaset ，replicaset 使用最新的配置来生成 Pod。而 deployment 考虑到应用可用性。他在默认情况下是不会直接替换新的 replicaset，而是逐个替换旧的 replicaset 的 Pod，可以理解成是一种滚动更新的模式。</p><p>  而这种模式会在 Pod 使用了 <code>hostNetwork: true</code> 配置的场景下触发端口冲突的问题。因为控制器需要先创建可用的副本，才能释放旧副本。但是旧副本又占用了新副本的资源，相当于形成了一个死锁。在这种情况下业务将无法更新。</p><p>  解决思路如下：</p><ul><li>直接停止掉所有的旧副本，通过最新的 replicaset 创建新的 Pod 副本来更新配置。</li><li>修改 deployment 的更新策略。将默认使用的 RollingUpdate，修改为 Recreate 的模式。</li></ul><h4 id="处理步骤-2"><a href="#处理步骤-2" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="方法一：暂停所有副本，重新创建应用"><a href="#方法一：暂停所有副本，重新创建应用" class="headerlink" title="方法一：暂停所有副本，重新创建应用"></a>方法一：暂停所有副本，重新创建应用</h5><p>​    通过 scale 的方式调整副本数量，可参考如下步骤执行，<strong>注意该步骤执行时会暂停业务</strong>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先释放所有旧副本，此时业务不可用</span></span><br><span class="line">kubectl scale deployment -n 命名空间 deployment名称 --replicas=0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改副本数，重新创建新 Pod，恢复业务</span></span><br><span class="line">kubectl scale deployment -n 命名空间 deployment名称 --replicas=期望副本数</span><br></pre></td></tr></table></figure><h5 id="方法二：修改-deployment-更新策略为-Recreate"><a href="#方法二：修改-deployment-更新策略为-Recreate" class="headerlink" title="方法二：修改 deployment 更新策略为  Recreate"></a>方法二：修改 deployment 更新策略为  Recreate</h5><p>​    修改 deployment 的更新策略为 Recreate，这样一旦触发 deployment 更新配置，就会直接重建所有 Pod。此时不会触发端口冲突类的问题。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑配置</span></span><br><span class="line">kubectl edit deployment -n 命名空间 deployment 名称</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出结果</span></span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:       # 默认为 RollingUpdate 配置的参数，删除掉</span><br><span class="line">      maxSurge: 1        # 默认为 RollingUpdate 配置的参数，删除掉</span><br><span class="line">      maxUnavailable: 1  # 默认为 RollingUpdate 配置的参数，删除掉</span><br><span class="line">    type: RollingUpdate  # 此处替换为 Recreate</span><br></pre></td></tr></table></figure><h2 id="亲和性配置类"><a href="#亲和性配置类" class="headerlink" title="亲和性配置类"></a>亲和性配置类</h2><h3 id="节点不满足-nodeSelector-配置"><a href="#节点不满足-nodeSelector-配置" class="headerlink" title="节点不满足 nodeSelector 配置"></a>节点不满足 nodeSelector 配置</h3><h4 id="故障现象-3"><a href="#故障现象-3" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> didn&#39;t match node selector.</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Node-Selectors:  app=123</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 30s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 30s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                    From               Message</span><br><span class="line">  ----     ------            ----                   ----               -------</span><br><span class="line">  Warning  FailedScheduling  3m47s (x509 over 11h)  default-scheduler  0/4 nodes are available: 4 node(s) didn&#x27;t match node selector.</span><br></pre></td></tr></table></figure><h4 id="解决思路-3"><a href="#解决思路-3" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该问题出现在 Pod 使用 <code>nodeSelector</code> 配置的场景，该配置是用于固定调度 Pod 到满足标签的节点上。如果集群内不存在满足 Pod 配置标签的节点，调度就会失败。解决此类问题的方法是，排查 Pod 的 nodeSelector 配置以及当前集群各个节点的 labels 配置。和 <code>nodeSelector</code> 有关的其他信息，可参考<a href="https://gdc.lexiangla.com/teams/k100044/docs/cf110fa8808611ebb2636263495fb44b?company_from=gdc">《 kubernetes 调度》</a> 中 <code>nodeSelector</code> 相关的章节。</p><h4 id="处理步骤-3"><a href="#处理步骤-3" class="headerlink" title="处理步骤"></a>处理步骤</h4><h5 id="1-确认当前-Pod-的-nodeSelector-配置"><a href="#1-确认当前-Pod-的-nodeSelector-配置" class="headerlink" title="1. 确认当前 Pod 的 nodeSelector 配置"></a>1. 确认当前 Pod 的 nodeSelector 配置</h5><p>  通过 <code>kubectl get pod -n 命名空间 Pod名称 -o yaml </code> 可以查询 Pod 的配置，查询关键字段是 spec 下的 nodeSelector</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">...</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - /bin/echo</span><br><span class="line">    - hello”,”world</span><br><span class="line">    image: busybox</span><br><span class="line">  nodeSelector:</span><br><span class="line">    app: &quot;123&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>  也可以通过 <code>kubectl get pod -n 命名空间 Pod名称 -o jsonpath=&#39;&#123;.spec.nodeSelector&#125;&#39;</code> 指令快速获取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出</span></span><br><span class="line">[root@tke-192-168-0-2 tttgl]# kubectl get pod  hello-world -o jsonpath=&#x27;&#123;.spec.nodeSelector&#125;&#x27;</span><br><span class="line">map[app:123]</span><br></pre></td></tr></table></figure><h5 id="2-确认当前-node-的-label-配置"><a href="#2-确认当前-node-的-label-配置" class="headerlink" title="2. 确认当前 node 的 label 配置"></a>2. 确认当前 node 的 label 配置</h5><p>  通过 <code>kubectl get node --show-labels </code> 可以查询节点标签</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get nodes --show-labels</span><br><span class="line">NAME           STATUS   ROLES    AGE   VERSION             LABELS</span><br><span class="line">192.168.0.2    Ready    master   98d   v1.16.15-alauda.3   alertmanager=kube-prometheus,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,es=true,ingress=true,kafka=true,kube-ovn/role=master,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.2,kubernetes.io/os=linux,log=true,monitoring=enabled,node-role.kubernetes.io/master=,nvidia-device-enable=false,prometheus=prometheus-0,recovery=true,zk=true</span><br><span class="line">192.168.0.25   Ready    master   98d   v1.16.15-alauda.3   app/ti-license=,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,es=true,ingress=true,kafka=true,kube-ovn/role=master,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.25,kubernetes.io/os=linux,log=true,node-role.kubernetes.io/master=,nvidia-device-enable=false,recovery=true,zk=true</span><br><span class="line">192.168.0.43   Ready    master   13d   v1.16.15-alauda.3   app/ti-license=,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ecovery=true,es=true,ingress=true,kafka=true,kube-ovn/role=master,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.43,kubernetes.io/os=linux,log=true,node-role.kubernetes.io/master=,nvidia-device-enable=false,recovery=true,zk=true</span><br><span class="line">192.168.0.57   Ready    &lt;none&gt;   20d   v1.16.15-alauda.3   GPUName=T4,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.57,kubernetes.io/os=linux,nvidia-device-enable=enable</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">也可以通过 grep 过滤具备标签的节点，示例输出如下</span></span><br><span class="line">[root@tke-192-168-0-2 ~]# kubectl get nodes --show-labels | grep GPUName</span><br><span class="line">192.168.0.57   Ready    &lt;none&gt;   20d   v1.16.15-alauda.3   GPUName=T4,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.0.57,kubernetes.io/os=linux,nvidia-device-enable=enable</span><br></pre></td></tr></table></figure><p>  如果发现没有节点符合 Pod 配置的 <code>nodeSelector</code>，需要确认一下是 Pod 配置 <code>nodeSelector</code> 键值是否合理，还是节点配置了错误的标签键值。</p><h2 id="污点与容忍配置类"><a href="#污点与容忍配置类" class="headerlink" title="污点与容忍配置类"></a>污点与容忍配置类</h2><h3 id="节点污点导致无法调度"><a href="#节点污点导致无法调度" class="headerlink" title="节点污点导致无法调度"></a>节点污点导致无法调度</h3><h4 id="故障现象-4"><a href="#故障现象-4" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code> had taint, that the pod didn&#39;t tolerate.</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age   From               Message</span><br><span class="line">  ----     ------            ----  ----               -------</span><br><span class="line">  Warning  FailedScheduling  99m   default-scheduler  0/2 nodes are available: 2 node(s) had taint, that the pod didn&#x27;t tolerate.</span><br></pre></td></tr></table></figure><h4 id="解决思路-4"><a href="#解决思路-4" class="headerlink" title="解决思路"></a>解决思路</h4><p>  该问题出现在集群内可用节点都存在污点的情况下，在不做出任何修改的情况下，控制节点一般会打 <code>node-role.kubernetes.io/master</code> 的污点，避免业务调度到控制节点。而计算节点一般会运行业务，无特殊需求的情况下，默认是不对节点进行污点配置的。如果计算节点存在污点，很有可能是因为节点满足了某个条件，导致其被控制器打上了污点。此时 Pod </p><p>  通常情况下我们是不需要主动为 Pod 添加容忍配置，我们需要去解决为什么节点被打上污点的问题。与污点容忍配置相关细节可参考<a href="https://gdc.lexiangla.com/teams/k100044/docs/cf110fa8808611ebb2636263495fb44b?company_from=gdc">《 kubernetes 调度》</a> 中 <code>污点与容忍</code> 相关的章节。</p><p>  常见的系统内置污点可参考如下：</p><ul><li><code>node.kubernetes.io/not-ready</code>：节点未准备好。这相当于节点状态 <code>Ready</code> 的值为 “<code>False</code>“。</li><li><code>node.kubernetes.io/unreachable</code>：节点控制器访问不到节点. 这相当于节点状态 <code>Ready</code> 的值为 “<code>Unknown</code>“。</li><li><code>node.kubernetes.io/memory-pressure</code>：节点存在内存压力。</li><li><code>node.kubernetes.io/disk-pressure</code>：节点存在磁盘压力。</li><li><code>node.kubernetes.io/pid-pressure</code>: 节点的 PID 压力。</li><li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li><li><code>node.kubernetes.io/unschedulable</code>: 节点不可调度。</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个 “外部” 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li></ul><h4 id="处理步骤-4"><a href="#处理步骤-4" class="headerlink" title="处理步骤"></a>处理步骤</h4><p><strong>1. 确认集群内各个节点的污点配置</strong></p><p>  通过 <code>kubectl describe node node名称 </code> 可以查询污点的配置，查询关键字段是 spec 下的 taints</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-1</span> <span class="string">~</span>]<span class="comment"># kubectl describe node k8s-1</span></span><br><span class="line"><span class="attr">Name:</span>               <span class="string">k8s-1</span></span><br><span class="line"><span class="attr">Roles:</span>              <span class="string">control-plane,master</span></span><br><span class="line"><span class="attr">Taints:</span>             <span class="string">node-role.kubernetes.io/master:NoSchedule</span>  <span class="comment"># Taints 这个字段代表污点</span></span><br><span class="line"><span class="attr">Unschedulable:</span>      <span class="literal">false</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>  也可以通如下 for 循环直接获取所有节点的污点配置，参考如下执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取所有节点的污点配置</span></span><br><span class="line">for node in `kubectl get nodes --no-headers | awk &#x27;&#123; print $1 &#125;&#x27;`; do echo &quot;==$node==&quot; &amp;&amp;  kubectl get nodes $node -o jsonpath=&#x27;&#123;.spec.taints&#125;&#x27; &amp;&amp; echo -e &quot;\n&quot; ; done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例输出如下</span></span><br><span class="line">[root@k8s-1 ~]# for node in `kubectl get nodes --no-headers | awk &#x27;&#123; print $1 &#125;&#x27;`; do echo &quot;==$node==&quot; &amp;&amp;  kubectl get nodes $node -o jsonpath=&#x27;&#123;.spec.taints&#125;&#x27; &amp;&amp; echo -e &quot;\n&quot; ; done</span><br><span class="line">==k8s-1==</span><br><span class="line">[&#123;&quot;effect&quot;:&quot;NoSchedule&quot;,&quot;key&quot;:&quot;node-role.kubernetes.io/master&quot;&#125;]</span><br><span class="line"></span><br><span class="line">==k8s-2==</span><br><span class="line">[&#123;&quot;effect&quot;:&quot;NoSchedule&quot;,&quot;key&quot;:&quot;node.kubernetes.io/disk-pressure&quot;,&quot;timeAdded&quot;:&quot;2021-09-17T06:59:14Z&quot;&#125;]</span><br></pre></td></tr></table></figure><p><strong>2. 解决污点问题</strong></p><p>  通过步骤 1 获取到污点的 key 以后，再参考解决思路中的 <strong>常见的系统内置污点</strong> 进行处理。可行的处理方法可参考<a href="https://gdc.lexiangla.com/teams/k100044/docs/842da35cae3511ebb4e66afd0c8a692a?company_from=gdc">《节点 NotReady 与驱逐的处理方法》</a>中的有关驱逐部分的章节，以及<a href="https://iwiki.woa.com/pages/viewpage.action?pageId=1042992399">《集群产生 Evicted 状态 Pod》</a>的定位思路。</p><h2 id="存储类"><a href="#存储类" class="headerlink" title="存储类"></a>存储类</h2><h3 id="卷节点亲和性冲突"><a href="#卷节点亲和性冲突" class="headerlink" title="卷节点亲和性冲突"></a>卷节点亲和性冲突</h3><h4 id="故障现象-5"><a href="#故障现象-5" class="headerlink" title="故障现象"></a>故障现象</h4><p>  通过 <code>kubectl describe pod</code> 获取到的错误信息如下，关键字是 <code>had volume node affinity conflict</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason            Age              From               Message</span><br><span class="line">  ----     ------            ----             ----               -------</span><br><span class="line">  Warning  FailedScheduling  5s (x2 over 5s)  default-scheduler  0/4 nodes are available: 1 node(s) had volume node affinity conflict, 3 node(s) didn&#x27;t match node selector.</span><br></pre></td></tr></table></figure><h4 id="解决思路-5"><a href="#解决思路-5" class="headerlink" title="解决思路"></a>解决思路</h4><p>  这类错误主要发生在 local pv 的场景。当 PV 指定了 <code>local</code> 参数，使用宿主机的本地目录时，此时 PV 需要配置 <code>nodeAffinity</code> 的配置，将 PV 固定创建在特定的节点上。如果 Pod 需要引用这个 PV 作为存储卷使用，则 Pod 需要运行到 PV 绑定的节点上。如果该 PV 绑定的节点故障了，则 Pod 将无法调度到其他节点上。此时创建 Pod 时调度器将输出 <code>had volume node affinity conflict</code>  的信息。</p><p>  确认问题的思路是，先查看 Pod YAML，确认挂载 volumes 中的 PVC，然后再去查询该 PVC 对应的 PV 是固定调度到哪个机器上。再确认该节点的可用性。</p><h4 id="处理步骤-5"><a href="#处理步骤-5" class="headerlink" title="处理步骤"></a>处理步骤</h4><p><strong>1. 确认当前 Pod 的 volumes 配置</strong></p><p>  通过 <code>kubectl get pod -n 命名空间 Pod名称 -o yaml </code> 可以查询 Pod 的配置，查询关键字段是 spec 下的 volumes</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/echo</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">hello”,”world</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">kubernetes.io/hostname:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.57</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span>    <span class="comment"># 代表使用了 PVC</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">pvctest</span>      <span class="comment"># PVC 的 名称</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><h5 id="2-确认当前-Pod-挂载的-PVC-配置"><a href="#2-确认当前-Pod-挂载的-PVC-配置" class="headerlink" title="2. 确认当前 Pod 挂载的 PVC 配置"></a>2. 确认当前 Pod 挂载的 PVC 配置</h5><p>  通过 <code>kubectl describe pvc -n 命名空间 PVC名称</code> 确认 PVC 对应的 PV</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Name:</span>          <span class="string">pvctest</span></span><br><span class="line"><span class="attr">Namespace:</span>     <span class="string">default</span></span><br><span class="line"><span class="attr">StorageClass:</span></span><br><span class="line"><span class="attr">Status:</span>        <span class="string">Bound</span>    <span class="comment"># PVC 为 Bound 状态，代表它关联上了 PV</span></span><br><span class="line"><span class="attr">Volume:</span>        <span class="string">pvtest</span>   <span class="comment"># PV 名称，通过查询 PV 确认配置</span></span><br><span class="line"><span class="attr">Labels:</span>        <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Annotations:   cpaas.io/creator:</span> <span class="string">kubernetes-admin</span></span><br><span class="line">               <span class="attr">cpaas.io/updated-at:</span> <span class="number">2021-09-16T07:10:49Z</span></span><br><span class="line">               <span class="attr">pv.kubernetes.io/bind-completed:</span> <span class="literal">yes</span></span><br><span class="line"><span class="attr">Finalizers:</span>    [<span class="string">kubernetes.io/pvc-protection</span>]</span><br><span class="line"><span class="attr">Capacity:</span>      <span class="string">5Gi</span></span><br><span class="line"><span class="attr">Access Modes:</span>  <span class="string">RWO</span></span><br><span class="line"><span class="attr">VolumeMode:</span>    <span class="string">Filesystem</span></span><br><span class="line"><span class="attr">Mounted By:</span>    <span class="string">hello-world</span></span><br><span class="line"><span class="attr">Events:</span>        <span class="string">&lt;none&gt;</span></span><br></pre></td></tr></table></figure><h5 id="3-确认-PVC-关联的-PV-配置"><a href="#3-确认-PVC-关联的-PV-配置" class="headerlink" title="3.确认 PVC 关联的 PV 配置"></a>3.确认 PVC 关联的 PV 配置</h5><p>  通过 <code>kubectl get pv PV名称 -o yaml</code> 确认 PV 的配置，关键字段是 <code>nodeAffinity</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">claimRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">pvctest</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/root/test</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span>  <span class="comment"># 使用 local 参数场景下必须指定该参数，这里的条件相当于是 PV 固定调度到 192.168.0.2 这个机器上</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure><p>  通过这三步确认出 PV 固定调度到哪些节点上。再通过<code> kubectl get nodes --show-labels | grep 标签键=值</code> 去确认该节点的状态是否正常。如果节点处于 NotReady 状态，可以根据<a href="https://gdc.lexiangla.com/teams/k100044/docs/842da35cae3511ebb4e66afd0c8a692a?company_from=gdc">《节点 NotReady 处理》</a>来进行问题排查。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;现象描述&quot;&gt;&lt;a href=&quot;#现象描述&quot; class=&quot;headerlink&quot; title=&quot;现象描述&quot;&gt;&lt;/a&gt;现象描述&lt;/h1&gt;&lt;p&gt;  如果 Pod 处于 Pending 状态，这意味着当前集群不存在满足 Pod 运行条件的节点，需要通过&lt;code&gt; ku</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>Pod 状态 Pending 处理思路</title>
    <link href="https://guoltan.github.io/2022/01/20/prometheus%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%BF%B0/"/>
    <id>https://guoltan.github.io/2022/01/20/prometheus%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%BF%B0/</id>
    <published>2022-01-20T10:00:15.000Z</published>
    <updated>2024-03-26T01:31:25.051Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prometheus-是什么"><a href="#Prometheus-是什么" class="headerlink" title="Prometheus 是什么?"></a>Prometheus 是什么?</h1><p>  Prometheus 是一个开源的系统监控和告警工具，用于收集、存储和查询时间序列数据，并提供灵活的告警机制。它采用拉取方式从目标系统中收集指标数据，支持多维度的标签和时间戳。Prometheus 提供了强大的查询语言PromQL，可用于灵活地查询和分析时间序列数据。</p><h1 id="Prometheus-主要特点"><a href="#Prometheus-主要特点" class="headerlink" title="Prometheus 主要特点"></a>Prometheus 主要特点</h1><ul><li>多维数据模型：使用指标名称和键值对来标识时间序列数据。</li><li>提供 PromQL，一种灵活的查询语言，可以利用这种多维特性进行查询。</li><li>不依赖分布式存储：单个服务器节点具有自治性。</li><li>通过HTTP的 Pull 模型进行时间序列数据的收集。</li><li>支持通过中间网关进行时间序列数据的推送。</li><li>支持动态服务发现和静态配置抓取目标。</li><li>支持多种图形和仪表盘模式。</li></ul><h2 id="Prometheus-组件"><a href="#Prometheus-组件" class="headerlink" title="Prometheus 组件"></a>Prometheus 组件</h2><p>Prometheus生态系统由多个组件组成，其中许多是可选的，如下：</p><ul><li>Prometheus Server，用于抓取和存储时间序列数据</li><li>用于为应用程序代码进行仪表化的客户端库</li><li>用于支持短期作业（如 K8S 的 job）的 pushgateway</li><li>用于特定目的的 exporters，例如：HAProxy、StatsD、Graphite 等</li><li>用于处理警报的 alertmanager</li><li>其他支持工具</li></ul><p>Prometheus 架构图（来源自官网）</p><p><img src="https://prometheus.io/assets/architecture.png" alt="Prometheus architecture"></p><p>什么场景适合用 Prometheus？</p><ul><li><p>云原生场景</p></li><li><p>采集的目标数据不需要长期存储，比如业务运行状态，用户更多可能关注的是近期内应用的状态，长期的数据可以做历史归档。不需要精细的存储监控指标。</p></li></ul><p>什么场景不适合用 Prometheus？</p><ul><li><p>需要 100% 准确性的监控采集场景，如服务请求的计费数据不适用 Prometheus。Prometheus 更多是实现的趋势精准。</p></li><li><p>简单的系统应用场景，或者说传统的一些监控场景，更适合使用 zabbix 类的监控组件去实现。</p></li><li><p>Prometheus 不用于日志存储的场景，日志存储应该使用 loki、elk。也不应用于链路追踪的场景。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Prometheus-是什么&quot;&gt;&lt;a href=&quot;#Prometheus-是什么&quot; class=&quot;headerlink&quot; title=&quot;Prometheus 是什么?&quot;&gt;&lt;/a&gt;Prometheus 是什么?&lt;/h1&gt;&lt;p&gt;  Prometheus 是一个开源的系</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>Pod 状态 Evicted 处理思路</title>
    <link href="https://guoltan.github.io/2022/01/17/Evicted/"/>
    <id>https://guoltan.github.io/2022/01/17/Evicted/</id>
    <published>2022-01-17T10:00:15.000Z</published>
    <updated>2024-03-25T15:09:02.127Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-现象描述"><a href="#1-现象描述" class="headerlink" title="1. 现象描述"></a><strong>1. 现象描述</strong></h1><p>如果 Pod 处于 Evicted 状态，这意味着当前集群有部分节点存在压力，可能这些节点是内存、磁盘、PID等资源不足，此时需要通过kubectl describe pod名称 -n 命名空间 获取该 Pod 相关的 Events。基于 Events 给出的信息来处理。实例输出如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@tke-192-168-0-2 ~]# kubectl describe pod -n cpaas-system warlock-84fdf6cd9b-vkgkz</span><br><span class="line">Name:                 warlock-84fdf6cd9b-vkgkz</span><br><span class="line">Namespace:            cpaas-system</span><br><span class="line">Priority:             1000000</span><br><span class="line">Priority Class Name:  high-priority</span><br><span class="line">Node:                 192.168.0.25</span><br><span class="line">Start Time:           Wed, 03 Nov 2021 09:36:35 +0800</span><br><span class="line">Labels:               app=warlock</span><br><span class="line">                      chart=cpaas-monitor</span><br><span class="line">                      pod-template-hash=84fdf6cd9b</span><br><span class="line">                      service_name=warlock</span><br><span class="line">                      version=v1</span><br><span class="line">Annotations:          ovn.kubernetes.io/allocated: true</span><br><span class="line">                      ovn.kubernetes.io/cidr: 10.199.0.0/16</span><br><span class="line">                      ovn.kubernetes.io/gateway: 10.199.0.1</span><br><span class="line">                      ovn.kubernetes.io/ip_address: 10.199.15.74</span><br><span class="line">                      ovn.kubernetes.io/logical_switch: ovn-default</span><br><span class="line">                      ovn.kubernetes.io/mac_address: 00:00:00:CD:F8:38</span><br><span class="line">                      ovn.kubernetes.io/network_types: geneve</span><br><span class="line">                      ovn.kubernetes.io/routed: true</span><br><span class="line">Status:               Failed</span><br><span class="line">Reason:               Evicted</span><br><span class="line">Message:              Pod The node had condition: [DiskPressure]. # 对于 Evicted 的 Pod，重点关注 Message 字段的内容，通过该字段判断原因。从示例来看，问题出节点触发了 DiskPressure。</span><br><span class="line">IP:</span><br><span class="line">IPs:                  </span><br><span class="line">Controlled By:        ReplicaSet/warlock-84fdf6cd9b</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h1 id="2-排查思路"><a href="#2-排查思路" class="headerlink" title="2. 排查思路"></a><strong>2. 排查思路</strong></h1><h2 id="2-1-节点资源类"><a href="#2-1-节点资源类" class="headerlink" title="2.1. 节点资源类"></a><strong>2.1. 节点资源类</strong></h2><h3 id="2-1-1-磁盘空间不足"><a href="#2-1-1-磁盘空间不足" class="headerlink" title="2.1.1. 磁盘空间不足"></a><strong>2.1.1. 磁盘空间不足</strong></h3><h4 id="2-1-1-1-故障现象"><a href="#2-1-1-1-故障现象" class="headerlink" title="2.1.1.1. 故障现象"></a><strong>2.1.1.1. 故障现象</strong></h4><p>通过 kubectl describe pod 获取到的错误信息如下，关键字段在 Message 处，关键字段 The node was low on resource</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例输出</span></span><br><span class="line">[root@k8s-1 ~]<span class="comment"># kubectl describe pod nginx-6db6848cf4-gr46n</span></span><br><span class="line">....</span><br><span class="line">Status:         Failed</span><br><span class="line">Reason:         Evicted</span><br><span class="line">Message:        The node was low on resource: ephemeral-storage. Container nginx was using 5594, <span class="built_in">which</span> exceeds its request of 0.</span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age    From               Message</span><br><span class="line">  ----     ------            ----   ----               -------</span><br><span class="line">  Normal   Scheduled         2m40s  default-scheduler  Successfully assigned default/nginx-6db6848cf4-gr46n to k8s-2</span><br><span class="line">  Normal   Pulling           2m38s  kubelet            Pulling image <span class="string">&quot;daocloud.io/nginx&quot;</span></span><br><span class="line">  Normal   Pulled            2m38s  kubelet            Successfully pulled image <span class="string">&quot;daocloud.io/nginx&quot;</span> <span class="keyword">in</span> 572.503087ms</span><br><span class="line">  Normal   Created           2m38s  kubelet            Created container nginx</span><br><span class="line">  Normal   Started           2m38s  kubelet            Started container nginx</span><br><span class="line">  Warning  Evicted           60s    kubelet            The node was low on resource: ephemeral-storage. Container nginx was using 5594, <span class="built_in">which</span> exceeds its request of 0.</span><br><span class="line">  Normal   Killing           60s    kubelet            Stopping container nginx</span><br></pre></td></tr></table></figure><h4 id="2-1-1-2-处理思路"><a href="#2-1-1-2-处理思路" class="headerlink" title="2.1.1.2. 处理思路"></a><strong>2.1.1.2. 处理思路</strong></h4><p>从 low on resource 反馈的资源类型来判断是内存不足、磁盘空间不足还是 PID 不足。从现网处理的案例来看，基本上都是磁盘不足引起的问题。对于磁盘空间不足的场景，需要登录到异常节点上，通过 dh 查询各个目录的使用空间，定位到占用容量的文件，并通过释放或者扩容的手段处理。</p><p>磁盘压力分为 <strong>根文件系统</strong> 和 <strong>映像文件系统</strong> 这两种，映像文件系统指的是 docker 或者其他 CRI 的数据盘。对于 TKE 来说通常会单独给 &#x2F;var&#x2F;lib&#x2F;docker 单独挂盘，TCNP 会给 &#x2F;data 单独挂盘。</p><p>默认情况下，<strong>根分区文件系统的使用容量达到 90%</strong> 或者 inodes 达到 95% 就会触发驱逐，并自动打上污点。<strong>映像文件系统则是使用容量达到 85%</strong> 就会触发驱逐并自动打上污点。</p><p>从现网的处理经验来看，造成磁盘容量使用率高的原因，通常原因如下：</p><ul><li><p>产品出包的配置误将日志输出到根分区，常出现在 TI 产品，当我们追查到空间使用率来源于组件日志时，需要拉起后端同学确认，擦看是否可以先清理掉过期的日志，再整改日志存放路径。</p></li><li><p>相关同学在初始化节点时操作不正确，未正确挂载数据盘。导致后续数据全部写入到了根分区，造成根分区磁盘压力大。</p></li><li><p>相关同学在初期部署时将所有物料到根分区，造成磁盘空间不足。</p></li><li><p>初期交付时磁盘空间规划较小。该类场景比较容易导致磁盘爆满。</p></li></ul><h4 id="2-1-1-3-处理步骤"><a href="#2-1-1-3-处理步骤" class="headerlink" title="2.1.1.3. 处理步骤"></a><strong>2.1.1.3. 处理步骤</strong></h4><p><strong>1、登录到故障节点</strong></p><p><code>通过 ssh 用户名@IP 进行连接，细节略</code></p><p><strong>2、查询磁盘使用率</strong></p><p><code>通过 df | grep -v -E &quot;overlay|shm|tmpfs&quot; 可以获取节点当前磁盘使用的情况。</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例输出，这个环境不存在 DiskPressure</span></span><br><span class="line">[root@tke-192-168-0-2 ~]<span class="comment"># df | grep -v -E &quot;overlay|shm|tmpfs&quot;</span></span><br><span class="line">Filesystem                 1K-blocks      Used Available Use% Mounted on</span><br><span class="line">/dev/vda1                  154685884  83911048  64383392  57% /          <span class="comment"># 如果根分区超过 90% 就会触发驱逐</span></span><br><span class="line">/dev/mapper/vgdata-lv_data 524027908 409582764 114445144  79% /data/platform</span><br><span class="line">ceph-fuse                  775245824 416186368 359059456  54% /data/platform-fs</span><br><span class="line">通过 <span class="built_in">df</span> -i | grep -v -E <span class="string">&quot;overlay|shm|tmpfs&quot;</span> 可以获取节点当前磁盘 inodes 使用的情况。</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输出，注：这个环境不存在 DiskPressure</span></span><br><span class="line">[root@tke-192-168-0-2 ~]<span class="comment"># df -i | grep -v -E &quot;overlay|shm|tmpfs&quot;</span></span><br><span class="line">Filesystem                    Inodes   IUsed     IFree IUse% Mounted on</span><br><span class="line">/dev/vda1                    9830400  313679   9516721    4% /</span><br><span class="line">/dev/mapper/vgdata-lv_data 231136976 2412258 228724718    2% /data/platform</span><br><span class="line">ceph-fuse                      33487   33487         0  100% /data/platform-fs</span><br></pre></td></tr></table></figure><p><strong>3、定位占用磁盘空间的文件</strong></p><p><strong>a. 确认各个目录的空间占用率</strong></p><p>通过 du -h –max-depth&#x3D;1 &#x2F; 可以获取节点当前磁盘使用的情况。通过这个方法我们可以追查磁盘使用率是来源于哪些目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例输出</span></span><br><span class="line">[root@k8s ~]<span class="comment"># du -h --max-depth=1 /</span></span><br><span class="line">118M    /boot</span><br><span class="line">0       /dev</span><br><span class="line"><span class="built_in">du</span>: cannot access ‘/proc/5891’: No such file or directory</span><br><span class="line"><span class="built_in">du</span>: cannot access ‘/proc/5944/task/5944/fd/3’: No such file or directory</span><br><span class="line"><span class="built_in">du</span>: cannot access ‘/proc/5944/task/5944/fdinfo/3’: No such file or directory</span><br><span class="line"><span class="built_in">du</span>: cannot access ‘/proc/5944/fd/4’: No such file or directory</span><br><span class="line"><span class="built_in">du</span>: cannot access ‘/proc/5944/fdinfo/4’: No such file or directory</span><br><span class="line">0       /proc</span><br><span class="line">794M    /run</span><br><span class="line">0       /sys</span><br><span class="line">36M     /etc</span><br><span class="line">29G     /root  <span class="comment"># 空间使用率较高，从此处可以看到，根分区的空间基本上是被 /root 使用掉了。</span></span><br><span class="line">754M    /var</span><br><span class="line">4.0K    /tmp</span><br><span class="line">1.5G    /usr</span><br><span class="line">0       /home</span><br><span class="line">0       /media</span><br><span class="line">0       /mnt</span><br><span class="line">88M     /opt</span><br><span class="line">0       /srv</span><br><span class="line">33G     /</span><br></pre></td></tr></table></figure><p>通过 du 追查到一级目录以后，再一步一步的追查，示例中是 &#x2F;root 使用空间较大，后续再通过 du -h –max-depth&#x3D;1 &#x2F;root 进一步的获取空间使用情况。循序渐进找到源头。</p><p><strong>b. 确认是否存在未释放 Evicted Pod</strong></p><p>在确认节点资源问题已解决后，各个业务 Pod 应该重新调度。此时需要将这些 Evicted 的 Pod 释放掉。执行如下指令完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生产不要直接复制~~~~</span></span><br><span class="line"><span class="keyword">for</span> ns <span class="keyword">in</span> `kubectl get ns |awk <span class="string">&#x27;NR&gt;1&#123;print $1&#125;&#x27;</span>` ;<span class="keyword">do</span> kubectl get pod -n &#123;ns&#125; |grep Evicted |awk <span class="string">&#x27;&#123;print 1&#125;&#x27;</span>|xargs kubectl delete pod -n <span class="variable">$&#123;ns&#125;</span> ;<span class="keyword">done</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-现象描述&quot;&gt;&lt;a href=&quot;#1-现象描述&quot; class=&quot;headerlink&quot; title=&quot;1. 现象描述&quot;&gt;&lt;/a&gt;&lt;strong&gt;1. 现象描述&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;如果 Pod 处于 Evicted 状态，这意味着当前集群有部分节点存</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 调度和驱逐</title>
    <link href="https://guoltan.github.io/2022/01/05/kubernetes%20%E8%B0%83%E5%BA%A6%E6%8E%A7%E5%88%B6%E5%9C%BA%E6%99%AF%E4%B8%8E%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/"/>
    <id>https://guoltan.github.io/2022/01/05/kubernetes%20%E8%B0%83%E5%BA%A6%E6%8E%A7%E5%88%B6%E5%9C%BA%E6%99%AF%E4%B8%8E%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/</id>
    <published>2022-01-05T10:00:15.000Z</published>
    <updated>2024-03-25T15:56:51.420Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kubernetes的调度和驱逐"><a href="#kubernetes的调度和驱逐" class="headerlink" title="kubernetes的调度和驱逐"></a>kubernetes的调度和驱逐</h1><h2 id="kubernetes的调度"><a href="#kubernetes的调度" class="headerlink" title="kubernetes的调度"></a>kubernetes的调度</h2><p>  我们在节点上创建Pod的时候，实际上是通过 kube-scheduler 进行了一系列的调度打分等动作才决定了Pod能够运行在哪个节点上。首先kube-scheduler会先进行过滤。Pod本身可能具备一些需求。要求会运行到特定的节点上。此时通过过滤器可以筛选出可调度的节点。在可调度节点的基础上。kube-scheduler 再根据内置的函数对节点进行打分。通过打分以后。选取最合适的节点。将该节点通知给 kube-apiserver。让 kube-apiserver 知道后续创建Pod到哪台节点上。</p><p>  有关kubernetes默认支持的调度函数，打分函数可以参考链接。</p><p><a href="https://kubernetes.io/zh/docs/reference/scheduling/policies/">https://kubernetes.io/zh/docs/reference/scheduling/policies/</a></p><h2 id="污点与容忍"><a href="#污点与容忍" class="headerlink" title="污点与容忍"></a>污点与容忍</h2><h3 id="什么是污点？"><a href="#什么是污点？" class="headerlink" title="什么是污点？"></a><strong>什么是污点？</strong></h3><p>  通过给节点添加污点，可以阻止Pod创建在带有污点的节点上。管理员可以通过合理的设置污点。来控制业务Pod的运行。如：管理员不希望业务Pod运行在master节点上。这种需要就可以通过在master节点上添加污点来实现。事实上kubernetes默认也为master添加上了 <code>node-role.kubernetes.io/master:NoSchedule</code> 的污点。避免业务Pod直接运行在master节点上。</p><p>  通过污点我们可以阻止Pod运行到带污点的节点上,如果希望Pod能运行到这些带污点的节点上则需要给Pod添加容忍配置。通过设置容忍能够让Pod支持调度到带有污点的节点。</p><h3 id="污点的应用场景"><a href="#污点的应用场景" class="headerlink" title="污点的应用场景"></a><strong>污点的应用场景</strong></h3><ul><li>通过污点规划专用节点，避免Pod被直接调度到该类节点上。比方说带有GPU设备的节点。可以打上一个特定污点。只要未设置该类污点容忍配置的Pod都无法被调度到这类GPU节点上。只有需要使用GPU资源的Pod配置了容忍才能被调度到。通过这种方法可以实现Pod调度控制。</li><li>基于污点来实现驱逐Pod，可以通过给节点打上污点，让不支持该污点的Pod被驱逐到其他节点上。</li></ul><h3 id="污点的配置支持下面三种类型"><a href="#污点的配置支持下面三种类型" class="headerlink" title="污点的配置支持下面三种类型"></a><strong>污点的配置支持下面三种类型</strong></h3><ul><li>NoSchedule 表示如果pod没有容忍这些污点，pod则法被调度到包含该污点的节点上。</li><li>PreferNoSchedule 相当于NoSchedule宽松的版本，在调度时会尽量阻止pod调度到具备PreferNoschedule 污点的节点上，但在没有其他节点可以提供调度的场景下。pod则可以被调度PreferNoschedule 污点的节点上。</li><li>NoExecute用于影响正在节点上运行的pod。如果当前节点上运行的pod没有容忍该节点上的NoExecute污点，则会Pod将会从该节点上去除。</li></ul><blockquote><p>上述的三种污点类型中，其中NoSchedule和PreferNoSchedule是用于控制Pod调度过程时的动作。而NoExecute则是影响调度后正在运行Pod的节点。</p></blockquote><h3 id="污点的配置示例"><a href="#污点的配置示例" class="headerlink" title="污点的配置示例"></a><strong>污点的配置示例</strong></h3><h4 id="添加污点示例"><a href="#添加污点示例" class="headerlink" title="添加污点示例"></a><strong>添加污点示例</strong></h4><p><code>kubectl taint node 主机名 key=value:污点类型 </code></p><blockquote><p> 在添加taint的时候，value字段是可以省略的。</p><p> 可以支持基于label来进行污点的添加删除，添加-l key&#x3D;value 选项即可实现</p><p> 污点支持的详细参数，可通过 <code>kubectl explain node.spec.taints</code> 查询。</p></blockquote><h4 id="移除污点示例"><a href="#移除污点示例" class="headerlink" title="移除污点示例"></a><strong>移除污点示例</strong></h4><p><code>kubectl taint node 主机名 key:污点类型-</code></p><blockquote><p>在移除污点时，key后面可以省略污点类型。这样做就会删除所有指定的key值污点。</p></blockquote><h4 id="污点作用测试"><a href="#污点作用测试" class="headerlink" title="污点作用测试"></a><strong>污点作用测试</strong></h4><p>当我们给节点添加上污点以后，可以怎么样去测试是否生效？这时候我们通过给Pod添加 <code>NodeSelector</code> 的配置，将Pod绑定到带污点的节点上。</p><p>添加 <code>NodeSelector</code> 的Pod配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">nodeSelector:</span>   <span class="comment">#通过nodeSelector，将Pod调度到带有设置的key：value标签节点。此处标签为node=master</span></span><br><span class="line">    <span class="attr">node:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><p>查看调度结果，此时会打印出 <code>node(s) had taint</code>的事件。遇到这类的提示，可以确认一下Pod是否配置容忍，容忍的配置是否匹配环境等。</p><p>![image-20201116155117361](D:\My\学习\6. 笔记\3. 微服务\k8s\note_picture\调度.asserts\image-20201116155117361.png)</p><h4 id="污点配置案例"><a href="#污点配置案例" class="headerlink" title="污点配置案例"></a><strong>污点配置案例</strong></h4><p><strong>1.给k8s-node01节点添加上污点，使得创建Pod时，无法调度到该节点上。</strong></p><p><code>kubectl taint node k8s-node01 node-type=production:NoSchedule</code></p><p><strong>2.给具备 node-type&#x3D;gray 标签的节点添加污点，使得Pod创建时，不优先调度到这类节点上。</strong></p><p><code>kubectl taint node -l node-type=gray node-type=gray:PreferNoSchedule</code></p><p><strong>3.移除k8s-node01节点上key值为 node-type 的污点</strong></p><p><code>kubectl taint node k8s-node01 node-type</code></p><p><strong>4.为k8s-node01添加污点，直接驱逐未容忍该污点的Pod。</strong></p><p><code>kubectl taint node k8s-node01 node-type=drain:NoExecute</code></p><h3 id="容忍的配置示例"><a href="#容忍的配置示例" class="headerlink" title="容忍的配置示例"></a><strong>容忍的配置示例</strong></h3><p>tolerations支持如下配置</p><table><thead><tr><th>选项</th><th>说明</th></tr></thead><tbody><tr><td>effect</td><td>effect可以填写污点的类型（NoSchedule，NoExecute等），当effect为空时，代表匹配key名称的所有污点类型。</td></tr><tr><td>key</td><td>指定容忍的污点key字段，当key值为空的场景（此时operator必须设置为Exists），则代表它匹配所有污点的key。</td></tr><tr><td>value</td><td>指定容忍的污点value字段，当operator设置为Exists的场景。此字段必须为空。</td></tr><tr><td>operator</td><td>支持Equal以及Exists两个参数。默认使用Equal，使用Exists时，只需要配置key字段，相当于key：*的匹配形式。使用Equal需要同时指定key和value值</td></tr><tr><td>tolerationSeconds</td><td>该字段作用于<code>NoExecute</code>，当节点打上了污点类型为<code>NoExecute</code>时，会驱逐没有容忍<code>NoExecute</code>的Pod。如果Pod指定了容忍<code>NoExecute</code>类型的同时，指定了<code>tolerationSeconds</code>选项。则Pod会等待<code>tolerationSeconds</code>设置的时间。再进行Pod驱逐。</td></tr></tbody></table><blockquote><p>通过如下命令可以查询tolerations可以支持的选项，以及选项的注释</p><p><code>kubectl explain deployment.spec.template.spec.tolerations</code></p></blockquote><h4 id="容忍配置案例"><a href="#容忍配置案例" class="headerlink" title="容忍配置案例"></a><strong>容忍配置案例</strong></h4><p><strong>1.添加容忍配置，允许Pod调度到节点标签为 node-type&#x3D;gray ，并且污点类型为 NoSchedule 的机器。</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoSchedule</span>        <span class="comment">#设置污点类型为NoSchedule</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">&quot;node-type&quot;</span>        <span class="comment">#key值为node-type</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Equal</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;gray&quot;</span>            <span class="comment">#value值为Equal</span></span><br></pre></td></tr></table></figure><p><strong>2.添加容忍配置，允许Pod调度到任何打 上NoSchedule 污点类型的节点。</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoSchedule</span>        <span class="comment">#设置污点类型为NoSchedule</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">&quot;&quot;</span>                    <span class="comment">#当key值为空，operator为Exists时，匹配任意key值</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Exists</span></span><br></pre></td></tr></table></figure><p><strong>3.添加容忍配置，允许Pod运行在具备 node-type&#x3D;gray 且污点类型为任意类型的节点上。</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;&quot;</span>                <span class="comment">#设置污点类型为空，代表它支持任意污点类型</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">&quot;node-type&quot;</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Equal</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;gray&quot;</span></span><br></pre></td></tr></table></figure><p><strong>4.添加容忍配置，允许Pod运行在具有 node-type&#x3D;drain 且污点类型为 NoExecute 的节点上。</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span>            <span class="comment">#设置污点类型为NoExecute，未容忍该污点的Pod直接被驱逐</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">&quot;node-type&quot;</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Equal</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;drain&quot;</span></span><br></pre></td></tr></table></figure><p><strong>5.添加容忍配置，允许Pod运行在具有 node-type&#x3D;drain 且污点类型为 NoExecute 的节点上，当300秒以后该污点依旧存在时，驱逐该Pod到其他节点。</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span>            <span class="comment">#设置污点类型为NoExecute，未容忍该污点的Pod直接被驱逐</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">&quot;node-type&quot;</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Equal</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;drain&quot;</span></span><br><span class="line">    <span class="attr">tolerationSeconds:</span> <span class="string">&quot;300&quot;</span>    <span class="comment">#设置容忍时间为300秒，当Pod所在的节点打上了node-type=drain:NoExecute的污点以后。该Pod会在该节点上继续运行300秒。300秒结束后污点依旧存在的话，驱逐该Pod。</span></span><br></pre></td></tr></table></figure><p>节点上是可以添加多个污点的，支持设置不同的污点类型。Pod同时也可以设置多个容忍。在这种场景下 kube-schedule 进行调度匹配时会以过滤器的形式来进行工作。相当于用污点减去容忍，结果为节点污点与Pod容忍之间的差集，再根据污点的差集结果来进行调度。</p><ul><li>经过滤以后，节点至少有一个 <code>effect</code> 为  <code>NoSchedule</code>  的污点时，Pod不调度到该类节点上。</li><li>经过滤以后，节点至少有一个<code>effect</code>为 <code>PreferNoSchedule</code> 的污点时，Pod可能调度到该类节点上。</li><li>经过滤以后，节点至少有一个 <code>effect </code>为 <code>NoExecute</code> 的污点时，Pod不调度到该类节点上。如果Pod存在于该节点，则进行驱逐。</li></ul><p><strong>关于tolerationSeconds的应用场景</strong></p><p>  为了维持集群的稳定性，当节点达到某种状态时，节点控制器会自动给节点添加污点，当前内置的污点类型如下：</p><table><thead><tr><th>污点key</th><th>说明</th></tr></thead><tbody><tr><td>node.kubernetes.io&#x2F;not-ready</td><td>节点状态未处于 <code>Ready</code></td></tr><tr><td>node.kubernetes.io&#x2F;unreachable</td><td>节点控制器访问不到节点. 这相当于节点状态  <code>Ready</code>  的值为 <code>Unknown</code></td></tr><tr><td>node.kubernetes.io&#x2F;out-of-disk</td><td>节点磁盘资源耗尽</td></tr><tr><td>node.kubernetes.io&#x2F;memory-pressure</td><td>节点存在内存压力</td></tr><tr><td>node.kubernetes.io&#x2F;disk-pressure</td><td>节点存在磁盘压力</td></tr><tr><td>node.kubernetes.io&#x2F;network-unavailable</td><td>节点网络不可达</td></tr><tr><td>node.kubernetes.io&#x2F;unschedulable</td><td>节点不可被调度</td></tr><tr><td>node.cloudprovider.kubernetes.io&#x2F;uninitialized</td><td>如果 kubelet 启动时指定了一个外部云平台，它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。</td></tr></tbody></table><p>  当节点出现上述情况时，就节点控制器就会自动为节点添加上述类型的污点，当状态恢复正常时，节点控制器会自动清除污点。</p><p>  那什么时候我们可以使用 <code>tolerationSeconds </code>选项，当Pod所在的节点发生类如上述状态时，希望Pod能够暂时运行一段时间。则可以在 <code>tolerations</code> 配置中添加 <code>tolerationSeconds</code> 并设置合适的时间参数。这样做的好处是用于提升节点上Pod运行的稳定性。在生产的场景。可能节点出现了短暂的网络异常或者内存压力等等。这设置 <code>tolerationSeconds</code>可以避免节点上的Pod被直接驱逐。提升Pod运行的稳定性。</p><p>  默认情况下，Pod会被自动添加上 <code>node.kubernetes.io/not-ready</code> 以及 <code>node.kubernetes.io/unreachable</code> 的污点容忍。具体配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">node.kubernetes.io/not-ready</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">    <span class="attr">tolerationSeconds:</span> <span class="number">300</span>  <span class="comment">#当节点出现 node.kubernetes.io/not-ready污点时，允许Pod继续运行5分钟</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">node.kubernetes.io/unreachable</span>  <span class="comment">#当节点出现 node.kubernetes.io/unreachable污点时，允许Pod继续运行5分钟</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">    <span class="attr">tolerationSeconds:</span> <span class="number">300</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>  通过上述的两个容忍配置，可以避免Pod因为节点短暂出现 <code>NotReady</code> 或者 <code>UnKnown</code> 的状态而导致Pod被驱逐。 </p><blockquote><p>通过DaemonSet创建出来的Pod会将 <code>tolerationSeconds</code> 设置为0。 避免Pod被驱逐。</p></blockquote><hr><h2 id="亲和性与反亲和性"><a href="#亲和性与反亲和性" class="headerlink" title="亲和性与反亲和性"></a><strong>亲和性与反亲和性</strong></h2><p>  污点是用于避免Pod调度到特定节点上。而亲和性&#x2F;反亲和性是用来控制Pod调度到满足亲和性&#x2F;反亲和性策略的特定节点。亲和性策略在粒度上分为节点亲和性和Pod亲和性。在规则上分为标准和扩展两种。</p><h3 id="nodeName以及nodeSelector"><a href="#nodeName以及nodeSelector" class="headerlink" title="nodeName以及nodeSelector"></a><strong>nodeName以及nodeSelector</strong></h3><p>  <code>nodeName</code> 与 <code>nodeSelector</code> 是早期实现的方法，它们是 pod.spec 中可配置选项。通过这两个配置可以实现让 Pod 调度到特定的节点上。</p><p>  通过 <code>nodeName</code> 可以将 Pod 调度到满足节点名称的节点上。该操作是强制匹配，会跳过 schedule 的调度过程。所以 <code>nodeName</code> 的调度优先级是最高的。</p><p>  <code>nodeName</code> 的配置示例</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">k8s-node01</span>        <span class="comment">#设置nginx Pod必须调度到节点名称为k8s-node01的节点上。</span></span><br></pre></td></tr></table></figure><p>  <code>nodeSelector</code> 是基于节点标签来实现的约束调度。通过配置 <code>nodeSelector </code>选项，我们可以让Pod运行在满足标签要求的节点上。<code>nodeSelector</code> 作用于 <code>MatchNodeSelector</code> 过滤方法。</p><p>  <code>nodeSelector</code> 的配置示例</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">node-type:</span> <span class="string">gray</span>           <span class="number">2</span> <span class="comment">#设置pod调度时，只允许调度到带有node-type=gray标签的节点上。</span></span><br></pre></td></tr></table></figure><p>  通过 <code>nodeName</code> 和 <code>nodeSelector</code> 可以快速的实现节点约束，让Pod调度到特定的节点上。但是这两种方法的灵活性不足。比方说现在有一个场景，业务系统由前后端组件构成。一类用于提供前端业务呈现。一类用于实现后端逻辑运算。每类组件的Pod具备多个副本。一方面我们希望Pod的副本处于不同的节点，保证组件的可靠性和稳定性。另一方面我们希望前端业务Pod和后端业务Pod能够尽可能部署在相同节点上。降低跨服务器，交换机，机房场景带来的延迟，提升应用性能。此时我们单使用 <code>nodeName</code> 或者 <code>nodeSelector</code> 不易于实现该类场景部署。对于复杂的调度场景。我们可以使用扩展性更好的亲和类配置来实现Pod调度。</p><hr><h3 id="基于命名空间修改-nodeSelecctor-配置"><a href="#基于命名空间修改-nodeSelecctor-配置" class="headerlink" title="基于命名空间修改 nodeSelecctor 配置"></a>基于命名空间修改 nodeSelecctor 配置</h3><p>  可以通过为 apiserver 启用 <code>PodNodeSelector</code> 准入控制器，为特定命名空间增加特定的注解。让 apiserver 在接收该类命名空间下的创建请求的时候自动添加<code>nodeSelector</code> 配置。实现 Pod 调度控制。</p><p>  <strong>配置方法</strong></p><p>  启用准入控制器，在 kube-apiserver 的启动命令中添加 <code>--enable-admission-plugins=...(原始参数),PodNodeSelector</code> 参数。让它启用 <code>PodNodeSelector</code>  准入控制。</p><p>  参考如下示例，为命名空间添加注解<code>scheduler.alpha.kubernetes.io/node-selector·，其值为 </code>rack&#x3D;1<code>（注意必须为字符串），这样 demo 命名空间下创建的 Pod 就会自动添加上</code>nodeSelector 配置<code>，选择的标签是 </code>rack&#x3D;1&#96; 。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">scheduler.alpha.kubernetes.io/node-selector:</span> <span class="string">&quot;rack=1&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo</span></span><br></pre></td></tr></table></figure><blockquote><p>准入控制是在apiserver这一层生效的。该命名空间下存量的 Pod 并不会因此自动添加上 nodeSelector 只有当新的创建请求出现时才会添加上。</p></blockquote><hr><h3 id="通过亲和配置实现Pod调度"><a href="#通过亲和配置实现Pod调度" class="headerlink" title="通过亲和配置实现Pod调度"></a><strong>通过亲和配置实现Pod调度</strong></h3><p>  亲和配置分为节点亲和以及 Pod 间亲和。通过节点亲和可以实现类似nodeSelector的能力。通过节点标签来控制让Pod调度到指定节点。而 Pod 间亲和则是通过 Pod 标签来控制。</p><blockquote><p>当前亲和性的配置仅在 Pod 调度期间工作，如果后续 Pod 被调度到的节点修改了标签将不会影响到Pod运行。</p></blockquote><h4 id="基于节点的亲和性调度"><a href="#基于节点的亲和性调度" class="headerlink" title="基于节点的亲和性调度"></a>基于节点的亲和性调度</h4><p>  当前节点亲和的可配置选项为 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 以及 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 两种。分别为软调度和硬调度的意思。</p><h5 id="基于requiredDuringSchedulingIgnoredDuringExecution实现节点亲和"><a href="#基于requiredDuringSchedulingIgnoredDuringExecution实现节点亲和" class="headerlink" title="基于requiredDuringSchedulingIgnoredDuringExecution实现节点亲和"></a><strong>基于requiredDuringSchedulingIgnoredDuringExecution实现节点亲和</strong></h5><p>  使用<code>requiredDuringSchedulingIgnoredDuringExecution</code> 可以实现类nodeSelector的能力。但是它支持的选项比nodeSelector更加丰富。</p><p>  <code>requiredDuringSchedulingIgnoredDuringExecution</code> 的示例配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">nodeAffinity:</span></span><br><span class="line">      <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">In</span>                            <span class="comment"># 使用 In 规则代表 value字段必须满足values的其中一个元素</span></span><br><span class="line">              <span class="attr">values:</span>                                <span class="comment"># values 字段属于数组，配置的时候允许多个元素</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;01&#x27;</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;02&#x27;</span></span><br></pre></td></tr></table></figure><p>  通过上述的亲和度配置。实现的效果是，创建一个名称为 nginx 的 Pod ，设置 Pod 标签为 <code>run=nginx</code> ，运行在 pod-demo 命名空间下。配置硬亲和性调度，要求该 Pod 必须在具备 <code>app=01</code> 或 <code>app=02</code> 标签的节点上运行。</p><p>matchExpressions支持的选项如下</p><table><thead><tr><th>字段</th><th>说明</th></tr></thead><tbody><tr><td>key</td><td>选择器所使用的标签键值</td></tr><tr><td>operator</td><td>有效的运算符有 In、NotIn、Exist、DoesNotExist 以及 GT 和 LT</td></tr><tr><td>values</td><td>当 operator 使用 In 或者 NotIn 时 values 字段必须非空。当operator使用 Exist 或者 DoestNotExist 时，该 values 字段必须为空。当 operator 使用 GT 或者 LT 时，values字段至少拥有一个元素。</td></tr></tbody></table><blockquote><p>使用 operator 中的 Exist 以及 DoestNotExist 相当于只需要判断节点标签的键值是否存在。</p><p>通过给operator使用 NotIn 或者 DoestNotExist 可以实现反亲和。</p></blockquote><p><code>requiredDuringSchedulingIgnoredDuringExecution</code> 下可以支持同时设置多个 <code>nodeSelectorTerms</code> ，单个 <code>nodeSelectorTerms</code>  可以同时设置多个 <code>matchExpressions</code> 。它们的匹配规则如下：</p><ul><li>同时指定多个 <code>nodeSelectorTerms</code> 的场景，当节点满足某一个 <code>nodeSelectorTerms</code>  的规则，即可被调度上。</li><li>当 <code>nodeSelectorTerms</code> 下同时指定了多个<code>matchExpressions</code>则节点需要满足所有的 <code>matchExpressions</code> 规则才能被调度上。</li></ul><h5 id="基于preferredDuringSchedulingIgnoredDuringExecution实现节点亲和"><a href="#基于preferredDuringSchedulingIgnoredDuringExecution实现节点亲和" class="headerlink" title="基于preferredDuringSchedulingIgnoredDuringExecution实现节点亲和"></a><strong>基于preferredDuringSchedulingIgnoredDuringExecution实现节点亲和</strong></h5><p>  <code>preferredDuringSchedulingIgnoredDuringExecution</code> 是一种软调度的方式，当节点满足 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 设定的规则时，在打分阶段可以得到更多的分值。从而提升满足规则对应节点被调度的优先级。</p><p>  <code>preferredDuringSchedulingIgnoredDuringExecution</code> 的示例配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">nodeAffinity:</span></span><br><span class="line">      <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">preference:</span></span><br><span class="line">            <span class="attr">matchExpressions:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">              <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;01&#x27;</span></span><br></pre></td></tr></table></figure><p>  通过上述的亲和度配置。实现的效果是，创建一个名称为 nginx 的 Pod ，设置标签为 <code>run=nginx</code> ，运行在 pod-demo 命名空间下。配置软亲和性调度，要求该 Pod 优先在具备 <code>app=01</code> 标签的节点上运行，该规则的权重值为1。</p><p>  <code>preferredDuringSchedulingIgnoredDuringExecution</code> 下必须指定 <code>preference</code> 以及 <code>weight</code> 。 <code>weight</code> 代表权重，可配置的数字范围1~100。<code>weight</code> 字段越大，在 schedule 中的打分阶段加分越多。<code>preference</code> 下则指定 <code>matchExpressions</code> 表示加分需要满足的规则。可以指定多个 <code>matchExpressions</code> ，节点只要满足任一条件即可。</p><p>  <code>preferredDuringSchedulingIgnoredDuringExecution</code> 和 <code>requiredDuringSchedulingIgnoredDuringExecution</code>  这两个亲和性配置是可以结合使用的。通过 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 可以限制 Pod 可调度的范围，再进一步通过 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 决定节点优先运行在该范围的哪台节点上。</p><blockquote><p>​    在配置亲和性调度时，要注意 <code>topologyKey</code> 的使用，相同的业务副本要使用相同的 <code>topologyKey</code> 作为调度拓扑的基准。否则会产生未知的调度情况。导致 Pod 副本不符合要求。</p></blockquote><h5 id="节点亲和性的配置案例"><a href="#节点亲和性的配置案例" class="headerlink" title="节点亲和性的配置案例"></a><strong>节点亲和性的配置案例</strong></h5><p>1.添加亲和性配置，要求 Pod 必须调度到键值为 product 标签的节点上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">nodeAffinity:</span></span><br><span class="line">      <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">product</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">Exist</span></span><br><span class="line">              <span class="attr">values:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>2.添加亲和性配置，要求 Pod 优先调度到具备 AZ1或者 AZ2 标签的节点。次优调度到 AZ3 的节点</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">nodeAffinity:</span></span><br><span class="line">      <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">preference:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">availibity-zone</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;az1&#x27;</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;az2&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">20</span></span><br><span class="line">        <span class="attr">preference:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">availibity-zone</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;az3&#x27;</span></span><br></pre></td></tr></table></figure><p>3.添加亲和性配置，要求 Pod 必须调度到带有 <code>dpdk=true</code> 的节点上。 并且优先调度到 AZ1 标签的节点。次优AZ2。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">nodeAffinity:</span></span><br><span class="line">      <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">dpdk</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">              <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">      <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">preference:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">availibity-zone</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;az1&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">60</span></span><br><span class="line">        <span class="attr">preference:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">availibity-zone</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">&#x27;az2&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="基于Pod的亲和性调度"><a href="#基于Pod的亲和性调度" class="headerlink" title="基于Pod的亲和性调度"></a>基于Pod的亲和性调度</h4><p>  通过基于节点的亲和性调度，我们可以实现类 <code>nodeSelector</code> 的能力，但具备更强的扩展性。上文提到了一种场景。在部署业务的时候划分了前端业务 Pod 和后端业务 Pod 。我们希望这两种 Pod 能够运行在相同的节点上。单通过节点亲和性调度。没办法很好的满足该场景。此时可以使用 Pod 亲和性调度来实现。</p><p>  通过 pod 亲和调度规则，可以实现基于已经在节点上运行的 pod 的标签来约束 pod 可以调度到的节点，而不是基于节点上的标签来约束。pod 亲和调度规则和节点亲和调度规则相同，具备<code>requiredDuringSchedulingIgnoredDuringExecution</code> 和 <code>preferredDuringSchedulingIgnoredDuringExecution</code>，分表表示硬性与软性要求。</p><p>  Pod间亲和规则引入 <code>topologyKey</code> 的选项，用于标识节点位置。通过设置 <code>topologyKey</code> 来决定被调度的节点是否为同一位置。来控制 Pod 的分布。我们可以将节点、机柜、机房、数据中心等作为 <code>topologyKey</code> 调度的粒度，为节点规划好对应的标签与值。</p><blockquote><p>  在使用 Pod 间反亲和性调度时，规划使用的 topologyKey 必须要在所有节点都具备，如果某些或所有节点缺少指定的 topologyKey 标签，可能会出现意外情况。Pod 可能会调度到未具备 topologyKey 的节点上。</p></blockquote><h5 id="Pod亲和性调度的配置示例"><a href="#Pod亲和性调度的配置示例" class="headerlink" title="Pod亲和性调度的配置示例"></a>Pod亲和性调度的配置示例</h5><p>  首先创建 frontend pod，提交下述的示例 YAML 创建资源，会在 default 命名空间下 创建一个名称为 frontend-pod 的 Pod。该 Pod 具备 <code>app=frontend</code> 的标签。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend-pod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><p>  然后创建一个 backend pod，提交下述的示例 YAML 创建资源，会在 default 命名空间下 创建一个名称为 backend-pod 的 Pod。该 Pod 具备 <code>app=backend</code> 的标签。 并且该 Pod 必须在带有 <code>app=frontend</code> 标签的 Pod 所运行的节点上运行。调度的拓扑域以<code>kubernetes.io/hostname</code> 为基准。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">backend-pod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">backend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">podAffinity:</span></span><br><span class="line">      <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">frontend</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backend</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><p>  最后创建名称为 frontend pod1 的 Pod ，提交下述的示例 YAML 创建资源， 要求该 Pod 不能在带有 <code>app=frontend</code> 标签的 Pod 所运行的节点上运行。调度的拓扑域以 <code>kubernetes.io/hostname</code> 为基准。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend-pod1</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">podAntiAffinity:</span></span><br><span class="line">      <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">frontend</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backend</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><p>  在kubernetes集群上执行上述动作以后。会产生三个 Pod ，其中 frontend pod 与 backend pod 会调度到相同的节点上。frontend  pod 和 frontend pod1 会在不同的节点上运行。</p><p><strong>通过 Pod 亲和性配置，优化前后端业务Pod部署</strong></p><p>  上文提到一个场景，业务系统由前后端组件构成。一类用于提供前端业务呈现。一类用于实现后端逻辑运算。每类组件的Pod具备多个副本。前后端 Pod 运行在同一个节点上。并且 Pod 副本分布在不同的节点上。现在可以通过 Pod 亲和性配置来实现该需求。</p><p>  创建前端业务 deployment ，提交下述的示例 YAML 创建资源，在 default 命名空间下创建名为 frontend-pod 的 deployment。并且设置标签为 <code>app=frontend</code> 。设置反亲和性规则，以 <code> kubernetes.io/hostname</code> 标签键值为调度基准。 要求该Deployment 创建 Pod 出的 Pod 不能运行在已带有 <code>app=frontend</code> 标签的 Pod 所运行的节点上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">frontend</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;kubernetes.io/hostname&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backend</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><p>  创建后端业务 deployment ，提交下述的示例 YAML 创建资源，在 default 命名空间下创建名为 backend-pod 的 deployment。并且设置标签为 <code>app=backend</code> 。设置亲和性规则，以 <code> kubernetes.io/hostname</code> 标签键值为调度基准。 要求该Deployment 创建 Pod 出的 Pod 必须运行在已带有 <code>app=frontend</code> 标签的 Pod 所运行的节点上。设置反亲和性规则，以 <code> kubernetes.io/hostname</code> 标签键值为调度基准。 要求该Deployment 创建 Pod 出的 Pod 不能运行在已带有 <code>app=backend</code> 标签的 Pod 所运行的节点上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">backend-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">backend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">backend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">frontend</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;kubernetes.io/hostname&quot;</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">backend</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;kubernetes.io/hostname&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backend</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><p>  通过上述两个动作，可以创建将前端业务 Pod 与 后端业务 Pod 的副本分布在三台不同的节点上。 并且前端业务 Pod 和 后端业务 Pod 会在相同的节点上运行。</p><hr><h3 id="Pod-拓扑约束"><a href="#Pod-拓扑约束" class="headerlink" title="Pod 拓扑约束"></a>Pod 拓扑约束</h3><p><strong>关于 topologySpreadConstraints 机制</strong></p><p>   上文提到的 nodeSelector、Affinity等机制都可以用于控制 Pod 调度。让 Pod 的分布符合预期。而不是随机散放在各个节点上。其中Affinity具备强大的扩展能力。可以很好的设置多个条件。让 Pod 调度能够实现复杂的亲和、反亲和场景。</p><p>  但是仅通过Affinity方法还不能满足某些场景下调度分布的需求。我们可以假设一种场景。现在有一个应用，它具备 4 个 Pod 副本。同时我们集群的服务器节点分布在不同的机架上。考虑到机柜级可靠性。我们需要将 4 个 Pod 副本尽可能的分布到不同的机架上。这个时候，我们是否可以如何通过 Affinity 配置来完成该项需求？</p><p>  首先我们现在的调度粒度希望是机柜级。给不同机柜的机器打上标签。此时一个拓扑域等同于一个机架。假设我们现在拥有一个 3 节点的kubernetes集群。节点分布在不同的机柜上。我们通过执行如下命令来完成配置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl label node k8s-master rack=1</span><br><span class="line">kubectl label node k8s-node01 rack=1</span><br><span class="line">kubectl label node k8s-node02 rack=2</span><br></pre></td></tr></table></figure><blockquote><p>​    为了模拟现实环境，给节点打上了 rack 标签，后续基于 rack 标签作为拓扑域调度。应反亲和性调度的要求， Topologykey 必须是所有节点都需要拥有的标签健。</p></blockquote><p>  此时节点标签如下</p><p>![image-20201128170755258](D:\My\学习\6. 笔记\3. 微服务\k8s\note_picture\kubernetes调度控制场景与配置方法.asserts\image-20201128170755258.png)</p><p>  通过给节点打上标签，相当于描述这两个节点位于不同的机架。现在通过如下 YAML 创建业务 Pod 。Pod 具备 4 个副本。并且要求分布在不同的机架上。这个时候我们可以尝试使用 Anti-Affinity 来完成该需求。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">business-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">4</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">business</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;rack&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">business</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><p>  执行完成以后，我们会发现，只有两个 Pod 处于 <code>Running</code> 状态，而另外两个则处于 <code>Pending</code> 状态</p><p>![image-20201128171200049](D:\My\学习\6. 笔记\3. 微服务\k8s\note_picture\kubernetes调度控制场景与配置方法.asserts\image-20201128171200049.png)</p><p>  在编写 YAML 的时候我们可以发现，我们的拓扑粒度是基于 rack 标签的。反亲和调度在执行的时候，它只会确保每个拓扑拥有一个 Pod  副本。但是我们现在只有两个机柜。 这样一个机柜至少要运行两个 Pod 才能满足我们的期望。 此时 Anti-Affinity 的缺陷就暴露出来了。它虽然能够满足反亲和配置。但是没有办法控制 Pod 在 拓扑上分布的数量。在多数据中心冗余，单一数据中心多副本的场景下。我们是不能很好的通过 Anti-ffinity 来完成该要求的。 </p><p>  我们修改上述的 YAML 配置，先删除原有的 deployment。执行如下 YAML 清单创建资源</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">business-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">4</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">rack</span></span><br><span class="line">        <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">        <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchLabels:</span></span><br><span class="line">            <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backend</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><blockquote><p>​    注意：topologySpreadConstraints 是在 v1.18 以后的版本才会默认打开。 之前的版本需要手动给 kube-apiserver 以及 kube-scheduler 添加 –feature-gates&#x3D;EvenPodsSpread&#x3D;true 的参数才能启用。</p></blockquote><p>  查看当前 Pod 的创建情况，从查询结果来看，Pod 似乎均匀的分布在了两个机柜上。</p><p>![image-20201128171934079](D:\My\学习\6. 笔记\3. 微服务\k8s\note_picture\kubernetes调度控制场景与配置方法.asserts\image-20201128171934079.png)</p><p>  查询kube-schedule日志我们可以看到，这次 Pod 创建的时候是有通过 spreadConstraint 去筛选节点的。避免 Pod 堆叠在某一个拓扑域上。实现了 Pod 打散。</p><p>![image-20201128171918527](D:\My\学习\6. 笔记\3. 微服务\k8s\note_picture\kubernetes调度控制场景与配置方法.asserts\image-20201128171918527.png)</p><p>  个人理解，spreadConstraint 计算过程大致如下，首先有这么几个变量</p><ul><li><p>MatchNum 节点当前拓扑具备条件的 Pod 数量</p></li><li><p>selfMatchNum 被调度的 Pod 是否满足 <code>topologySpreadConstraints</code> 中设置的 <code>labelSelector</code> 标签。如果满足则为 1</p></li><li><p>minMatchNum 在全局拓扑中具备满足条件 Pod 数量的最小值</p></li><li><p>maxSkew 在不同拓扑域中不均匀分布的最大程度</p><p>分析 <code>node &#39;k8s-node02&#39; failed spreadConstraint[rack]: MatchNum(1) + selfMatchNum(1) - minMatchNum(0) &gt; maxSkew(1) </code> 当时的计算场景。</p><p>MatchNum(1)，代表当前 k8s-node02 节点所在的拓扑域已经拥有 1 个满足条件的 Pod 。selfMatchNum(1)，表示本次待调度的 Pod 满足 <code>topologySpreadConstraints</code> 中设置的 <code>labelSelector</code> 标签。minMatchNum(0) 意味着当前拓扑域 rack&#x3D;1 当时具备满足条件的 Pod 数量为 0 。 此时计算结果，k8s-node02 所在节点的拓扑域 Skew 为 2。大于 maxSkew。此时 Pod 将不会调度到 k8s-node02。</p></li></ul><p>​    </p><p><strong>PodTopologySpread</strong>  选项解释</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mypod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="string">&lt;integer&gt;</span></span><br><span class="line">      <span class="attr">topologyKey:</span> <span class="string">&lt;string&gt;</span></span><br><span class="line">      <span class="attr">whenUnsatisfiable:</span> <span class="string">&lt;string&gt;</span></span><br><span class="line">      <span class="attr">labelSelector:</span> <span class="string">&lt;object&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>maxSkew</strong> 描述 Pod 分布不均的程度。这是给定拓扑类型中任意两个拓扑域中 匹配的 pod 之间的最大允许差值。它必须大于零。取决于的 <code>whenUnsatisfiable</code> 取值，其语义会有不同。<ul><li>当 <code>whenUnsatisfiable</code> 等于 “DoNotSchedule” 时，<code>maxSkew</code> 是目标拓扑域 中匹配的 Pod 数与全局最小值之间可存在的差异。</li><li>当 <code>whenUnsatisfiable</code> 等于 “ScheduleAnyway” 时，调度器会更为偏向能够降低 偏差值的拓扑域。</li></ul></li><li><strong>topologyKey</strong> 是节点标签的键。如果两个节点使用此键标记并且具有相同的标签值， 则调度器会将这两个节点视为处于同一拓扑域中。调度器试图在每个拓扑域中放置数量 均衡的 Pod。</li><li><strong>whenUnsatisfiable</strong>，指示如果 Pod 不满足分布约束时如何处理：<ul><li><code>DoNotSchedule</code>（默认）告诉调度器不要调度。</li><li><code>ScheduleAnyway</code> 告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对 节点进行排序。</li></ul></li><li><strong>labelSelector</strong> 用于查找匹配的 pod。匹配此标签的 Pod 将被统计，以确定相应 拓扑域中 Pod 的数量。</li></ul><p><strong>多<code>topologySpreadConstraints</code> 调度的例子</strong></p><p>  通过上述的例子，可以知道 <code>topologySpreadConstraints</code> 可以实现对 Pod 拓扑分布的数量控制。满足某些场景下的 Pod 分布需求。 它还支持多<code>topologySpreadConstraints</code>，进一步的控制 Pod 的分布。比方说我们现在有两个 AZ，希望创建的 Pod 首先需要分布在两个 AZ 下。在 AZ 内部我们又有多个 rack ，我们希望他在 AZ 内又分布在不同的 rack 下。这样做的好处是，不仅实现了跨 AZ 的分布。还避免了 Pod 堆叠在某一个机架上。</p><p>  大致的操作示例如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">business-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">4</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">az</span></span><br><span class="line">        <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">        <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchLabels:</span></span><br><span class="line">            <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">      <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">rack</span></span><br><span class="line">        <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">        <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchLabels:</span></span><br><span class="line">            <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backend</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><p><strong>与亲和性调度搭配工作的例子</strong></p><p>  <code>topologySpreadConstraints</code> 默认是从所有节点上搜寻 Topologykey的，我们可以结合 <code>nodeSelector</code> 或者 <code>nodeAffinity</code> 筛选符合条件的节点。减少<code>topologySpreadConstraints</code> 需要匹配的节点数量，并且将 Pod 分布控制在某一维度。使用下面的例子可以实现，创建 business-pod 将其均衡分布在带有 <code>rack=1</code> 或者 <code>rack=2</code>的节点上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">business-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">4</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">rack</span></span><br><span class="line">        <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">        <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchLabels:</span></span><br><span class="line">            <span class="attr">app:</span> <span class="string">business</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">rack</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="number">1</span></span><br><span class="line">                <span class="bullet">-</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backend</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">daocloud.io/nginx</span></span><br></pre></td></tr></table></figure><hr><p><strong>参考链接：</strong></p><p><a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/">https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/</a></p><p><a href="https://cloud.tencent.com/developer/article/1639217">https://cloud.tencent.com/developer/article/1639217</a></p><p><a href="https://github.com/kubernetes/kubernetes/blob/v1.17.0/pkg/scheduler/algorithm/predicates/predicates.go">https://github.com/kubernetes/kubernetes/blob/v1.17.0/pkg/scheduler/algorithm/predicates/predicates.go</a></p><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;kubernetes的调度和驱逐&quot;&gt;&lt;a href=&quot;#kubernetes的调度和驱逐&quot; class=&quot;headerlink&quot; title=&quot;kubernetes的调度和驱逐&quot;&gt;&lt;/a&gt;kubernetes的调度和驱逐&lt;/h1&gt;&lt;h2 id=&quot;kubernete</summary>
      
    
    
    
    
    <category term="K8S" scheme="https://guoltan.github.io/tags/K8S/"/>
    
  </entry>
  
</feed>
