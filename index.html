<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"guoltan.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="guoltan个人博客">
<meta property="og:url" content="https://guoltan.github.io/index.html">
<meta property="og:site_name" content="guoltan个人博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="guoltan">
<meta property="article:tag" content="摸鱼，自嗨，困奋">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://guoltan.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>guoltan个人博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="guoltan个人博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">guoltan个人博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">guoltan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/guoltan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;guoltan" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:897156356@qq.com" title="E-Mail → mailto:897156356@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/03/26/Kafka%20%E8%BF%90%E7%BB%B4%E6%8C%87%E4%BB%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/26/Kafka%20%E8%BF%90%E7%BB%B4%E6%8C%87%E4%BB%A4/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-26 11:23:30" itemprop="dateCreated datePublished" datetime="2024-03-26T11:23:30+08:00">2024-03-26</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h1><p>  本文主要介绍一些 kafka 常见的操作指令。并指导如何在 TKE 的场景下，对 kafka 进行操作。</p>
<h1 id="运维指令"><a href="#运维指令" class="headerlink" title="运维指令"></a><strong>运维指令</strong></h1><h2 id="Topic-类"><a href="#Topic-类" class="headerlink" title="Topic 类"></a><strong>Topic 类</strong></h2><h3 id="列出-topic"><a href="#列出-topic" class="headerlink" title="列出 topic"></a><strong>列出 topic</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --list --exclude-internal --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="查看-topic-详情"><a href="#查看-topic-详情" class="headerlink" title="查看 topic 详情"></a><strong>查看 topic 详情</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --describe --topic topicname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="创建-topic"><a href="#创建-topic" class="headerlink" title="创建 topic"></a><strong>创建 topic</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --create --topic topicname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="删除指定-topic"><a href="#删除指定-topic" class="headerlink" title="删除指定 topic"></a>删除指定 topic</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-topics.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --delete --topic topicname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h2 id="Consumer-groups-类"><a href="#Consumer-groups-类" class="headerlink" title="Consumer groups 类"></a><strong>Consumer groups 类</strong></h2><h3 id="列出-consumer-groups"><a href="#列出-consumer-groups" class="headerlink" title="列出 consumer groups"></a><strong>列出 consumer groups</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --list --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="查看-consumer-group-详细信息"><a href="#查看-consumer-group-详细信息" class="headerlink" title="查看 consumer group 详细信息"></a><strong>查看 consumer group 详细信息</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --describe --group groupname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h3 id="删除指定-consumer-group"><a href="#删除指定-consumer-group" class="headerlink" title="删除指定 consumer group"></a><strong>删除指定 consumer group</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092  --delete --group groupname --command-config /opt/kafka/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h2 id="生产与消费类"><a href="#生产与消费类" class="headerlink" title="生产与消费类"></a><strong>生产与消费类</strong></h2><h3 id="生产"><a href="#生产" class="headerlink" title="生产"></a><strong>生产</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-console-producer.sh --broker-list kf1:9092,kf2:9092,kf3:9092 --topic topicname --producer.config /opt/kafka_2.12-2.2.1/config/producer.properties</span><br></pre></td></tr></table></figure>

<h3 id="消费"><a href="#消费" class="headerlink" title="消费"></a><strong>消费</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --topic topicname --from-beginning --consumer.config /opt/kafka_2.12-2.2.1/config/consumer.properties</span><br></pre></td></tr></table></figure>

<h2 id="查看kafka数据保存时间"><a href="#查看kafka数据保存时间" class="headerlink" title="查看kafka数据保存时间"></a><strong>查看kafka数据保存时间</strong></h2><p>查看topic数据清理策略：</p>
<p><code>cat /opt/kafka/config/server.properties</code>  查看 <code>log.retention.hours</code> 参数，这个参数含义是kafka数据日志的保存时间，</p>
<h2 id="重新消费topic全部数据"><a href="#重新消费topic全部数据" class="headerlink" title="重新消费topic全部数据"></a><strong>重新消费topic全部数据</strong></h2><p>场景：需要重新消费一个topic里面的数据</p>
<p>消费者要从头开始消费某个topic的全量数据，需要满足2个条件（spring-kafka）：</p>
<p>（1）使用一个全新的”group.id”（就是之前没有被任何消费者使用过）;</p>
<p>（2）指定”auto.offset.reset”参数的值为earliest；</p>
<p>修改 <code>/opt/kafka/config/consumer.properties</code> 里面的<code>group.id</code>和<code>auto.offset.reset</code></p>
<p>把<a target="_blank" rel="noopener" href="http://group.id/"><u><span class="15">group.id</span></u></a>值改成一个新的值,<code>auto.offset.reset</code> 值改成 <code>earliest</code>;</p>
<p>执行消费命令消费topic就可以重头开始消费数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kf1:9092,kf2:9092,kf3:9092 --topic topicname --from-beginning --consumer.config /opt/kafka_2.12-2.2.1/config/consumer.properties</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/03/26/Pod%20%E5%86%85%20DNS%20%E6%9C%8D%E5%8A%A1%E8%A7%A3%E6%9E%90%E6%85%A2%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/26/Pod%20%E5%86%85%20DNS%20%E6%9C%8D%E5%8A%A1%E8%A7%A3%E6%9E%90%E6%85%A2%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-26 00:11:34" itemprop="dateCreated datePublished" datetime="2024-03-26T00:11:34+08:00">2024-03-26</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="简析-Pod-内部域名解析的过程"><a href="#简析-Pod-内部域名解析的过程" class="headerlink" title="简析 Pod 内部域名解析的过程"></a>简析 Pod 内部域名解析的过程</h1><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="1-创建测试-Pod"><a href="#1-创建测试-Pod" class="headerlink" title="1. 创建测试 Pod"></a>1. 创建测试 Pod</h3><p>创建 busybox.yaml 文件内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox</span><br><span class="line">  namespace: timatrix</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: busybox</span><br><span class="line">    image: centos:latest</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;360000&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">  nodeSelector:</span><br><span class="line">    kubernetes.io/hostname: &quot;192.168.0.2&quot;</span><br></pre></td></tr></table></figure>

<p>执行 <code>kubectl apply -f busybox.yaml</code> 创建 Pod，执行通过以后，执行 <code>kubectl get pod -n timatrix busybox</code> 命令查询 Pod 状态，确保 Pod 处于运行状态。</p>
<h3 id="2-安装-bind-utils-工具"><a href="#2-安装-bind-utils-工具" class="headerlink" title="2. 安装 bind-utils 工具"></a>2. 安装 bind-utils 工具</h3><p><strong>进入 busybox 容器</strong></p>
<p><code>kubectl exec -it -n timatrix busybox /bin/bash</code></p>
<p><strong>安装 bind-utils 工具（需确保当前容器的网络可以访问到 YUM 仓库）</strong></p>
<p><code>yum -y install bind-utils</code></p>
<h3 id="3-进行域名解析测试"><a href="#3-进行域名解析测试" class="headerlink" title="3. 进行域名解析测试"></a>3. 进行域名解析测试</h3><p><strong>执行 host 命令，进行域名解析功能验证，示例如下：</strong></p>
<p><code>host proxysql.ti-base.svc.cluster.local</code></p>
<p><strong>返回结果如下：</strong></p>
<p>![image-20210802235935863](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210802235935863.png)</p>
<h4 id="Pod-请求域名解析过程分析"><a href="#Pod-请求域名解析过程分析" class="headerlink" title="Pod 请求域名解析过程分析"></a>Pod 请求域名解析过程分析</h4><p>通过 host -v 域名 的指令，可以看到客户端请求域名解析的过程</p>
<p>![image-20210803091036319](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803091036319.png)</p>
<p>在 CoreDNS 所在的宿主机上抓包，也能看到客户端发起的解析请求</p>
<p>![image-20210803091119674](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803091119674.png)</p>
<p>从上述的操作来看，我们请求解析 proxysql.ti-base.svc.cluster.local 这个域名，客户端至少发起了四次查询，最终才拿到正确的 A 记录解析结果。</p>
<p>实际上，只有第四次请求的域名，才存在于 kubernetes 的 DNS 记录中。对于 Kubernetes 来说，它的 DNS 记录全域名的格式通常为 <code>service_name.namespace_name.svc.cluster.local</code> 所以请求发送到第四次，CoreDNS 才能正确的返回 A 记录。</p>
<p>为什么会产生多余的解析请求，这个和默认的 &#x2F;etc&#x2F;resolv.conf 配置有关，打开 &#x2F;etc&#x2F;resolv.conf，我们可以看到容器内部会有如下默认配置。</p>
<p>![image-20210803092022332](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803092022332.png)</p>
<p>这些配置项的作用如下：</p>
<ul>
<li>nameserver 用于配置 DNS 服务器，本地 hosts 无法解析域名的情况下，解析域名的请求将会发送到这个服务器地址上。</li>
<li>search 用于主机名查找的列表，当用户提交的主机名中，带有的 . 数量低于 ndots 设置的值时，会把用户提交的主机名加上 search 配置的域进行查询。</li>
<li>options ndots，用于控制 . 的数量，默认值为 5。这代表如果 Pod 请求域名的 . 数小于 5 时，就会先遍历 search 定义的域，组合成主机名再查询。</li>
</ul>
<p>当前 search 中定义了三个域，分别是 <code>timatrix.svc.cluster.local</code> <code>svc.cluster.local</code> <code>cluster.local</code> 而 <code>proxysql.ti-base.svc.cluster.local</code> 这个请求只使用了 4 个 <code>.</code> 这低于 ndots 设置的 5 个 <code>.</code> 于是乎就产生了如下的查询顺序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">第一次查询，coredns 返回无此记录</span></span><br><span class="line">&quot;proxysql.ti-base.svc.cluster.local.timatrix.svc.cluster.local&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">第二次查询，coredns 返回无此记录</span></span><br><span class="line">&quot;proxysql.ti-base.svc.cluster.local.svc.cluster.local&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">第三次查询，coredns 返回无此记录</span></span><br><span class="line">&quot;proxysql.ti-base.svc.cluster.local.cluster.local&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">第四次查询，当 主机名 + search域 都无法返回查询结果时，才直接使用提交的主机名进行查询</span></span><br><span class="line">&quot;proxysql.ti-base.svc.cluster.local&quot;</span><br></pre></td></tr></table></figure>

<h4 id="search-配置项带来的风险"><a href="#search-配置项带来的风险" class="headerlink" title="search 配置项带来的风险"></a>search 配置项带来的风险</h4><p>如果我们的 Pod 在调用 service 的时候，填写的是 <code>service_name.namespace_name.svc.cluster.local</code> 这种的形式时，如 <code>proxysql.ti-base.svc.cluster.local.timatrix.svc.cluster.local</code> 这个时候会因为 <code>.</code> 的数量小于 ndots 设置的值，此时请求的主机名将会和 search 配置的域进行组合成 FQDN 再进行请求，在此处实际上存在一个问题，如果 search 中引入了一个无效的域时，可能会影响到客户端请求域名解析的工作。</p>
<p>编辑 busybox 容器里的 &#x2F;etc&#x2F;resolv.conf 配置文件，在 search 中添加一个 aaa 的域。</p>
<p>![image-20210803100346723](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803100346723.png)</p>
<blockquote>
<p>只要 Pod 所在的宿主机 &#x2F;etc&#x2F;resolv.conf 上配置了 search，在创建 Pod 时都会自动添加到容器的 &#x2F;etc&#x2F;resolv.conf 中。</p>
</blockquote>
<p>此时再通过 host 指令，发起解析请求，可以看到在解析时，增加了一次查询，新增查询的记录为 <code>proxysql.ti-base.svc.cluster.local.aaa</code></p>
<p>![image-20210803101544682](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803101544682.png)</p>
<p>在 coredns 所在的宿主机上抓包，可以看到的是，这个域名请求实际上会转发给 183.60.83.19 这台上级 DNS，这是因为 coredns 默认情况下会将本地无法解析的域名，转发给 coredns 所在节点 &#x2F;etc&#x2F;resolv.conf 配置的 nameserver上。</p>
<p>![image-20210803101850591](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803101850591.png)</p>
<p>可以看到的是，如果 coredns 对于无法解析的域名，都会转发给上级服务器。而这个操作实际上引入了一个 244 毫秒的查询耗时，这还是在网络连通性正常的情况下，如果 coredns 无法及时得到上级 DNS 的响应。会阻塞到整个域名解析的请求工作。</p>
<p>修改 coredns 所在宿主机的 &#x2F;etc&#x2F;resolv.conf 配置，将 nameserver 的地址设置为一个无效的 DNS 服务器地址 10.100.100.1</p>
<p>![image-20210803105213518](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803105213518.png)</p>
<p>重启 coredns</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod -n kube-system coredns-xxx-xxx</span><br></pre></td></tr></table></figure>

<p>在 busybox 容器中再次发起解析域名的请求</p>
<p>![image-20210803111514224](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803111514224.png)</p>
<p>在 coredns 侧抓包，由于配置了不可达的 DNS 服务器，我们可以看到这次 coredns 转发给上级 DNS 请求，耗费了大量的时间，整个过程大约是 4 秒。虽然最终 busybox 还是可以获取到解析结果。</p>
<p>![image-20210803111204944](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803111204944.png)</p>
<p>一线在交付项目的时候，客户侧提供的机器模板中，虚拟机配置的 nameserver 不一定是有效的，或者说，客户侧的 DNS 服务不一定是可靠的，也有可能会出现故障。所以当 coredns 将请求转发给上级 DNS 时，会很有可能会引入性能问题，影响到容器平台组件的运行。</p>
<h4 id="规避无效的-search-引入的问题方法"><a href="#规避无效的-search-引入的问题方法" class="headerlink" title="规避无效的 search 引入的问题方法"></a>规避无效的 search 引入的问题方法</h4><p>操作系统自动添加 search 的触发条件比较多， cloudinit、dhclient、网络服务启动都有可能自动添加上 search，目前的想法是开机完成以后，在系统里面自动禁用 search 选项。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">规避开机后自动添加 search 的方法</span></span><br><span class="line">echo &quot;sed -i &#x27;s/^search/#search/&#x27; /etc/resolv.conf&quot; &gt;&gt; /etc/profile.d/disable_search.sh</span><br></pre></td></tr></table></figure>

<p>当前禁用 search 的方法只是避免开机后自动添加 search，对于其他场景如：ifup-post 钩子中存在添加 search 配置的操作时，这个方法是无效的，并且客户的操作系统究竟有哪些添加 search 的配置项我们是很难去控制的。</p>
<p>并且禁用宿主机 &#x2F;etc&#x2F;resolv.conf 中的 search 配置其实也只是避免客户端发送 coredns 本地无法处理的解析请求，我们在调用服务，请求域名解析时依旧引入了一些无效的域名解析工作。我认为要在 Pod 请求 DNS 就需要做优化。避免引入一些额外的查询开销或风险。</p>
<h1 id="Pod-请求-DNS-解析域名优化方案"><a href="#Pod-请求-DNS-解析域名优化方案" class="headerlink" title="Pod 请求 DNS 解析域名优化方案"></a>Pod 请求 DNS 解析域名优化方案</h1><h2 id="方案一，优化请求服务的域名"><a href="#方案一，优化请求服务的域名" class="headerlink" title="方案一，优化请求服务的域名"></a>方案一，优化请求服务的域名</h2><p>各个组件在调用 service 的配置中，使用如下规则：</p>
<ul>
<li>同命名空间下的 svc 调用，只填写 svc 名称，如调用 proxysql.ti-base.svc.cluster.local，仅填写 proxysql</li>
<li>不同命名空间下的 svc 调用，填写svc名称.命名空间名称，如调用 proxysql.ti-base.svc.cluster.local，仅填写 proxysql.ti-base</li>
</ul>
<h3 id="在同一个命名空间下调用，仅使用-svc-名称的原因"><a href="#在同一个命名空间下调用，仅使用-svc-名称的原因" class="headerlink" title="在同一个命名空间下调用，仅使用 svc 名称的原因"></a>在同一个命名空间下调用，仅使用 svc 名称的原因</h3><p> Pod 创建时会添加 search，第一个 search 域是 命名空间.svc.cluster.local。如果 Pod 调用的 svc 刚好是相同命名空间下，在解析时只写 svcname，这个时候会和 search 组合成完整的域名。可参考如下解析过程，整个过程只发起了一次解析请求。</p>
<p>![image-20210803120009040](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803120009040.png)</p>
<h3 id="在同一个命名空间下调用，填写svc名称-命名空间名称的原因"><a href="#在同一个命名空间下调用，填写svc名称-命名空间名称的原因" class="headerlink" title="在同一个命名空间下调用，填写svc名称.命名空间名称的原因"></a>在同一个命名空间下调用，填写svc名称.命名空间名称的原因</h3><p>当调用的 svc 和调用它的 Pod 处于不同的命名空间下时，在只填写 svc 名称的情况下是无法命中的，可参考下图</p>
<p>![image-20210803120302221](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803120302221.png)</p>
<p>原因是调用 svc 的 Pod 中，search 配置项是不会把其他命名空间加上的。</p>
<p>![image-20210803120529926](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803120529926.png)</p>
<p>此时是无法组合出有效的 kubernetes 域名，当前环境的 timatrix 这个 namespace 下并不存在 proxysql 这个 svc。</p>
<p>![image-20210803120605239](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803120605239.png)</p>
<p>实际上我们要调用的 proxysql 处于 ti-base 命名空间下，此时我们修改请求的域名，调整为 proxysql.ti-base</p>
<p>![image-20210803120655886](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803120655886.png)</p>
<p>这一次可以看到，在请求解析域名时，只提交了两次请求。可以减少无效的解析域名请求。</p>
<p>最关键的一点是，就算宿主机配置了 search，将 search 的配置添加到容器的 &#x2F;etc&#x2F;resolv.conf 中，它也不会触发问题。只要我们保证请求的域名是一个有效并且存在于 coredns 中的记录即可。</p>
<h2 id="方案二，使用-FQDN-调用域名"><a href="#方案二，使用-FQDN-调用域名" class="headerlink" title="方案二，使用 FQDN 调用域名"></a>方案二，使用 FQDN 调用域名</h2><p>如果在对接的配置文件中，需要填写完整的域名名称，也可以通过在末尾添加 ‘.’ 来规避，只要请求解析域名时在域名尾部添加一个 <code>.</code>，此时就会直接以 FQDN 去请求，此时会忽略 search 配置。</p>
<p>未在末尾添加 . 的场景，可以看到会尝试和 search 配置的域进行组合，再提交请求</p>
<p>![image-20210803121242248](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803121242248.png)</p>
<p>添加 . 在末尾的场景，可以看到解析过程中，不会再组合 search 中的域，而是直接提交请求</p>
<p>![image-20210803121634253](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803121634253.png)</p>
<p>如果在调用服务时，在末尾添加一个 <code>.</code> 也可以优化请求，以这个 Pod 为例子，再去调用 <code>proxysql.ti-base.svc.cluster.local</code>，使用 <code>proxysql.ti-base.svc.cluster.local.</code> 即可完成查询</p>
<p>![image-20210803121532403](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803121532403.png)</p>
<p>从查询返回的结果来看。它只提交了一次。</p>
<p>从 coredns 侧抓包，也可以看到它只收到了一次请求</p>
<p>![image-20210803121849867](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803121849867.png)</p>
<p>这样即可以避免产生了无效的域名解析请求的次数，也保证它不会受到宿主机的 &#x2F;etc&#x2F;resolv.conf 中 search 配置的影响</p>
<h2 id="方案三，优化-Pod-的-DNS-配置项"><a href="#方案三，优化-Pod-的-DNS-配置项" class="headerlink" title="方案三，优化 Pod 的 DNS 配置项"></a>方案三，优化 Pod 的 DNS 配置项</h2><p>上文中提到 ndots 可以控制请求的域名小于多少的 <code>.</code> 时才去组合 search。可以去调整特定组件的 dnsConfig 配置，调低这个数值。</p>
<p>使用默认的 ndots: 5 配置时，请求 proxysql.ti-base.svc.cluster.local 的解析过程</p>
<p>![image-20210803122307077](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803122307077.png)</p>
<p>调整 ndots: 5 配置为 ndots: 2 时，请求 proxysql.ti-base.svc.cluster.local 的解析过程</p>
<p>![image-20210803122356111](D:\My\工作\Tools man\3. 案例与故障排查文档\Kubernetes Pod DNS 解析优化方法.assets\image-20210803122356111.png)</p>
<p>通过合理的调整 ndots，也可以减少产生无效的域名解析请求，并且也可以避免宿主机 search 带来的影响。</p>
<p>在实际运用中配置实际上需要在 deployment、statefulset、pod 处指定，可参考如下</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">timatrix</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">centos:latest</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">sleep</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;360000&quot;</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">kubernetes.io/hostname:</span> <span class="string">&quot;192.168.0.2&quot;</span></span><br><span class="line">  <span class="attr">dnsConfig:</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ndots</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;2&quot;</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">同时启用 tcp_timestamp 和 tcp_tw_recycle 导致业务不通排查记录</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-02-28 15:11:25" itemprop="dateCreated datePublished" datetime="2024-02-28T15:11:25+08:00">2024-02-28</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-问题背景"><a href="#1-问题背景" class="headerlink" title="1. 问题背景"></a>1. 问题背景</h1><p>  经项目同学反馈，有个环境出现了一个怪异的问题，环境中有三台机器，一台一体机服务器(172.16.1.201)，一台 Linux 客户端(172.16.1.202)，一台 Windows 客户端(172.16.1.204)。在一体机服务器上有个 web-server 的应用，通过 nodePort 在机器上暴露了端口。让外部通过该端口访问这个应用。但奇怪的现象是，一体机和 Linux 客户端都可以通过 nodePort 发布的端口正常连通该机器的业务端口，但是 Windows 客户端不可以。整体关系如下图：</p>
<img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/1.jpg" class="" title="1.jpg">

<h1 id="2-排查过程"><a href="#2-排查过程" class="headerlink" title="2. 排查过程"></a>2. 排查过程</h1><h2 id="2-1-测试业务配置问题"><a href="#2-1-测试业务配置问题" class="headerlink" title="2.1. 测试业务配置问题"></a>2.1. 测试业务配置问题</h2><p>  此类问题应该优先确认网络侧的信息，而当时忘记使用 netstat -s 直接查看网络丢包信息。所以先选择通过查看应用配置的方式去确认原因。当时假设出现的问题可能如下：</p>
<ul>
<li><p>因为反馈 linux 客户端可以正常连通，但是 windows 客户端不可以，可能是 windows 发送的数据包有某些字段服务器接受不了（从抓包结果看有带 EW flags）</p>
</li>
<li><p>业务构建的 nginx 镜像有一些特殊配置，或者镜像使用的函数库有一些 BUG 导致 windows 发的包无法接收。</p>
</li>
<li><p>某个资源定义的配置影响到业务</p>
</li>
</ul>
<h3 id="2-1-1-排查服务端不接收-ECN-包导致丢包"><a href="#2-1-1-排查服务端不接收-ECN-包导致丢包" class="headerlink" title="2.1.1. 排查服务端不接收 ECN 包导致丢包"></a>2.1.1. 排查服务端不接收 ECN 包导致丢包</h3><p>  因为项目侧同学提供的抓包看，windows 发的包是有带 EW 字段的，该字段用于 ECN 功能。如果服务端不接受 ECN 功能，有导致异常的可能。所以先检查了该配置。</p>
<p>cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_ecn</p>
<ul>
<li><p>如果返回 0，代表不支持 tcp_ecn 的发送和接收。</p>
</li>
<li><p>如果返回 1，代表支持接收但不会主动发送这类报文。</p>
</li>
<li><p>如果返回 2，代表可以发也可以收 ECN 类的报文。</p>
</li>
</ul>
<p>  经确认现场的业务容器里面输出结果是 2，也就是环境未触发该问题。</p>
<h3 id="2-1-2-排查业务镜像问题"><a href="#2-1-2-排查业务镜像问题" class="headerlink" title="2.1.2. 排查业务镜像问题"></a>2.1.2. 排查业务镜像问题</h3><p>  先通过普通的 nginx 镜像重新下发一个 deployment，并配置 nodePort 的 service 进行连通性测试。此时发现 windows 客户端和 nginx 的业务连通性是正常的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 deployment 测试</span></span><br><span class="line">kubectl create deploy --image nginx:latest testweb --target-port 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 service 测试</span></span><br><span class="line">kubectl expose deploy testweb --port 80 --target-port 80 --name testweb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改类型为 NodePort，将 ClusterIP 替换为 NodePort，并在 Port 下追加一个 nodePort: 31805 的字段（测试端口，环境未使用）</span></span><br><span class="line">kubectl edit svc testweb</span><br></pre></td></tr></table></figure>

<p>  验证后发现基本连通性正常，此时怀疑是否和镜像有关，将镜像更新成 web-server 服务的镜像，并配置 nodePort 的 service 进行连通性测试。此时发现 windows 客户端和 nginx 的业务连通性还是正常的。到此可以推测问题可能和业务镜像无关。</p>
<h3 id="2-1-3-排查-deployment-配置问题"><a href="#2-1-3-排查-deployment-配置问题" class="headerlink" title="2.1.3. 排查 deployment 配置问题"></a><strong>2.1.3. 排查 deployment 配置问题</strong></h3><p>  由于镜像无误，此时尝试复制 web-server 的 YAML，重新配置一份副本。进行业务测试。此时复现了 windows 客户端请求业务不通的问题。</p>
<p>  由此现象可以得知，有个配置影响到了业务连通性，于是乎进行了配置删除的测试。测试的类型和结果如下：</p>
<ul>
<li><p>dnsConfig 配置  – 业务依旧不通</p>
</li>
<li><p>volumes,volumeMount – 业务依旧不通</p>
</li>
<li><p>SecurityContext – 业务依旧不通</p>
</li>
<li><p>LivenessProbe,ReadnessProbe – 业务正常连通。</p>
</li>
</ul>
<p>  经测试，基本上可以确认去除探针配置以后，业务就能够正常了。目前表现看是探针配置引发了业务异常。此时我先假设问题可能是：</p>
<ul>
<li><p>业务 BUG？</p>
</li>
<li><p>Kubelet 探针的 BUG？</p>
</li>
</ul>
<p>  顺着这个思路进行问题搜索，网上没有相关的案例。此时思路转变为追加探针以后，实际上 kubelet 会一直发送请求来测试业务的健康性。本质上存在建立连接的动作。此时定位的思路转变为如何确认建立多个连接下，新连接会丢包的原因。</p>
<h2 id="2-2-确认容器丢包原因"><a href="#2-2-确认容器丢包原因" class="headerlink" title="2.2. 确认容器丢包原因"></a><strong>2.2. 确认容器丢包原因</strong></h2><p>  基于 2.1 的排查结果，最后确认的思路是需要确认什么时候建立多个 TCP 连接会导致业务丢包。为此刚好记起通过 netstat -s 通常情况下可以获取到业务丢包的原因。需要基于该指令进行进一步的问题范围收敛。重新为测试 Pod 添加回探针以后，在 windows 客户端上进行连通性测试。在对应 Pod 容器进行 netstat -s ，结果如下：</p>
<p><strong>在该容器的网络命名空间查询</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker inspect -f &#x27;&#123;&#123; .State.Pid &#125;&#125;&#x27;</span><br><span class="line">nsenter -t PID -n netstat -s</span><br></pre></td></tr></table></figure>

<p><strong>输出的结果</strong></p>
<img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/2.jpg" class="" title="2.jpg">

<p>从截图来看，已经可以找到丢包的原因，就是因为 timestamps 的问题导致业务丢包。此时问题已经比较明确了。此时通过搜索关键字，发现同时启用 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps 会导致业务丢包。（具体原因分析参考 3 原因分析章节）</p>
<h2 id="2-3-解决方法"><a href="#2-3-解决方法" class="headerlink" title="2.3. 解决方法"></a><strong>2.3. 解决方法</strong></h2><p>  通过原因分析，可以得知问题是因为同时启用 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps，此时只能开启一个，从优先级判断，tcp_tw_recycle 优先级更低，需要将其关闭。</p>
<p>  由于业务容器内没有 &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_tw_recycle 参数，于是乎选择在宿主机上关闭 tcp_tw_recycle 并重启业务进行修复。最终效果如下</p>
<img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/3.jpg" class="" title="3.jpg">

<h1 id="3-原因分析"><a href="#3-原因分析" class="headerlink" title="3. 原因分析"></a><strong>3. 原因分析</strong></h1><h2 id="3-1-TimeStamps-用途"><a href="#3-1-TimeStamps-用途" class="headerlink" title="3.1. TimeStamps 用途"></a><strong>3.1. TimeStamps 用途</strong></h2><p>  timestamps 机制的用处：</p>
<ul>
<li><p>防止上一个TCP连接的延迟的数据包，影响到新的 TCP 连接。</p>
</li>
<li><p>提供更为精确的 RTT 计算能力。</p>
</li>
</ul>
<p>  timestamps 的组成</p>
<img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/4.jpg" class="" title="4.jpg">

<p>  其中，timestamp value（简称 tsval）用于存储当前请求报文的时间戳值，timestamp echo reply（简称 TSecr）用于存储最近接收到的对端所发送报文的 TSopt 选项中包含的 TSval 时间戳值。</p>
<p>  那 timestamps 的值 tsval 是怎么获取的，实际上是通过 CPU tick 来获取的（也就是说每个机器的 CPU tick 是很难保持相同的）。</p>
<h2 id="3-2-windows-机访问-web-server-为什么会导致丢包？"><a href="#3-2-windows-机访问-web-server-为什么会导致丢包？" class="headerlink" title="3.2. windows 机访问 web-server 为什么会导致丢包？"></a><strong>3.2. windows 机访问 web-server 为什么会导致丢包？</strong></h2><h3 id="3-2-1-tcp-timestamps-和-tcp-tw-recycle-的作用"><a href="#3-2-1-tcp-timestamps-和-tcp-tw-recycle-的作用" class="headerlink" title="3.2.1. tcp_timestamps 和 tcp_tw_recycle 的作用"></a><strong>3.2.1. tcp_timestamps 和 tcp_tw_recycle 的作用</strong></h3><p>  tcp_timestamps 用于启用时间戳特性以提供更加精确的 RTT 计算以及额外特性功能的支持（如 PAWS），而 tcp_tw_recycle 用于配置 TIME_WAIT 快速回收以增快资源回收。</p>
<p>  tcp_timestamps 和 tcp_tw_recycle 选项开启之后，自动开启 per-host 的 PAWS 机制，也就是会对对端 IP 进行 PAWS，而不是 IP + 端口的四元组进行 PAWS 检查。</p>
<h3 id="3-2-2-同时开启-tcp-timestamps-和-tcp-tw-recycle-导致丢包的场景"><a href="#3-2-2-同时开启-tcp-timestamps-和-tcp-tw-recycle-导致丢包的场景" class="headerlink" title="3.2.2. 同时开启 tcp_timestamps 和 tcp_tw_recycle 导致丢包的场景"></a><strong>3.2.2. 同时开启 tcp_timestamps 和 tcp_tw_recycle 导致丢包的场景</strong></h3><p>  根据内核参数的解释，如果同时开启 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps 时，此时服务端在处理数据包时会要求报文中的 timestamps 字段是递增的，如果服务端运行在 NAT 环境下，处理来自不同客户端的请求时会有可能触发丢包。下图以一个 CLB 的场景为例进行解释：</p>
<img src="/2024/02/28/%E5%90%8C%E6%97%B6%E5%90%AF%E7%94%A8%20tcp_timestamp%20%E5%92%8C%20tcp_tw_recycle%20%E5%AF%BC%E8%87%B4%E4%B8%9A%E5%8A%A1%E4%B8%8D%E9%80%9A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/5.jpg" class="" title="5.jpg">

<p>  当客户端 1 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME-WAIT 状态的连接后，客户端 2 也通过 NAT 网关和服务器建立 TCP 连接，注意客户端 1 和 客户端 2 因为通过 CLB 的 NAT，所以对服务端来说是使用相同的 IP 地址建立 TCP 连接，如果客户端 2 的 timestamp 比 客户端 1 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 2 发来的 SYN 包。</p>
<h3 id="3-2-3-我们的-web-server-应用是怎么满足了-tcp-tw-recycle-丢包场景？"><a href="#3-2-3-我们的-web-server-应用是怎么满足了-tcp-tw-recycle-丢包场景？" class="headerlink" title="3.2.3. 我们的 web-server 应用是怎么满足了 tcp_tw_recycle 丢包场景？"></a><strong>3.2.3. 我们的 web-server 应用是怎么满足了 tcp_tw_recycle 丢包场景？</strong></h3><p>  <strong>根据上述的环境总结出的丢包触发规律如下：</strong></p>
<ul>
<li><p>tcp_timestamps 和 tcp_tw_recycle 处于启用状态。</p>
</li>
<li><p>服务端运行在 NAT 环境下，此时服务端接收到请求的客户端 IP 是相同的。</p>
</li>
<li><p>客户端 IP 实际上对应不同的客户端，短时间（60秒）内存在多个 TCP 请求业务。</p>
</li>
<li><p>多个客户端的 timestamps 存在偏差，报文收到的 timestamps opt 中的 tsval 时间要小于 peer 保存的 timestamp 时间。timestamp 来自 CPU Tick，非同一个节点很难保证相同的，所以这个时候时间戳基本上很难保证的线性递增的。</p>
</li>
</ul>
<p>  实际上 web-server 应用是通过 nodePort 类型的 service 进行发布的，而 kube-proxy 无论是 iptables 还是 ipvs 模式，他们都是通过 NAT 的方式来实现网络，此时服务端就满足了 NAT 环境的要素。而我们业务配置了探针，这就满足了多个客户端在短时间内存在多个 TCP 请求业务的场景。此时 windows 客户端发包时，就会被进行报文时间戳的比较。如果报文低于已经存在 peer 最近一次时间戳，就会满足丢包的条件。</p>
<p>  <strong>本次案例 windows 访问不通的大体总结如下：</strong></p>
<p> 1. 一体机的 kubelet 进程发起 TCP 连接 web-server 的 80 端口进行探测。然后关闭连接。由于开启了 recycle 此时 time_wait 的连接会被快速回收掉，但 web-server 容器环境会记录一体机这个 peer 的时间戳（可以通过 ip tcp_metrics show 的方式去查 看，其中 tw_ts 字段缓存了该信息）也就是连接虽然释放了，但是 peer 信息还在系统中留存。</p>
<p> 2. 此时 windows 客户端发起请求，请求发送到一体机暴露的 30180 端口以后，会被进行 NAT 转换，再传递到后端 web-server 容器。</p>
<p> 3. 此时 web-server 容器环境因为配置了 tcp_timestamp 和 tcp_tw_recycle 这时候是通过 per-host 的PAWS 进行检查。此时会将 windows 新发的报文提取 tsval 和 peer 里面留存的 timestamps 进行比较。</p>
<p> 4. 由于 windows 客户端和一体机所在节点的 CPU Tick 不同，导致他们生成的 timestamp 也不相同。只要 windows CPU tick 计算出的时间比一体机的小，这时候就会导致 web-server 将 windows 客户端发起的 SYN 进行丢弃。造成业务不通。</p>
<h3 id="3-2-4-关闭-tcp-tw-recycle-以后，为什么会正常？"><a href="#3-2-4-关闭-tcp-tw-recycle-以后，为什么会正常？" class="headerlink" title="3.2.4. 关闭 tcp_tw_recycle 以后，为什么会正常？"></a>3.2.4. 关闭 tcp_tw_recycle 以后，为什么会正常？</h3><p>  一旦关闭 tcp_tw_recycle ，PAWS 就不会 per-host 去检查，而是以四元组的方式进行检查，这样一体机节点和 Windows 就算发起连接会被 NAT 成相同的 IP。也会因为不同的源端口而被区分。</p>
<h3 id="3-2-5-为什么现场说-Linux-客户端可以正常的联通业务？"><a href="#3-2-5-为什么现场说-Linux-客户端可以正常的联通业务？" class="headerlink" title="3.2.5. 为什么现场说 Linux 客户端可以正常的联通业务？"></a>3.2.5. 为什么现场说 Linux 客户端可以正常的联通业务？</h3><p>  我对此问题的假设是满足了下面其中一个条件，所以没有触发丢包问题：</p>
<ul>
<li><p>这台 Linux 客户端和服务端拥有相同的 CPU Clock 或者比一体机大</p>
</li>
<li><p>Linux 客户端节点已关闭 timestamps</p>
</li>
</ul>
<p>参考连接</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Ivan_Wz/article/details/112250255">踩坑内核参数tcp_tw_recycle_Ivan_Wz的博客-CSDN博客</a> - 踩坑内核参数tcp_tw_recycle</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sunsky303/p/12818009.html">不要启用 net.ipv4.tcp_tw_recycle - sunsky303 - 博客园</a> - 不要启用 net.ipv4.tcp_tw_recycle</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/thehunters/article/details/122240272">客户端第二次连接失败，SYN包发了，没有收到服务端回 SYN+ACK ，SYN包被丢弃了_服务器句柄资源不足 丢弃 syn包_thehunters的博客-CSDN博客</a> – 客户端第二次连接失败，SYN包发了，没有收到服务端回 SYN+ACK ，SYN包被丢弃了</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64690239">https://zhuanlan.zhihu.com/p/64690239</a> – Linux内核协议栈丢弃SYN报文的主要场景剖析</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/02/15/%E6%9F%90%E7%8E%AF%E5%A2%83%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8%E5%81%B6%E7%8E%B0%20502%20%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/02/15/%E6%9F%90%E7%8E%AF%E5%A2%83%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8%E5%81%B6%E7%8E%B0%20502%20%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">某环境接口调用偶现 502 问题分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-02-15 19:00:25" itemprop="dateCreated datePublished" datetime="2024-02-15T19:00:25+08:00">2024-02-15</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a><strong>1. 概述</strong></h1><p>  本文的内容主要是对 TI361演示环境偶发 502 该问题的处理进行一次记录。其中章节 2 - 章节 3 主要讲述了问题现象、整个问题的排查过程、以及最后的解决方法。章节 4 主要是记录本次排查问题中对于某些问题的思考。<strong>章节 5</strong> 主要是对 2 ~ 3 章节的排查过程进行一次思路整理。如果需要定位类似问题，可以直接参考<strong>章节 5</strong> 的内容操作。</p>
<h1 id="2-问题现象"><a href="#2-问题现象" class="headerlink" title="2. 问题现象"></a><strong>2. 问题现象</strong></h1><p>  用户反馈 361 演示环境存在异常，在使用的过程中偶发出现 Bad Gateway 的提示，接口返回 502。并且出现异常的接口不限。具体现象如下图：</p>
<h1 id="3-排查过程"><a href="#3-排查过程" class="headerlink" title="3. 排查过程"></a><strong>3. 排查过程</strong></h1><h2 id="3-1-确认故障点"><a href="#3-1-确认故障点" class="headerlink" title="3.1. 确认故障点"></a><strong>3.1. 确认故障点</strong></h2><p>  出现该问题后，根据研发同学最初提供了思路，建议先去排查下 ti-gateway 的日志看看是否存在错误，以及集群机器负载是否正常。</p>
<h3 id="3-1-1-确认集群负载情况以及-Pod-状态"><a href="#3-1-1-确认集群负载情况以及-Pod-状态" class="headerlink" title="3.1.1. 确认集群负载情况以及 Pod 状态"></a><strong>3.1.1. 确认集群负载情况以及 Pod 状态</strong></h3><p>  首先确认机器负载，通过 grafana 查看各个 Node 的机器负载，目前 CPU&#x2F;内存的使用率没有特别高的，属于正常。该问题可能不是机器负载导致的。</p>
<h3 id="3-1-2-确认-ti-gateway-workload-状态"><a href="#3-1-2-确认-ti-gateway-workload-状态" class="headerlink" title="3.1.2. 确认  ti-gateway-workload 状态"></a><strong>3.1.2. 确认  ti-gateway-workload 状态</strong></h3><p>  查看 gateway 的状态，需要考虑的点：</p>
<p>· gateway的状态是否正常，是否有出现过重启的问题（OOM或者程序异常退出）</p>
<p>· 是否有异常的服务日志</p>
<p>· 如何确认流量是否达到了 ti-gateway</p>
<h4 id="3-1-2-1-确认-gateway-pod-状态"><a href="#3-1-2-1-确认-gateway-pod-状态" class="headerlink" title="3.1.2.1. 确认 gateway pod 状态"></a><strong>3.1.2.1. 确认 gateway pod 状态</strong></h4><p>  查看 gateway 的状态， Pod 未发生过重启，此时可以说明该问题不会是 gateway 程序异常退出或者 OOM 引发的。 </p>
<h4 id="3-1-2-2-确认-gateway-的错误日志"><a href="#3-1-2-2-确认-gateway-的错误日志" class="headerlink" title="3.1.2.2. 确认 gateway 的错误日志"></a><strong>3.1.2.2. 确认 gateway 的错误日志</strong></h4><p>  进入 ti-gateway-workload 服务，查看 &#x2F;usr&#x2F;local&#x2F;apisix&#x2F;logs 目录下的 error.log，没发现相关调用接口的异常日志。</p>
<h4 id="3-1-2-3-排查请求是否有达到-gateway"><a href="#3-1-2-3-排查请求是否有达到-gateway" class="headerlink" title="3.1.2.3. 排查请求是否有达到 gateway"></a><strong>3.1.2.3. 排查请求是否有达到 gateway</strong></h4><p>  从 gateway 的状态和服务日志来看，请求可能没有达到 gateway。需要一些方法去界定流量是否有抵达过 gateway。参考研发同学的 iwiki <a href="/pages/viewpage.action?pageId=596973203"><u><span class="16">Ti网关服务排查手册</span></u></a> 提供的思路进行排查。里面提到一个标准的错误场景，调用接口 5xx 的思路。如下图</p>
<p>  通过该说法，可以推断，如果流量经过网关，那网关响应会重写一个响应头部 <strong>X-TiGateway-Upstream-Status</strong> 通过该头部我们可以去判断流量有没有经过 ti-gateway。如果未产生该头部，则代表流量没达到 gateway 则可能流量没有达到 gateway。当前的响应头部没有  <strong>X-TiGateway-Upstream-Status</strong> 此时可以推断流量没有抵达 gateway，需要向上排查。</p>
<h3 id="3-1-3-确认-alb-状态"><a href="#3-1-3-确认-alb-状态" class="headerlink" title="3.1.3. 确认 alb 状态"></a><strong>3.1.3. 确认 alb 状态</strong></h3><p>  gateway 的上一层应该是 ingress-controller，环境使用的是灵雀 TKE 作为底座，此时需要对 alb 进行排查。</p>
<p>  查看 alb 的状态，需要考虑的点和 gateway 类似：</p>
<p>· gateway的状态是否正常，是否有出现过重启的问题（OOM或者程序异常退出）</p>
<p>· 是否有异常的服务日志，请求日志可以通过 &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log（错误日志） 以及 &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;logs&#x2F;access.log（访问日志）</p>
<p>· 如何确认流量是否达到了 alb2</p>
<h4 id="3-1-3-1-确认-alb-pod-状态"><a href="#3-1-3-1-确认-alb-pod-状态" class="headerlink" title="3.1.3.1. 确认 alb pod 状态"></a><strong>3.1.3.1. 确认 alb pod 状态</strong></h4><p>  查看 alb 状态，发现 global-alb2 虽然状态都是 Running 的，但是 192.168.25.19 以及 192.168.25.5 节点上的 global-alb2 分别出现过重启。</p>
<p>  从容器的状态来看 192.168.25.19 的 alb 是出现过 Exit Code 137（OOM）的，但触发的事件节点是 2022.7.17 的 19:38。而我们当前出现故障的时间节点是 2022.7.18 的下午。初步可以推断不是由于该原因引起的。</p>
<p>  192.168.25.5 的容器也出现过退出，但同样的，时间节点对不上。</p>
<p>通过上述的分析，引发该问题的原因应该不是 alb 程序退出或者 OOM 等原因</p>
<h4 id="3-1-3-2-确认-alb-的错误日志"><a href="#3-1-3-2-确认-alb-的错误日志" class="headerlink" title="3.1.3.2. 确认 alb 的错误日志"></a><strong>3.1.3.2. 确认 alb 的错误日志</strong></h4><p>  此时需要查询该时间节点下，alb 是否有产生过相关日志，进入 global 分别执行过过滤指令，得到的结果如下。（注：当时排查时未截图，其中 10:18:32 是当时请求失败的时间节点。+8 的话就是 CST 时间的 18:18）</p>
<p>&#x2F;var&#x2F;log&#x2F;nginx # grep “10:18:” * | grep 07&#x2F;18</p>
<p>error.log:2022&#x2F;07&#x2F;18 10:18:01 [warn] 53#53: *142405699 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647558, client: 159.75.199.13, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p>
<p>error.log:2022&#x2F;07&#x2F;18 10:18:21 [warn] 53#53: *142407716 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647559, client: 159.75.198.73, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p>
<p>error.log:2022&#x2F;07&#x2F;18 10:18:42 [warn] 53#53: *142408115 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647560, client: 159.75.199.142, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p>
<p>error.log:2022&#x2F;07&#x2F;18 10:18:54 [warn] 53#53: *142408413 a client request body is buffered to a temporary file &#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;client_body_temp&#x2F;0001647561, client: 159.75.198.73, server: _, request: “POST &#x2F;v4&#x2F;callback&#x2F;audits HTTP&#x2F;1.1”, host: “demo-v36.tiplatform.qq.com”</p>
<p>&#x2F;var&#x2F;log&#x2F;nginx #</p>
<p>  通过查询，没有在 2022&#x2F;07&#x2F;18 10:18:32 这个时间看到明显的报错，相关时间给出的都是一些告警信息。同样的命令在其他节点也查询过，得不到关键的信息。</p>
<h4 id="3-1-3-3-分析请求是否有达到-alb"><a href="#3-1-3-3-分析请求是否有达到-alb" class="headerlink" title="3.1.3.3. 分析请求是否有达到 alb"></a><strong>3.1.3.3. 分析请求是否有达到 alb</strong></h4><p>  将所有节点的 access.log 导出进行过滤，在 2022&#x2F;07&#x2F;18 10:18:32 这个时间没有看到请求。推测这个请求并没有达到平台。</p>
<p>[root@VM-25-41-tlinux ~&#x2F;log&#x2F;alb_log]# grep DescribeGroupDevices * | grep 10:18</p>
<p>192.168.25.19_access.log:192.168.25.41 - - [18&#x2F;Jul&#x2F;2022:10:18:57 +0000] “POST &#x2F;gateway?action&#x3D;DescribeGroupDevices HTTP&#x2F;1.1” 200 678 “<a target="_blank" rel="noopener" href="https://demo-v36.tiplatform.qq.com/tim/device">https://demo-v36.tiplatform.qq.com/tim/device</a>“ “Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;103.0.0.0 Safari&#x2F;537.36”</p>
<p>192.168.25.41_access.log:159.75.198.50 - - [18&#x2F;Jul&#x2F;2022:10:18:05 +0000] “POST &#x2F;gateway?action&#x3D;DescribeGroupDevices HTTP&#x2F;1.1” 200 654 “<a target="_blank" rel="noopener" href="https://demo-v36.tiplatform.qq.com/tim/Application/deviceControl">https://demo-v36.tiplatform.qq.com/tim/Application/deviceControl</a>“ “Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;103.0.0.0 Safari&#x2F;537.36”</p>
<p>192.168.25.5_access.log:192.168.25.41 - - [18&#x2F;Jul&#x2F;2022:10:18:07 +0000] “POST &#x2F;gateway?action&#x3D;DescribeGroupDevices HTTP&#x2F;1.1” 200 678 “<a target="_blank" rel="noopener" href="https://demo-v36.tiplatform.qq.com/tim/device">https://demo-v36.tiplatform.qq.com/tim/device</a>“ “Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;103.0.0.0 Safari&#x2F;537.36”</p>
<h3 id="3-1-4-分析问题是否由-WAF-引发"><a href="#3-1-4-分析问题是否由-WAF-引发" class="headerlink" title="3.1.4. 分析问题是否由 WAF 引发"></a><strong>3.1.4. 分析问题是否由 WAF 引发</strong></h3><p>  经过上述的排查，怀疑请求没有达到平台，而是在 WAF 侧直接返回的失败。通过联系腾讯云助手（cloud IT support）咨询。将情况描述以后，得到的确认是该请求有到 WAF，并且 WAF 也提交给了后端，但是后端得到的返回是 502。也就是说该问题<strong>还有节点被遗漏</strong>了，需要继续排查。</p>
<h2 id="3-2-重新梳理流量路径"><a href="#3-2-重新梳理流量路径" class="headerlink" title="3.2. 重新梳理流量路径"></a><strong>3.2. 重新梳理流量路径</strong></h2><p>  基于上述的排查进展，梳理出的情况是有部分流量节点没有去定位，通过（学习）查看 WAF 关联的节点配置，找到了其关联的源站 IP。通过该 IP 成功找到了关联的后端。发现它实际上是关联的 HA VIP。也就是说实际上环境还存在一个 keepalived 的组件在提供 VIP 功能。还需要排查此节点。</p>
<p>  基于上述排查过程，简单总结出的流量链路如下</p>
<p>浏览器 -&gt; DNS -&gt; WAF -&gt; EIP -&gt; HA VIP(Keepalived) -&gt; alb -&gt; ti-gateway -&gt; 业务pod</p>
<p>  每个节点角色功能如下</p>
<p>· 浏览器 – 请求发起者</p>
<p>· DNS – 提供站点域名解析，将请求引入 WAF</p>
<p>· WAF – 提供 WEB 应用防火墙功能，负责拦截 Web 攻击，过滤流量后将请求转发给后端服务</p>
<p>· EIP 公网 IP – 提供公网访问功能</p>
<p>· HA VIP – 虚拟 IP，提供网络层高可用功能结合 TKE 自建 VIP 的 keepalived 使用。实现平台网络层高可用。</p>
<p>· alb – 灵雀自研的 ingress-controller，基于 URL 将请求分发到对应的后端业务 Pod。</p>
<p>· ti-gateway – TI平台自研的网关组件，负责将 TI 业务接口的调用分发到指定的业务 Pod。</p>
<p>· 业务 Pod – 提供业务处理逻辑的进程。</p>
<h2 id="3-3-分析-keepalived-异常"><a href="#3-3-分析-keepalived-异常" class="headerlink" title="3.3. 分析 keepalived 异常"></a><strong>3.3. 分析 keepalived 异常</strong></h2><h3 id="3-3-1-排查-keepalived-日志"><a href="#3-3-1-排查-keepalived-日志" class="headerlink" title="3.3.1. 排查 keepalived 日志"></a><strong>3.3.1. 排查 keepalived 日志</strong></h3><p>  重新梳理流量路径后，需要对 keepalived 进行排查，通过 kubectl logs -n kube-system keepalived-x.x.x.x &gt; keepalived-x.x.x.x.log 获取日志，发现了两个比较醒目的错误，内容如下：</p>
<h1 id="频繁出现的日志，表示网络环境中有重复的-VRID"><a href="#频繁出现的日志，表示网络环境中有重复的-VRID" class="headerlink" title="频繁出现的日志，表示网络环境中有重复的 VRID"></a>频繁出现的日志，表示网络环境中有重复的 VRID</h1><p>Mon Jul 18 10:41:52 2022: (VI_1) ip address associated with VRID 10 not present in MASTER advert : 192.168.25.13</p>
<p>Mon Jul 18 10:41:52 2022: (VI_1) ip address associated with VRID 10 not present in MASTER advert : 192.168.25.13</p>
<h1 id="这一段日志表示出现过切主的情况"><a href="#这一段日志表示出现过切主的情况" class="headerlink" title="这一段日志表示出现过切主的情况"></a>这一段日志表示出现过切主的情况</h1><p>Mon Jul 18 10:41:52 2022: (VI_1) Entering MASTER STATE</p>
<p>Mon Jul 18 10:41:52 2022: (VI_1) ip address associated with VRID 10 not present in MASTER advert : 192.168.25.13</p>
<p>Mon Jul 18 10:41:52 2022: (VI_1) Received advert from 192.168.25.5 with lower priority 100, ours 100, forcing new election</p>
<p>Mon Jul 18 10:41:52 2022: (VI_1) Master received advert from 192.168.25.41 with same priority 100 but higher IP address than ours</p>
<p>Mon Jul 18 10:41:52 2022: (VI_1) Entering BACKUP STATE</p>
<p>  由于日志中发生过切主，而切主的过程中业务是会出现闪断的。推测问题与 keepalived 切主有关系。接下来需要一些方法去验证。</p>
<h3 id="3-3-2-验证问题是否和-keepalived-频繁切主有关"><a href="#3-3-2-验证问题是否和-keepalived-频繁切主有关" class="headerlink" title="3.3.2. 验证问题是否和 keepalived 频繁切主有关"></a><strong>3.3.2. 验证问题是否和 keepalived 频繁切主有关</strong></h3><h4 id="3-3-2-1-通过日志确认"><a href="#3-3-2-1-通过日志确认" class="headerlink" title="3.3.2.1. 通过日志确认"></a><strong>3.3.2.1. 通过日志确认</strong></h4><p>  以下图请求为例，于 GMT 时间的 Mon, 18 Jul 2022 15:12:07 进行的请求失败了。我在同一个时间段内，我通过这个时间节点去查询 gateway、alb 都无法查询到记录。也就是说请求在进入 alb 之前已经丢失了。</p>
<p>  通过查询 keepalived 的日志，可以发现，这个时间段发生了重新选举，也就是切主。这说明切主的过程中可能会造成请求异常。</p>
<h4 id="3-3-2-2-通过请求对比"><a href="#3-3-2-2-通过请求对比" class="headerlink" title="3.3.2.2. 通过请求对比"></a><strong>3.3.2.2. 通过请求对比</strong></h4><p>  通过比较访问 HAVIP 的情况下（就是直接访问 <a target="_blank" rel="noopener" href="https://demo-v36.tiplatform.qq.com/"><u><span class="16">https://demo-v36.tiplatform.qq.com/</span></u></a> ）以及不访问 HAVIP（也就是直接访问对应节点的公网IP），发现同一时间段内，走 HA VIP 的请求是不通的。绕过 HA VIP 的请求是可以正常通的。</p>
<p><strong>正常情况下的请求比较</strong></p>
<p><strong>异常情况下的请求比较</strong></p>
<p>异常情况下有出现强制选举日志</p>
<p>  经此对比，基本上可以实锤问题就是 keepalived 切主引发的。</p>
<h3 id="3-3-3-排查-Keepalived-切主原因"><a href="#3-3-3-排查-Keepalived-切主原因" class="headerlink" title="3.3.3. 排查 Keepalived 切主原因"></a><strong>3.3.3. 排查 Keepalived 切主原因</strong></h3><p>  为了排查切主原因，此时查看 Keepalived 的配置、日志，得出信息如下：</p>
<p>· 自建 VIP 的 Keepalived 配置是使用了非抢占模式，每个 keepalived 配置相同优先级。在初步启动时协商 Keepalived Master，在成员相同优先级的情况下，IP 地址越大，优先级越高。</p>
<p>· Keepalived 使用 &#x2F;etc&#x2F;kubernetes&#x2F;keepalived&#x2F;check.sh 作为 track 条件。目前该脚本的执行情况来看，除非执行时间过长（超出计时器配置），不然是不会返回 1 的。</p>
<p>· 从日志中只能看到切主，并没有提到是因为 track 导致优先级下降导致切主。应该不是 track 的原因引发的。</p>
<p>· 日志时刻在刷新 vrid 冲突问题。理论上 vrid 冲突确实会影响业务。</p>
<p>  基于初步诊断的结果，目前怀疑导致切主的问题是因为 vrid 冲突，需要先解决该问题后持续观察业务是否恢复正常。</p>
<h2 id="3-4-修复-vrid-冲突并验证功能"><a href="#3-4-修复-vrid-冲突并验证功能" class="headerlink" title="3.4. 修复 vrid 冲突并验证功能"></a><strong>3.4. 修复 vrid 冲突并验证功能</strong></h2><h3 id="3-4-1-找到-vrid-冲突源"><a href="#3-4-1-找到-vrid-冲突源" class="headerlink" title="3.4.1. 找到 vrid 冲突源"></a><strong>3.4.1. 找到 vrid 冲突源</strong></h3><p>  刚开始想通过 subnet 判断冲突源，通过查看环境清单，只有一套环境处于同一个 subnets。推测修改该环境 vrid 就可以解决问题。</p>
<p>  此时通过在集群任意节点上，执行 tcpdump vrrp 指令，这时候发现冲突的 vrid 有很多套环境。。。。所以这个时候还是得修改演练环境的 vrid。</p>
<h3 id="3-4-2-修改-keepalived-的-vrid"><a href="#3-4-2-修改-keepalived-的-vrid" class="headerlink" title="3.4.2. 修改 keepalived 的 vrid"></a><strong>3.4.2. 修改 keepalived 的 vrid</strong></h3><p>  在 global 3 台 master 分别修改配置文件 vim &#x2F;etc&#x2F;kubernetes&#x2F;keepalived&#x2F;keepalived.conf</p>
<p>vrrp_instance VI_1 {</p>
<p>    state BACKUP</p>
<p>    interface eth0</p>
<p>    virtual_router_id 250 #将此处的 virtual_router_id 从 10 替换成 250.</p>
<p>    priority 100</p>
<p>    advert_int 1</p>
<p>    authentication {</p>
<p>        auth_type PASS</p>
<p>        auth_pass 1111</p>
<p>    }</p>
<p>    virtual_ipaddress {</p>
<p>        192.168.25.13</p>
<p>    }</p>
<p>    track_script {</p>
<p>        check</p>
<p>    }</p>
<p>}</p>
<p>  重启 keepalived 服务</p>
<h1 id="获取-docker-id"><a href="#获取-docker-id" class="headerlink" title="获取 docker id"></a>获取 docker id</h1><p>[root@vm-25-19-tlinux ~]# docker ps -a | grep keepalived</p>
<p>417092ae5316   c3efc585784a                                                          “&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;kee…”   4 months ago   Up 31 hours                           k8s_keepalived_keepalived-192.168.25.19_kube-system_c915363bfce6a644ac7c8e054aeae619_7</p>
<p>4f22d8a93303   192.168.25.13:60080&#x2F;tkestack&#x2F;pause:3.2                                “&#x2F;pause”                 4 months ago   Up 4 months                           k8s_POD_keepalived-192.168.25.19_kube-system_c915363bfce6a644ac7c8e054aeae619_1</p>
<h1 id="重启-keepalived-容器"><a href="#重启-keepalived-容器" class="headerlink" title="重启 keepalived 容器"></a>重启 keepalived 容器</h1><p>docker restart 417092ae5316</p>
<p>  重启服务后，查看 keepalived 日志，未出现过异常日志。</p>
<h1 id="4-排查过程中的一些思考"><a href="#4-排查过程中的一些思考" class="headerlink" title="4. 排查过程中的一些思考"></a><strong>4. 排查过程中的一些思考</strong></h1><h2 id="4-1-怎么快速抓-VRRP-包，查看-vrid-冲突源？"><a href="#4-1-怎么快速抓-VRRP-包，查看-vrid-冲突源？" class="headerlink" title="4.1. 怎么快速抓 VRRP 包，查看 vrid 冲突源？"></a><strong>4.1. 怎么快速抓 VRRP 包，查看 vrid 冲突源？</strong></h2><p>通过在集群任意节点上，执行 tcpdump vrrp 指令即可。如果出现多个IP使用了相同的 vrid，代表他们之间 vrid 是冲突的。类似输出如下图。</p>
<p>如果希望统计有哪些环境出现了冲突，可以将捕获结果导入到 wireshark，通过 统计 -&gt; Ipv4 Staticitsc -&gt; All Adress 可以获取本次捕获所有包的 IP 地址列表。通过显示过滤器，使用过滤条件 vrrp.virt_rtr_id &#x3D;&#x3D; VRID 可以筛选出需要确认冲突的 IP。类似如下的输出。</p>
<p>  基于 VRRP 的原理，只有 Master 节点会主动给成员发心跳报文。而一个 VRRP 实例代表一个 Keepalived，也就是一套环境。通过这个方法就可以得出有哪些环境存在冲突需要修复。</p>
<h2 id="4-2-为什么-ifconfig-看不到-keepalived-这个设备？"><a href="#4-2-为什么-ifconfig-看不到-keepalived-这个设备？" class="headerlink" title="4.2. 为什么 ifconfig 看不到 keepalived 这个设备？"></a><strong>4.2. 为什么 ifconfig 看不到 keepalived 这个设备？</strong></h2><p>  在验证问题时想查看 keepalived 网口的信息，此时发现了一个现象，通过 ifconfig 是没有办法直接显示 keepalived 的。但是 ifconfig keepalived 可以输出结果。输出如下：</p>
<h2 id="4-3-为什么在-VRID-冲突的情况下，环境依然可用？"><a href="#4-3-为什么在-VRID-冲突的情况下，环境依然可用？" class="headerlink" title="4.3. 为什么在 VRID 冲突的情况下，环境依然可用？"></a><strong>4.3. 为什么在 VRID 冲突的情况下，环境依然可用？</strong></h2><p>  keepalived 是怎么知道 vrid 冲突的，VRRP报文里面有携带 vrid 和 vip 字段，通过判断本地配置文件的 vrid 和 vip 和报文中是否出现 vrid 一样但 vip 不同的报文即可。</p>
<p>  理论上 keepalived 的虚拟 MAC 地址是通过 vrid 去决定的，如果两组 vrrp 实例使用了相同的 vrid，那么结果就是他们协商通过以后会发送免费 ARP 报文，MAC 地址相同。此时对于物理网络来说应该会产生 MAC 地址漂移的问题。对外体现就是网络不稳定，通信时断时续。</p>
<p>  目前原因还在确认，从自己抓包测试的结果初步推断和腾讯云实现虚拟网络的机制(vpc.ko) 有关，目前还在确认中。中间可能有一层网络封装屏蔽了 MAC 冲突的问题。最后呈现的结果就是虽然有 VRID 冲突，但是业务还是可用。</p>
<h2 id="4-4-VRID-冲突会导致切主？"><a href="#4-4-VRID-冲突会导致切主？" class="headerlink" title="4.4. VRID 冲突会导致切主？"></a><strong>4.4. VRID 冲突会导致切主？</strong></h2><p>  当前通过配置修改验证，解决了 VRID 冲突问题以后，偶发切主现象确实消失了。具体原因还没有理出来。</p>
<h2 id="4-5-请求回包的过程中，keepalived-切主是否会有影响？"><a href="#4-5-请求回包的过程中，keepalived-切主是否会有影响？" class="headerlink" title="4.5. 请求回包的过程中，keepalived 切主是否会有影响？"></a><strong>4.5. 请求回包的过程中，keepalived 切主是否会有影响？</strong></h2><p>  当前环境 keepalived 的 ipvs 工作模式是 nat 模式，进出流量都会经过 keepalived 配置的 ipvs。在进行转发作业的时候 lvs 会维护一张连接表。如果说中途出现 keepalaived 切主的情况。新的主 keepalived 对这些连接信息存在丢失。此时请求&#x2F;响应链路应该会中断一会。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ipvs 配置</span></span><br><span class="line">virtual_server 192.168.25.90 60080 &#123;</span><br><span class="line">    delay_loop 10</span><br><span class="line">    lb_algo rr</span><br><span class="line">    lb_kind NAT  <span class="comment"># 使用了 NAT 模式</span></span><br><span class="line">    protocol TCP</span><br><span class="line">    real_server 192.168.25.127 60080 &#123;</span><br><span class="line">        weight 1</span><br><span class="line">        HTTP_GET &#123;</span><br><span class="line">            url &#123;</span><br><span class="line">                path /</span><br><span class="line">            &#125;</span><br><span class="line">            connect_timeout 10</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>  实际上我在排查过程中，是遇到了两种场景，一种是请求没有到 gateway（如上文 2~3 的过程，另一种情况是请求已经到了 gateway 并且 gateway 也转给了业务 Pod，业务 Pod 也有正常返回。但最终结果也是 502。也就是说切主确实会对回程流量造成影响。</p>
<h2 id="4-6-WAF-是怎么工作的？"><a href="#4-6-WAF-是怎么工作的？" class="headerlink" title="4.6. WAF 是怎么工作的？"></a><strong>4.6. WAF 是怎么工作的？</strong></h2><p>  具体可参考腾讯云官网的示例 <a target="_blank" rel="noopener" href="https://cloud.tencent.com/document/product/627/39843">Web 应用防火墙 产品分类-产品简介-文档中心-腾讯云</a>，当前演示环境配置的是 SaaS 型的，总体逻辑如下</p>
<p><img src="C:\Users\89715\AppData\Roaming\marktext\images\2023-08-26-19-36-48-1693049798627.jpg"></p>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h1><h2 id="5-1-思路"><a href="#5-1-思路" class="headerlink" title="5.1. 思路"></a><strong>5.1.</strong> <strong>思路</strong></h2><p>  后续如果遇到类似问题，可优先尝试以下思路去定位：</p>
<ul>
<li><p>可以先通过响应头部是否有携带 <strong>X-ABC-Upstream-Status</strong> 来定界下范围，如果有携带那应该是后端服务有异常，优先查看后端服务。如果没有携带可能是请求没抵达，也可能是中途因为其他原因连接丢失。</p>
</li>
<li><p>如果组件找不到请求的日志，或者有收到请求但是有正常响应，页面接口报 502，大概率是中间某个节点触发了主从切换。可以往带主从切换的组件优先进行排查。</p>
</li>
<li><p>梳理网络拓扑，确认入&#x2F;出流量的路径。再基于每个流量节点进行排查。类似场景可参考 5.2、5.3 的方法进行确认。</p>
</li>
</ul>
<h2 id="5-2-演示环境网络路径"><a href="#5-2-演示环境网络路径" class="headerlink" title="5.2. 演示环境网络路径"></a><strong>5.2.</strong> <strong>演示环境网络路径</strong></h2><p>  <strong>拓扑</strong></p>
<p>  <strong>每个节点角色功能如下</strong></p>
<ul>
<li><p>浏览器 – 请求发起者</p>
</li>
<li><p>DNS – 提供站点域名解析，将请求引入 WAF</p>
</li>
<li><p>WAF – 提供 WEB 应用防火墙功能，负责拦截 Web 攻击，过滤流量后将请求转发给后端服务</p>
</li>
<li><p>EIP 公网 IP – 提供公网访问功能</p>
</li>
<li><p>HA VIP – 虚拟 IP，提供网络层高可用功能结合 TKE 自建 VIP 的 keepalived 使用。实现平台网络层高可用。</p>
</li>
<li><p>alb –  ingress-controller，基于 URL 将请求分发到对应的后端业务 Pod。</p>
</li>
<li><p>内部 gateway – 自研的网关组件，负责将业务接口的调用分发到指定的业务 Pod。</p>
</li>
<li><p>业务 Pod – 提供业务处理逻辑的进程。</p>
</li>
</ul>
<h2 id="5-3-各环节排查思路-处理方法"><a href="#5-3-各环节排查思路-处理方法" class="headerlink" title="5.3. 各环节排查思路&#x2F;处理方法"></a><strong>5.3.</strong> <strong>各环节排查思路&#x2F;处理方法</strong></h2><p>  每个环节的排查&#x2F;处理思路</p>
<table>
<thead>
<tr>
<th><strong>排查节点</strong></th>
<th><strong>排查以及处理思路</strong></th>
</tr>
</thead>
<tbody><tr>
<td>客户端浏览器&#x2F;Postman 等程序</td>
<td>1. 如果是接口偶现 502，可能是类似的问题，需要先确认下 Response Headers 有没有带 <strong>X-ABC-Upstream-Status</strong>。2. 也有可能是客户端自身问题，比如配置或者网络环境质量不好。如果有多个用户反馈类似的情况。这种场景应该是平台侧的问题。需要从平台侧找原因。</td>
</tr>
<tr>
<td>DNS</td>
<td>1. 通常情况下不会是它的原因。域名访问不通，平台是无法访问的。2. 如果出现了异常，可以确认近期是否有进行过配置变更 3. 联系腾讯云小助手支持</td>
</tr>
<tr>
<td>WAF</td>
<td>1. 同 DNS 的处理思路，如果需要访问日志等信息可联系腾讯云小助手支持。联系支持时需要提供接口调用的具体时间、请求的 URL。小助手可以提供对应的访问日志。</td>
</tr>
<tr>
<td>EIP</td>
<td>1. 同 DNS 的处理思路。通常情况下应该不是它的原因</td>
</tr>
<tr>
<td>HA VIP</td>
<td>1. 查看日志，相关接口报错的时间点内是否发生过切主。可通过 kubectl logs -n kube-system keepalived-x.x.x.x 获取日志。2. 日志中是否有产生重复 vrid 的提示，如果有则需要修复。此时可以通过 tcpdump vrrp 的方法快速确认冲突源。</td>
</tr>
<tr>
<td>alb（ingress-controller）</td>
<td>1. 确认 alb 状态是否正常，近期状态是否有异常退出、重启等情况2. 确认 alb 的请求日志和错误日志。进入 alb 容器访问请求日志路径</td>
</tr>
<tr>
<td>内部 gateway</td>
<td>1. 组件找不到请求的日志，应该是上面其他环节存在异常。2. 组件可以找到请求日志，从后端服务也能找到正常处理的日志，此时可能是上面有个环节存在异常。比如 keepalived 切主等。建议在上面其他环节进行排查。</td>
</tr>
<tr>
<td>业务 Pod</td>
<td>1. 确认业务 Pod 状态是否为 Running 并就绪。2. 确认业务 Pod 是否有产生过重启记录（重启计数器不为 0），重启的时间节点以及退出码。</td>
</tr>
<tr>
<td>主机</td>
<td>1. 通过 grafana 查看监控，集群各主机状态是否正常，负载是否正常。</td>
</tr>
</tbody></table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/01/31/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%9D%82%E8%AE%B0%E4%B8%80%EF%BC%9A%E6%A6%82%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/31/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%9D%82%E8%AE%B0%E4%B8%80%EF%BC%9A%E6%A6%82%E5%BF%B5/" class="post-title-link" itemprop="url">边缘计算杂记一：概念</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-01-31 23:19:11" itemprop="dateCreated datePublished" datetime="2024-01-31T23:19:11+08:00">2024-01-31</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>什么是边缘计算？它具备什么价值？</strong></p>
<p>  边缘计算（Edge Computing）是一种分布式计算的架构，将计算和数据处理的能力推向离数据源和用户更近的边缘设备上。边缘设备可以是工控机、路由器、网关、传感器等，这些设备都有一定的计算、存储和网络连接能力。</p>
<p>  边缘计算的价值在于：</p>
<ul>
<li><p>减少数据传输延迟：边缘设备位于数据源和用户之间，可以避免长距离的数据传输，减少了数据传输延迟。</p>
</li>
<li><p>提供实时决策和响应：由于边缘设备的计算和数据处理能力强大，可以实时对数据进行处理和分析，对数据进行过滤、聚合、计算等，快速做出决策和响应。</p>
</li>
<li><p>提高网络安全性：由于数据处理在边缘设备上进行，有助于减少对云端和网络的依赖性，从而提高系统的安全性和可靠性。</p>
</li>
<li><p>降低云计算成本：通过使用边缘计算，可以将计算和数据处理从云端下移，减少云计算的负载和成本。</p>
</li>
<li><p>支持物联网应用：边缘计算的应用场景非常适合物联网领域，因为物联网涉及到大量的传感器和设备，需要对这些设备产生的数据进行实时监控和分析，并做出实时决策和调整。</p>
<p>边缘计算的典型应用包括自动驾驶、智能家居、智能制造、智慧城市等领域，在这些应用场景中，边缘计算可以快速响应用户需求，处理实时数据并进行决策。这种分布式计算架构的优势在于提高了系统的性能和效率，并能够更好地满足复杂的实时业务需求。</p>
</li>
</ul>
<p><strong>边缘计算的架构大概是怎么样的？</strong></p>
<p>  边缘计算的架构通常包括以下几个主要组件：</p>
<ol>
<li><p>边缘设备（Edge Devices）：边缘设备是指部署在离数据源和用户更近的位置上的设备，例如传感器、工控机、路由器、智能手机等。这些设备具备一定的计算、存储和网络连接能力，能够处理和分析数据，并执行一些简单的计算任务。</p>
</li>
<li><p>边缘节点（Edge Nodes）：边缘节点是一组连接在一起的边缘设备，它们协同工作，共同处理和管理数据。边缘节点可以是一个集群、网关或者边缘服务器等。边缘节点负责接收、存储、处理和转发来自边缘设备的数据，并与云端或中心节点进行通信。</p>
</li>
<li><p>云端或中心节点（Cloud or Central Nodes）：云端或中心节点是位于数据中心或云平台的中央服务器，负责接收、存储和管理从边缘节点传输过来的数据。云端或中心节点具备强大的计算和存储能力，可以执行复杂的数据分析和处理任务。</p>
</li>
<li><p>网络连接（Network Connectivity）：边缘计算依赖于可靠的网络连接，用于连接边缘设备、边缘节点和云端或中心节点之间的数据传输。可以利用有线网络（如以太网）或者无线网络（如Wi-Fi、蜂窝网络等）来实现连接。</p>
</li>
<li><p>边缘计算软件平台（Edge Computing Software Platform）：边缘计算软件平台提供了管理和协调边缘设备、边缘节点和云端节点之间通信和数据处理的功能。它包括边缘节点操作系统、容器化技术、边缘计算框架等组件，帮助实现数据的分发、处理、存储和安全管理。</p>
</li>
</ol>
<p>在边缘计算架构中，边缘设备收集和生成数据，通过边缘节点进行本地处理和决策，然后将关键数据传输到云端或中心节点进行进一步的分析和计算。这种分布式计算架构能够提高系统的性能和效率，减少数据传输延迟，并支持实时的决策和响应。</p>
<p><strong>边缘计算和 Kubernetes 存在什么关联？</strong></p>
<ol>
<li><p>边缘计算是一种分布式计算模型，它将计算、存储和网络功能推近到数据产生源头和最终用户之间，以提供更低延迟、更高带宽和更好的用户体验。边缘计算可以将计算资源放置在离用户或设备更近的位置，例如工厂、物流中心、智能城市等地方。</p>
</li>
<li><p>Kubernetes（简称K8s）是一个用于自动化容器化应用程序部署、扩展和管理的开源平台。它提供了管理容器化应用程序所需的资源调度、服务发现、负载均衡、弹性伸缩等功能。Kubernetes通过使用容器技术，如Docker，可以简化应用程序的部署和管理，并提供高可用性和弹性的运行环境。</p>
</li>
<li><p>边缘计算和Kubernetes结合在一起，可以实现在边缘设备上部署和管理容器化应用程序的能力。通过在边缘节点上运行Kubernetes集群，可以将容器化的应用程序部署到边缘设备中，实现边缘计算的灵活性和可扩展性。</p>
</li>
<li><p>在边缘计算环境中，通过Kubernetes可以实现容器化应用程序的自动部署、资源调度和服务发现。Kubernetes可以根据边缘设备的资源状况进行自动伸缩，确保应用程序在边缘节点上的高可用性和性能。</p>
</li>
</ol>
<p><strong>边缘计算和 CDN 的关系？</strong></p>
<p>  个人理解边缘计算和 CDN 都属于解决方案，它们之间非常相似。下面是我找到的一个 CDN 与边缘计算联合使用的案例：</p>
<p>  假设有一家电子商务公司，他们的在线平台需要快速响应用户的请求并提供高质量的服务。用户遍布全球各地，访问平台的内容包括网页、图片、视频等。这家电子商务公司决定采用CDN来优化内容传输和加速用户访问体验。他们与CDN服务提供商合作，在全球范围内建立了许多节点服务器，这些节点服务器位于离用户最近的位置。</p>
<p>  当用户请求访问该电子商务平台时，CDN会根据用户的位置选择距离最近的节点服务器来提供所需内容。CDN节点服务器具有缓存功能，如果所请求的内容已经存在于某个节点服务器上，CDN可以直接从该节点服务器中返回内容，而无需通过远程的主要服务器。这样可以大大减少数据传输的延迟，加快内容的加载速度。</p>
<p>  此外，这些CDN节点服务器也可以提供边缘计算的能力。例如，当用户请求访问一个动态生成的页面时，CDN节点服务器可以在接收到请求后执行一些简单的计算任务，如数据处理、模板渲染等，以生成所需的页面，并将结果返回给用户。这样，不仅减少了从主要服务器到用户的数据传输量，还可以提供更快速的响应时间。</p>
<p>  CDN作为边缘计算节点发挥了双重作用。它们不仅通过缓存和分发内容来加速内容传输，提高用户访问体验，而且通过执行一些简单的边缘计算任务，减少了数据传输延迟，并提供更快速的响应。</p>
<p>边缘计算平台，大概需要解决哪些问题？</p>
<p>  如果应用需要在边缘计算的场景下使用，对提供边缘计算能力的平台需要解决下面这几个问题：</p>
<ul>
<li><p>云边通信，边端通常没有固定IP，云端怎么去管控它们？云端又怎么去访问边端上的服务？</p>
</li>
<li><p>边缘自治，边端和云端连通性出现异常时，边端怎么保障还能正常的对外提供服务？</p>
</li>
<li><p>边缘部署，边缘节点间可能存在差异（如ARM、x86 或者服务配置有一些不同），怎么快速把应用发布在边缘节点。</p>
</li>
</ul>
<p>那这里可以通过什么技术或者手段去实现这些能力？</p>
<p><strong>云边通信</strong></p>
<p>  云边通信需要解决的问题是边端没有固定IP，这使得传统的 ssh 主动去连接边缘的机器是无法做到的。边端虽然没办法被云端主动去连接，但可以反过来，让边端主动去连接云端，这时候就可以实现云边通信了。</p>
<p>  所以这里可以设计一个程序，实现这个反向隧道。来解决掉云端和边端之间的通信问题。如 superedge 就是通过 tunnel-cloud、tunnel-edge 组件来实现的反向隧道。让云端可以实现边端管理。</p>
<p><strong>边缘自治</strong></p>
<p>  边缘自治边缘自治，笔者的理解是边缘节点能够自主的去进行决策和协作。实际上这里的实现需要和边缘上的应用实现相关联。我这里只讨论边缘计算的平台（基于 Kubernetes）要怎么实现基础的边缘自治能力。</p>
<p>  如果只考虑到 Kubernetes 这一层次，假设云端和边端节点之间的网络不稳定，出现了几分钟断连的情况，这时候基于 Kubernetes 自身的机制，节点需要每隔一段时间进行上报的动作。当上报失败次数到某个次数，Kubernetes 会认为节点不健康，将状态设置为 NotReady，并驱逐节点上的所有服务。在其他节点上拉起新的工作副本。</p>
<p>  对于边缘计算的场景下，这个设计其实是不可靠的，毕竟边端节点可能没有异常，它只是和云端存在网络通信问题。边端节点自身还是能够给边缘设备提供访问能力的。所以这时候边端节点不能只依赖 Kubernetes 原生的健康检查机制。还需要依赖一些分布式的检查方式。比如某个站点下的边缘节点异常了。那该站点下的其他节点可以和它进行连通性测试。确保该节点只是和云端通信存在异常。本身能力还是可以继续对外提供的。不进行驱逐。</p>
<p>  另外当云端无法访问了，边端节点如果依赖云端的一些个性化数据无法获取到，也可能会造成业务影响，所以边端节点自身也需要去实现一层类似缓存、同步的机制。比如在本地拉起一个简易的数据库程序。将平常访问到云端的一些数据、在脱离云端通信过程中执行的一些决策数据等缓存到本地数据库中。等恢复后再执行同步。</p>
<p><strong>边缘部署</strong></p>
<p>  基于实际环境应用去考虑，边缘部署可能要解决这几个点：</p>
<ul>
<li><p>环境可能存在站点的概念，比如环境下存在非常多的节点。某些边缘节点属于一个站点，某些节点属于另一个站点，我一个边缘应用在同时发布在若干个站点。怎么快速去部署？</p>
</li>
<li><p>部署过程中可能存在差异化的配置，比如边缘节点的 CPU 架构可能是不一致的，比如同时存在 x86 和 arm 架构，那这个时候部署的时候怎么去快速渲染差异化的配置。</p>
<p>那这里就是说要解决站点间部署的差异性，以及快速将应用部署在若干站点。这里就需要 Kubernetes 这一层再去设计一些额外的 CR 资源来实现，以 superedge 为例子。它设计了 NodeUnit、NodeGroup、ServiceGroup 的概念。</p>
<p>我们可以将具备相同属性（例子中就是同一站点）的节点配置划分到 NodeUnit。这样不同站点间它们就归属在不同的 NodeUnit，所有的 NodeUnit 都归属于一个 NodeGroup。然后基于 NodeGroup 可以创建对应的 deploymentGrid。在 deploymentGrid 中可以配置 poolTemplate，将每个不同 NodeUnit 的差异化配置进行设置。实现差异化部署。之后再基于 NodeGroup 可以创建 ServiceGroup。去自动给这些 NodeUnit 部署的应用创建 service。来实现对外服务。</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/01/26/go%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E7%8E%AF%E5%A2%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/26/go%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E7%8E%AF%E5%A2%83/" class="post-title-link" itemprop="url">kubernetes go开发环境安装</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-01-26 20:11:25" itemprop="dateCreated datePublished" datetime="2024-01-26T20:11:25+08:00">2024-01-26</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>337</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="go-安装"><a href="#go-安装" class="headerlink" title="go 安装"></a>go 安装</h2><p>wget <a target="_blank" rel="noopener" href="https://dl.google.com/go/go1.21.6.linux-amd64.tar.gz">https://dl.google.com/go/go1.21.6.linux-amd64.tar.gz</a> tar xvf go1.21.6.linux-amd64.tar.gz<br>mv go &#x2F;usr&#x2F;local&#x2F;go<br>echo “PATH&#x3D;PATH:&#x2F;usr&#x2F;local&#x2F;go&#x2F;bin” &gt;&gt; &#x2F;etc&#x2F;profile<br>source &#x2F;etc&#x2F;profile</p>
<h2 id="使用国内代理"><a href="#使用国内代理" class="headerlink" title="使用国内代理"></a>使用国内代理</h2><p>go env -w GOPROXY&#x3D;<a target="_blank" rel="noopener" href="https://goproxy.cn,direct/">https://goproxy.cn,direct</a></p>
<p>go env -w GO111MODULE&#x3D;auto</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/01/23/kubectl%20logs%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E5%B7%A5%E5%85%B7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/23/kubectl%20logs%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E5%B7%A5%E5%85%B7/" class="post-title-link" itemprop="url">基于资源对应 pod 批量执行 kubectl logs 采集工具</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-01-23 20:11:25" itemprop="dateCreated datePublished" datetime="2024-01-23T20:11:25+08:00">2024-01-23</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>  在交付运维阶段，有时候在日志平台还没有就绪，或者日志平台有问题情况下，我们需要对 Pod 进行诊断，这时候可能需要去读取 pod 的标准输出。但是一般情况下环境的 Pod 都是多副本高可用的，服务的请求可能会随机落到任意 Pod。对此通常的解决方法是拉多个窗口去观测，这种方式不大方便。这里就通过开发一个简易日志采集脚本来简化改工作。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>  实际上我只需要获取 pod 的清单，然后挨个 pod 执行 kubectl logs 的指令，并挂在后台。然后再通过 tail 命令去获取所有 log 的输出即可。本身 tail 对多个文件的输出是自带了 title。可以简化工作量。</p>
<p>  获取 pod 清单的方式是让使用者输入 pod 控制器的类型（deployment、daemonset等等或者简写），命名空间，对应控制器资源的名称，每个控制器都是通过标签选择来判断 pod 是否在集群中满足数量。所以这里通过获取控制器的 matchlabels 来生成判断条件。后续通过 kubectl get pod -l 标签的方式就能直接拿到清单了。</p>
<p>  考虑到日志的展示本身就是需要生成临时日志文件，并事后可能需要保存，或者多次执行，脚本会自动创建一个命名空间+资源名称的目录，来存储日志文件。并加入一些删除的逻辑，强制退出脚本或者启动脚本时都会检查 kubectl logs 的进程还有没有运行在环境，如果有就删除残留的进程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">print_log()</span><br><span class="line">&#123;</span><br><span class="line">    log_level=$1</span><br><span class="line">    log_msg=$2</span><br><span class="line">    currentTime=&quot;$(date &#x27;+%F %T&#x27;)&quot;</span><br><span class="line">    echo &quot;$currentTime    [$log_level]    $log_msg&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">clear() &#123;</span><br><span class="line">    if [ -f clearlist ]; then</span><br><span class="line">        while IFS= read -r line; do</span><br><span class="line">          if [ $(ps aux | grep &quot;$&#123;line&#125;&quot; | wc -l) -gt 1 ] ; then</span><br><span class="line">            ps aux | grep &quot;$&#123;line&#125;&quot; | grep -v grep | awk &#x27;&#123; print $2 &#125;&#x27; | xargs kill</span><br><span class="line">          fi</span><br><span class="line">        done &lt; clearlist</span><br><span class="line">    fi</span><br><span class="line">    rm -f clearlist</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ctrl_c() &#123;</span><br><span class="line">    clear</span><br><span class="line">    print_log &quot;INFO&quot; &quot;Is k8s_log_collector.sh: line xx:  xxxx Terminated appeared, please ingore.&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">trap ctrl_c INT</span><br><span class="line"></span><br><span class="line">baseName=$(basename &quot;$0&quot;)</span><br><span class="line"></span><br><span class="line">if [ $# -lt 3 ]; then</span><br><span class="line">    print_log &quot;INFO&quot; &quot;usage: $baseName resource ns name&quot;</span><br><span class="line">    print_log &quot;INFO&quot; &quot;Example:&quot;</span><br><span class="line">    print_log &quot;INFO&quot; &quot;  $baseName ds kube-flannel kube-flannel-ds&quot;</span><br><span class="line">    print_log &quot;INFO&quot; &quot;  $baseName deploy default nginx&quot;</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if ! kubectl get $1 -n $2 $3 &amp;&gt; /dev/null; then</span><br><span class="line">    echo &quot;resource not exits.&quot;</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">log_path=log_collect/$2/$1_$2_$3/</span><br><span class="line">if [ -d log_collect/$2 ]; then</span><br><span class="line">    clear</span><br><span class="line">    mv log_collect/$2 log_collect/$2_$(date +%Y%d%H%M%T)</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">mkdir -p $&#123;log_path&#125;</span><br><span class="line"></span><br><span class="line">labels=$(kubectl get $1 -n $2 $3 -o jsonpath=&quot;&#123;.spec.selector.matchLabels&#125;&quot;  | sed &#x27;s/&#123;//&#x27; | sed &#x27;s/&#125;//&#x27; | sed &#x27;s/&quot;//g&#x27; | sed &#x27;s/:/=/g&#x27;)</span><br><span class="line"></span><br><span class="line">podlist=$(kubectl get pod -n $2 -l $&#123;labels&#125; --no-headers | awk &#x27;&#123; print $1 &#125;&#x27;)</span><br><span class="line"></span><br><span class="line">for pod in $&#123;podlist&#125;</span><br><span class="line">do</span><br><span class="line">    echo &quot;kubectl logs -n $2 $&#123;pod&#125;&quot; &gt;&gt; clearlist</span><br><span class="line">    kubectl logs -n $2 $&#123;pod&#125; -f &amp;&gt; $&#123;log_path&#125;/$&#123;pod&#125;.log &amp;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">print_log &quot;INFO&quot; &quot;Log collection started, wait for 2 seconds.&quot;</span><br><span class="line">sleep 2</span><br><span class="line"></span><br><span class="line">tail -f $&#123;log_path&#125;/*</span><br></pre></td></tr></table></figure>

<p>  效果预览：</p>
<img src="/2024/01/23/kubectl%20logs%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E5%B7%A5%E5%85%B7/1.png" class="" title="1.png">

<p>  目前初版只是简单的实现了功能，整体逻辑和功能还比较搓。后续代码在 orca-tools 中继续维护。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/01/20/prometheus%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/20/prometheus%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%BF%B0/" class="post-title-link" itemprop="url">Prometheus入门（一）概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-01-20 18:00:15" itemprop="dateCreated datePublished" datetime="2024-01-20T18:00:15+08:00">2024-01-20</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>830</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Prometheus-是什么"><a href="#Prometheus-是什么" class="headerlink" title="Prometheus 是什么?"></a>Prometheus 是什么?</h1><p>  Prometheus 是一个开源的系统监控和告警工具，用于收集、存储和查询时间序列数据，并提供灵活的告警机制。它采用拉取方式从目标系统中收集指标数据，支持多维度的标签和时间戳。Prometheus 提供了强大的查询语言PromQL，可用于灵活地查询和分析时间序列数据。</p>
<h1 id="Prometheus-主要特点"><a href="#Prometheus-主要特点" class="headerlink" title="Prometheus 主要特点"></a>Prometheus 主要特点</h1><ul>
<li>多维数据模型：使用指标名称和键值对来标识时间序列数据。</li>
<li>提供 PromQL，一种灵活的查询语言，可以利用这种多维特性进行查询。</li>
<li>不依赖分布式存储：单个服务器节点具有自治性。</li>
<li>通过HTTP的 Pull 模型进行时间序列数据的收集。</li>
<li>支持通过中间网关进行时间序列数据的推送。</li>
<li>支持动态服务发现和静态配置抓取目标。</li>
<li>支持多种图形和仪表盘模式。</li>
</ul>
<h2 id="Prometheus-组件"><a href="#Prometheus-组件" class="headerlink" title="Prometheus 组件"></a>Prometheus 组件</h2><p>Prometheus生态系统由多个组件组成，其中许多是可选的，如下：</p>
<ul>
<li>Prometheus Server，用于抓取和存储时间序列数据</li>
<li>用于为应用程序代码进行仪表化的客户端库</li>
<li>用于支持短期作业（如 K8S 的 job）的 pushgateway</li>
<li>用于特定目的的 exporters，例如：HAProxy、StatsD、Graphite 等</li>
<li>用于处理警报的 alertmanager</li>
<li>其他支持工具</li>
</ul>
<p>Prometheus 架构图（来源自官网）</p>
<p><img src="https://prometheus.io/assets/architecture.png" alt="Prometheus architecture"></p>
<p>什么场景适合用 Prometheus？</p>
<ul>
<li><p>云原生场景</p>
</li>
<li><p>采集的目标数据不需要长期存储，比如业务运行状态，用户更多可能关注的是近期内应用的状态，长期的数据可以做历史归档。不需要精细的存储监控指标。</p>
</li>
</ul>
<p>什么场景不适合用 Prometheus？</p>
<ul>
<li><p>需要 100% 准确性的监控采集场景，如服务请求的计费数据不适用 Prometheus。Prometheus 更多是实现的趋势精准。</p>
</li>
<li><p>简单的系统应用场景，或者说传统的一些监控场景，更适合使用 zabbix 类的监控组件去实现。</p>
</li>
<li><p>Prometheus 不用于日志存储的场景，日志存储应该使用 loki、elk。也不应用于链路追踪的场景。</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2024/01/16/kubernetes%20%E5%8D%95%E8%8A%82%E7%82%B9%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/16/kubernetes%20%E5%8D%95%E8%8A%82%E7%82%B9%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/" class="post-title-link" itemprop="url">Kubernetes 快速拉起单节点集群</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-01-16 18:11:25" itemprop="dateCreated datePublished" datetime="2024-01-16T18:11:25+08:00">2024-01-16</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>  考虑到近期会做多 Kubernetes 集群相关的一些验证，需要频繁创建单机版的 K8S 集群。需要编写一个简单的自动化 K8S 安装脚本。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>  基于社区 Kubernetes 提供的安装步骤，做了一下简单的脚本自动化封装。初版内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用于实现单节点部署 K8S，仅用于部署调试、开发环境，验证于 CentOS 7.9 环境</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">部署后使用默认 K8S CIDR，机器主机名称修改为 K8S-Master01，并且安装 flannel。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">适用于国区</span></span><br><span class="line"></span><br><span class="line">init_repo() &#123;</span><br><span class="line">    cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key</span><br><span class="line">EOF</span><br><span class="line">    yum -y install yum-utils</span><br><span class="line">    yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">install_pkg() &#123;</span><br><span class="line">    yum -y install kubelet kubeadm containerd kubectl</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">config_ntp() &#123;</span><br><span class="line">    yum -y install chrony</span><br><span class="line">    sed -i &#x27;/server/ d&#x27; /etc/chrony.conf</span><br><span class="line">    echo &quot;server ntp1.aliyun.com iburst&quot; &gt;&gt; /etc/chrony.conf</span><br><span class="line">    systemctl enable chronyd</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">crictl_init() &#123;</span><br><span class="line">    cat &gt; /etc/crictl.yaml &lt;&lt;EOF</span><br><span class="line">runtime-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">image-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">timeout: 0</span><br><span class="line">debug: false</span><br><span class="line">pull-image-on-create: false</span><br><span class="line">EOF</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">init_containerd() &#123;</span><br><span class="line">    containerd config default &gt; /etc/containerd/config.toml</span><br><span class="line">    sed -i &#x27;s/registry.k8s.io/registry.aliyuncs.com\/google_containers/&#x27; /etc/containerd/config.toml</span><br><span class="line">    systemctl restart containerd</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">service_onboot() &#123;</span><br><span class="line">    systemctl enable containerd</span><br><span class="line">    systemctl enable kubelet</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">disable_swap() &#123;</span><br><span class="line">    swapoff -a</span><br><span class="line">    sed -i &#x27;/swap/ s/^/#/g&#x27;  /etc/fstab</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据默认网关出接口的 IP 配置 hosts</span></span><br><span class="line">init_etchosts() &#123;</span><br><span class="line">    netif=$(ip route show | grep default | awk &#x27;&#123; print $5 &#125;&#x27;)</span><br><span class="line">    ipaddr=$(ip addr show $&#123;netif&#125; | grep -w inet | awk &#x27;&#123;print $2&#125;&#x27; | awk -F&#x27;/&#x27; &#x27;&#123; print $1 &#125;&#x27;)</span><br><span class="line">    echo &#x27;$&#123;ipaddr&#125; k8s-master01&#x27; &gt;&gt; /etc/hosts</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">init_sysctl() &#123;</span><br><span class="line">  cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf</span><br><span class="line">overlay</span><br><span class="line">br_netfilter</span><br><span class="line">EOF</span><br><span class="line">    modprobe overlay</span><br><span class="line">    modprobe br_netfilter</span><br><span class="line">  cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-iptables  = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.ip_forward                 = 1</span><br><span class="line">EOF</span><br><span class="line">    sysctl --system</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k8s_install() &#123;</span><br><span class="line">    kubeadm init --pod-network-cidr=10.244.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --node-name k8s-master01</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k8s_post() &#123;</span><br><span class="line">    mkdir -p /root/.kube</span><br><span class="line">    cp -a /etc/kubernetes/admin.conf /root/.kube/config</span><br><span class="line">    kubectl taint node k8s-master01 node-role.kubernetes.io/control-plane-</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">install_flannel() &#123;</span><br><span class="line">    wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class="line">    kubectl apply -f kube-flannel.yml</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">init() &#123;</span><br><span class="line">    init_repo</span><br><span class="line">    install_pkg</span><br><span class="line">    crictl_init</span><br><span class="line">    disable_swap</span><br><span class="line">    service_onboot</span><br><span class="line">    init_containerd</span><br><span class="line">    init_sysctl</span><br><span class="line">    init_etchosts</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">install() &#123;</span><br><span class="line">    k8s_install</span><br><span class="line">    k8s_post</span><br><span class="line">    install_flannel</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main() &#123;</span><br><span class="line">    init</span><br><span class="line">    install</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main</span><br></pre></td></tr></table></figure>

<p>  填入文件 k8s_install.sh，直接执行脚本即可完成安装。执行 bash k8s_install.sh，就会自动创建出一个单节点的。</p>
<p>  目前实现的非常简单，不考虑任何变化，逻辑判断也比较宽松，后续变动继续在 orca-tools 维护。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://guoltan.github.io/2023/09/11/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E8%BF%9B%E7%A8%8B%E5%B1%9E%E4%BA%8E%E5%93%AA%E4%B8%AA%E5%AE%B9%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="guoltan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="guoltan个人博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | guoltan个人博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/11/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E8%BF%9B%E7%A8%8B%E5%B1%9E%E4%BA%8E%E5%93%AA%E4%B8%AA%E5%AE%B9%E5%99%A8/" class="post-title-link" itemprop="url">如何查看进程属于哪个容器</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-11 20:11:25" itemprop="dateCreated datePublished" datetime="2023-09-11T20:11:25+08:00">2023-09-11</time>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a><strong>1.</strong> 背景</h2><p> 项目上可能会遇到一种场景，运维同学反馈某几个进程存在异常（CPU、内存占用率过高），希望定位出这些进程是哪些业务去创建的此时可以根据本文的处理方法进行诊断。</p>
<h2 id="2-处理方法"><a href="#2-处理方法" class="headerlink" title="2. 处理方法"></a>2. 处理方法</h2><h3 id="2-1-方法一：查看各个容器环境中，是否有对应进程名称在运行"><a href="#2-1-方法一：查看各个容器环境中，是否有对应进程名称在运行" class="headerlink" title="2.1. 方法一：查看各个容器环境中，是否有对应进程名称在运行"></a>2.1. 方法一：查看各个容器环境中，是否有对应进程名称在运行</h3><p>执行如下指令获取各个容器中运行的进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for id in $(docker ps -q | xargs); do echo echo &quot;----$id----&quot; ; docker top $id | grep 进程名称 ; done</span><br></pre></td></tr></table></figure>

<p>将获取到有回显的结果，进一步执行 docker ps -a | grep $id 来确认容器名称</p>
<p> </p>
<p>kubernetes 的容器命名规则： <code>k8s_&#123;containerName&#125;_&#123;podFullName&#125;_&#123;namespace&#125;_&#123;podUID&#125;_&#123;podrestartCount&#125;</code></p>
<p>通过 docker ps 获取到的容器名称，根据 podFullName 和 namespace 的命名，我们就能判断这个容器属于哪个命名空间下。</p>
<p> </p>
<h3 id="2-2-方法二：通过进程当前的-cgroup-判断它属于哪个容器或者-Pod"><a href="#2-2-方法二：通过进程当前的-cgroup-判断它属于哪个容器或者-Pod" class="headerlink" title="2.2. 方法二：通过进程当前的 cgroup 判断它属于哪个容器或者 Pod"></a><strong>2.2.</strong> 方法二：通过进程当前的 cgroup 判断它属于哪个容器或者 Pod</h3><p>通过查看 &#x2F;proc&#x2F;PID&#x2F;cgroup 文件，通过其中的pod字段来确认进程是否属于容器环境</p>
<p>cat &#x2F;proc&#x2F;xxxx&#x2F;cgroup</p>
<h2 id="3-示例：获取某个进程的-PID，该-PID-属于运行在容器环境"><a href="#3-示例：获取某个进程的-PID，该-PID-属于运行在容器环境" class="headerlink" title="3. 示例：获取某个进程的 PID，该 PID 属于运行在容器环境"></a>3. 示例：获取某个进程的 PID，该 PID 属于运行在容器环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]# cat /proc/7644/cgroup</span><br><span class="line"></span><br><span class="line">11:cpuset:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">10:perf_event:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">9:memory:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">8:hugetlb:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">7:devices:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">6:pids:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">5:freezer:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">4:net_prio,net_cls:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">3:blkio:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">2:cpuacct,cpu:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br><span class="line"></span><br><span class="line">1:name=systemd:/kubepods/besteffort/podda498366ad0b5d5f487968aaa924c6ab/7f77d3c235e601df71a70530fee4677230a768bd4442d46bad27210f6e520edc</span><br></pre></td></tr></table></figure>

<p>其中 podda498366ad0b5d5f487968aaa924c6ab 就是该进程所属的 pod，此时通过 docker ps -a | grep da4983 可以确认进程所属容器</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">guoltan</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
